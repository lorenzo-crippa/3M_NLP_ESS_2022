{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial Two (Python): Introduction to NLP Pipelines",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2151f6847584a708f63e29ca760085e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7eb67aa30a014975806ef273d0cc8202",
              "IPY_MODEL_733b094ca18e4ad5b21e4be2502fdffd",
              "IPY_MODEL_9d447109aaac4e24998c8e9a12cef9b5"
            ],
            "layout": "IPY_MODEL_9ef288a437564a399c025bc2e6b581a0"
          }
        },
        "7eb67aa30a014975806ef273d0cc8202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151fb9def52c4f988b3d968d75d0a4c8",
            "placeholder": "​",
            "style": "IPY_MODEL_2e5052e4c56444cba08cdde917634514",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: "
          }
        },
        "733b094ca18e4ad5b21e4be2502fdffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c25489c5fb841289056e86cde89a2fa",
            "max": 25998,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83dce59560d8409eb422fb623068cccf",
            "value": 25998
          }
        },
        "9d447109aaac4e24998c8e9a12cef9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589a1cd5375c4209bdfe7198f17096c9",
            "placeholder": "​",
            "style": "IPY_MODEL_2cab8f79be484bdda060963d05a3d403",
            "value": " 154k/? [00:00&lt;00:00, 2.79MB/s]"
          }
        },
        "9ef288a437564a399c025bc2e6b581a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "151fb9def52c4f988b3d968d75d0a4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5052e4c56444cba08cdde917634514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c25489c5fb841289056e86cde89a2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83dce59560d8409eb422fb623068cccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "589a1cd5375c4209bdfe7198f17096c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cab8f79be484bdda060963d05a3d403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3672e1726fc14cd4b7145a7482e84f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c618ff070404346860a1d22daeb449a",
              "IPY_MODEL_7124b30c350e412aa895827282b91621",
              "IPY_MODEL_127045425a6e4d0a977d4eaa5b738d78"
            ],
            "layout": "IPY_MODEL_58c94a865bdc4f72bb08374c0f6eddf4"
          }
        },
        "0c618ff070404346860a1d22daeb449a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2da2351b37a44fca34559e5a7906ddd",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6540bfa6e4481a8e860822954d0cd4",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/default.zip: 100%"
          }
        },
        "7124b30c350e412aa895827282b91621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2bd1b4e84354167af511b3ea245732a",
            "max": 479293702,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d095493aab5403fb3163b4dafa466ef",
            "value": 479293702
          }
        },
        "127045425a6e4d0a977d4eaa5b738d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0667fb2613274c1d910801e0d84046e8",
            "placeholder": "​",
            "style": "IPY_MODEL_068c0eccd70e403ca5924f10aa6e7fbb",
            "value": " 479M/479M [00:09&lt;00:00, 45.3MB/s]"
          }
        },
        "58c94a865bdc4f72bb08374c0f6eddf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2da2351b37a44fca34559e5a7906ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6540bfa6e4481a8e860822954d0cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2bd1b4e84354167af511b3ea245732a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d095493aab5403fb3163b4dafa466ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0667fb2613274c1d910801e0d84046e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068c0eccd70e403ca5924f10aa6e7fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "345f180de0274f3a8cea8b78fed3c1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6672a17e4ac4074b20ad8a175b6a862",
              "IPY_MODEL_ad0e80b93f1b4cfbbdc5b44297e1802d",
              "IPY_MODEL_c79984bc46e14e38bffe291d8cbda72f"
            ],
            "layout": "IPY_MODEL_5958c33d5179442485bcf2c40c440f4c"
          }
        },
        "c6672a17e4ac4074b20ad8a175b6a862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2c2b84f83e94bad98da2c8af98a1bb8",
            "placeholder": "​",
            "style": "IPY_MODEL_4c514703d0f04aa48b7b971fee013af7",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: "
          }
        },
        "ad0e80b93f1b4cfbbdc5b44297e1802d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7d44ed331c49399f14bf5a09952abe",
            "max": 25998,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a136e2eadbf4305a31dbfc04e413075",
            "value": 25998
          }
        },
        "c79984bc46e14e38bffe291d8cbda72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68da0e8eb2141269dd0c0e7c6066f9e",
            "placeholder": "​",
            "style": "IPY_MODEL_576cdfc042d347339fc06fa0cbb93ec3",
            "value": " 154k/? [00:00&lt;00:00, 2.83MB/s]"
          }
        },
        "5958c33d5179442485bcf2c40c440f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c2b84f83e94bad98da2c8af98a1bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c514703d0f04aa48b7b971fee013af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7d44ed331c49399f14bf5a09952abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a136e2eadbf4305a31dbfc04e413075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a68da0e8eb2141269dd0c0e7c6066f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576cdfc042d347339fc06fa0cbb93ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qp-kum7jK8y"
      },
      "source": [
        "# Introduction to NLP Annotation Pipelines in Python\n",
        "\n",
        "## Douglas Rice\n",
        "\n",
        "*This tutorial was originally created by Burt Monroe for his prior work with the Essex Summer School. I've updated and modified it.*\n",
        "\n",
        "In this notebook, we'll learn about doing NLP tasks in Python, including tokenization, stemming, part-of-speech tagging, named entity recognition, and dependency parsing. After completing this notebook, you should be familar with:\n",
        "\n",
        "\n",
        "1. Tokenization\n",
        "2. Stemming & Lemmatization\n",
        "3. Part-of-Speech Tagging\n",
        "4. Named Entity Recognition\n",
        "5. Dependency Parsing\n",
        "\n",
        "# Annotation Pipelines\n",
        "\n",
        "NLP tasks are typically organized around the concept of an annotation \"pipeline\" based on a given \"language model.\" Basically, you download/install/load a given model, then pass the model and your input text to the software's annotation pipeline and receive an output object with annotated text.\n",
        "\n",
        "For example, the pipeline for Stanford CoreNLP is depicted below. The text is first tokenized, then split into sentences, then tokens are tagged with respect to parts of speech, then the tokens are lemmatized, then the named entity recognizer is applied, and finally the dependency parser is applied. The output is an object from which all of those annotations can be accessed.\n",
        "\n",
        "![CoreNLP Pipeline (Source: https://stanfordnlp.github.io/CoreNLP/index.html)](https://stanfordnlp.github.io/CoreNLP/assets/images/pipeline.png)\n",
        "\n",
        "There are many different NLP libraries. In this notebook, we will focus on spaCy, Stanza/coreNLP, and NLTK. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbrlPhVivW5y"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "The **spaCy** package is, by some accounts, now the \"default\" standard NLP pipeline, especially in industry. Unlike its Python predecessor, NLTK, spaCy is \"opinionated\" -- it tries to provide easy, computationally efficient access to the best available model for any given task; NLTK provides many options and more direct ability for the researcher to test and modify different models. Also in contrast to NLTK, spaCy interacts nicely with modern neural / deep learning methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZCwgz5mx06S",
        "outputId": "c34771ff-0715-496b-e25f-84a24289e850"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiN2LFBo4pi9"
      },
      "source": [
        "### The standard spaCy pipeline (tokens, lemmas, pos, dependencies, ner, morphology, etc.)\n",
        "\n",
        "As noted above, the first step is to load the model. With spaCy, you have a number of different models to choose from that follow a standard naming convention. The first part specifies the language (\"en\"), the second part specifies the capabilities (\"core\"), the third part specifies what it was trained on (\"web\" or \"news\") and the last part specifies the size (\"sm\", \"md\", \"lrg\", or \"trf\"). \n",
        "\n",
        "In the below, we load \"en_core_web_sm\", which is the \"English, core capability, trained on the web, small\" model.\n",
        "\n",
        "Note that the model is loaded and assigned to the variable `nlp_spacy`. Now `nlp_spacy` is a *function* that says \"run this model's annotation pipeline on these string(s).\" This is very standard syntax for NLP pipelines.\n",
        "\n",
        "By default, spaCy runs *everything* supported by the given model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtBkZWEIyBXi"
      },
      "source": [
        "import spacy \n",
        "import sys\n",
        "\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II76RY0H5Dp3"
      },
      "source": [
        "We'll set up a small text and then look at what spaCy does for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_FLNL-syHMs",
        "outputId": "d09222db-1845-4ad9-c171-d8d48fca6018"
      },
      "source": [
        "annotated_doc_spacy = nlp_spacy(\"Joel Embiid should have been the 2022 NBA MVP, but instead Nikola Jokic won the award.\")\n",
        "for token in annotated_doc_spacy:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joel PROPN compound\n",
            "Embiid PROPN nsubj\n",
            "should AUX aux\n",
            "have AUX aux\n",
            "been AUX ROOT\n",
            "the DET det\n",
            "2022 NUM nummod\n",
            "NBA PROPN compound\n",
            "MVP PROPN attr\n",
            ", PUNCT punct\n",
            "but CCONJ cc\n",
            "instead ADV advmod\n",
            "Nikola PROPN compound\n",
            "Jokic PROPN nsubj\n",
            "won VERB conj\n",
            "the DET det\n",
            "award NOUN dobj\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCm1nwRwyQ-t"
      },
      "source": [
        "Notice that we are only printing a few fields in the above. The annotated object that we created (`annotated_doc_spacy`) retained much more detailed information. To access that information, we just need to specify what fields from the object we want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbDZ3qEUyG9R",
        "outputId": "0358faa7-becb-4754-c824-4a938fc30880"
      },
      "source": [
        "for token in annotated_doc_spacy:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joel Joel PROPN NNP compound Xxxx True False\n",
            "Embiid Embiid PROPN NNP nsubj Xxxxx True False\n",
            "should should AUX MD aux xxxx True True\n",
            "have have AUX VB aux xxxx True True\n",
            "been be AUX VBN ROOT xxxx True True\n",
            "the the DET DT det xxx True True\n",
            "2022 2022 NUM CD nummod dddd False False\n",
            "NBA NBA PROPN NNP compound XXX True False\n",
            "MVP MVP PROPN NNP attr XXX True False\n",
            ", , PUNCT , punct , False False\n",
            "but but CCONJ CC cc xxx True True\n",
            "instead instead ADV RB advmod xxxx True False\n",
            "Nikola Nikola PROPN NNP compound Xxxxx True False\n",
            "Jokic Jokic PROPN NNP nsubj Xxxxx True False\n",
            "won win VERB VBD conj xxx True False\n",
            "the the DET DT det xxx True True\n",
            "award award NOUN NN dobj xxxx True False\n",
            ". . PUNCT . punct . False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVmhrSm8Llv7"
      },
      "source": [
        "Beyond what we specified above, the model object contains a host of additional attributes, which are listed below. Many attributes that are intuitively strings (e.g., the token's lemma, its part of speech tag) are stored internally by spaCy as \"hashes\" (an integer). The attribute that provides the corresponding text will end in an underscore character.\n",
        "\n",
        "* `doc`: The parent document.\n",
        "* `lex`: The underlying lexeme.\n",
        "* `sent`: The sentence span that this token is a part of.\n",
        "* `text`:\tVerbatim text content.\n",
        "* `text_with_ws`:\tText content, with trailing space character if present.\n",
        "* `whitespace_`:\tTrailing space character if present.\n",
        "* `orth`:\tID of the verbatim text content.\n",
        "* `orth_`:\tVerbatim text content (identical to Token.text).\n",
        "* `vocab`:\tThe vocab object of the parent Doc.\n",
        "* `tensor`:\tThe token’s slice of the parent Doc’s tensor.\n",
        "* `head`:\tThe syntactic parent, or “governor”, of this token.\n",
        "* `left_edge`: The leftmost token of this token’s syntactic descendants.\n",
        "* `right_edge`:\tThe rightmost token of this token’s syntactic descendants.\n",
        "* `i`:\tThe index of the token within the parent document.\n",
        "* `ent_type`:\tNamed entity type. (integer)\n",
        "* `ent_type_`:\tNamed entity type. (string)\n",
        "* `ent_iob`:\tIOB code of named entity tag. 3 means the token begins an entity, 2 means it is outside an entity, 1 means it is inside an entity, and 0 means no entity tag is set.\n",
        "* `ent_iob_`:\tIOB code of named entity tag. “B” means the token begins an entity, “I” means it is inside an entity, “O” means it is outside an entity, and \"\" means no entity tag is set.\n",
        "* `ent_kb_id`:\tKnowledge base ID that refers to the named entity this token is a part of, if any. (integer)\n",
        "* `ent_kb_id_`:\tKnowledge base ID that refers to the named entity this token is a part of, if any. (string)\n",
        "* `ent_id`:\tID of the entity the token is an instance of, if any. Currently not used, but potentially for coreference resolution.\n",
        "* `ent_id_`:\tID of the entity the token is an instance of, if any. Currently not used, but potentially for coreference resolution.\n",
        "* `lemma`:\tBase form of the token, with no inflectional suffixes. (integer)\n",
        "* `lemma_`:\tBase form of the token, with no inflectional suffixes. (string)\n",
        "* `norm`:\tThe token’s norm, i.e. a normalized form of the token text. Can be set in the language’s tokenizer exceptions. (integer)\n",
        "* `norm_`:\tThe token’s norm, i.e. a normalized form of the token text. Can be set in the language’s tokenizer exceptions. (string)\n",
        "* `lower`:\tLowercase form of the token. (integer)\n",
        "* `lower_`:\tLowercase form of the token text. Equivalent to Token.text.lower(). (string)\n",
        "* `shape`:\tTransform of the token’s string to show orthographic features. Alphabetic characters are replaced by x or X, and numeric characters are replaced by d, and sequences of the same character are truncated after length 4. For example,\"Xxxx\"or\"dd\". (integer)\n",
        "* `shape_`:\tTransform of the token’s string to show orthographic features. Alphabetic characters are replaced by x or X, and numeric characters are replaced by d, and sequences of the same character are truncated after length 4. For example,\"Xxxx\"or\"dd\". (string)\n",
        "* `prefix`:\tHash value of a length-N substring from the start of the token. Defaults to N=1. (integer)\n",
        "* `prefix_`:\tA length-N substring from the start of the token. Defaults to N=1.\n",
        "* `suffix`:\tHash value of a length-N substring from the end of the token. Defaults to N=3. (integer)\n",
        "* `suffix_`:\tLength-N substring from the end of the token. Defaults to N=3.\n",
        "* `is_alpha`:\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
        "* `is_ascii`:\tDoes the token consist of ASCII characters? Equivalent to all(ord(c) < 128 for c in token.text).\n",
        "* `is_digit`:\tDoes the token consist of digits? Equivalent to token.text.isdigit().\n",
        "* `is_lower`:\tIs the token in lowercase? Equivalent to token.text.islower().\n",
        "* `is_upper`:\tIs the token in uppercase? Equivalent to token.text.isupper().\n",
        "* `is_title`:\tIs the token in titlecase? Equivalent to token.text.istitle().\n",
        "* `is_punct`:\tIs the token punctuation?\n",
        "* `is_left_punct`:\tIs the token a left punctuation mark, e.g. \"(\" ?\n",
        "* `is_right_punct`:\tIs the token a right punctuation mark, e.g. \")\" ?\n",
        "* `is_space`:\tDoes the token consist of whitespace characters? Equivalent to token.text.isspace().\n",
        "* `is_bracket`:\tIs the token a bracket?\n",
        "* `is_quote`:\tIs the token a quotation mark?\n",
        "* `is_currency`:\tIs the token a currency symbol?\n",
        "* `like_url`:\tDoes the token resemble a URL?\n",
        "* `like_num`:\tDoes the token represent a number? e.g. “10.9”, “10”, “ten”, etc.\n",
        "* `like_email`:\tDoes the token resemble an email address?\n",
        "* `is_oov`:\tIs the token out-of-vocabulary (i.e. does it not have a word vector)?\n",
        "* `is_stop`:\tIs the token part of a “stop list”?\n",
        "* `pos`:\tCoarse-grained part-of-speech from the Universal POS tag set. (integer)\n",
        "* `pos_`:\tCoarse-grained part-of-speech from the Universal POS tag set. (string)\n",
        "* `tag`:\tFine-grained part-of-speech. (integer)\n",
        "* `tag_`:\tFine-grained part-of-speech. (string)\n",
        "* `morph`:\tMorphological analysis.\n",
        "* `dep`:\tSyntactic dependency relation. (integer)\n",
        "* `dep_`:\tSyntactic dependency relation. (string)\n",
        "* `lang`:\tLanguage of the parent document’s vocabulary. (integer)\n",
        "* `lang_`:\tLanguage of the parent document’s vocabulary. (string)\n",
        "* `prob`:\tSmoothed log probability estimate of token’s word type (context-independent entry in the vocabulary).\n",
        "* `idx`:\tThe character offset of the token within the parent document.\n",
        "* `sentiment`:\tA scalar value indicating the positivity or negativity of the token.\n",
        "* `lex_id`:\tSequential ID of the token’s lexical type, used to index into tables, e.g. for word vectors.\n",
        "* `rank`:\tSequential ID of the token’s lexical type, used to index into tables, e.g. for word vectors.\n",
        "* `cluster`:\tBrown cluster ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE3eAmvy5VE5"
      },
      "source": [
        "As a sidebar, I note Spacy stores vocabulary strings internally as hashes, and these can be accessed directly in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UDM3po_y-q1",
        "outputId": "1e2e2f4a-3794-44a8-d339-1d34e0e1b43e"
      },
      "source": [
        "print(annotated_doc_spacy.vocab.strings[\"Apple\"]) # 6418411030699964375\n",
        "print(annotated_doc_spacy.vocab.strings[6418411030699964375]) # \"Apple\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6418411030699964375\n",
            "Apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rmU_4ow5apv"
      },
      "source": [
        "As noted, the model here is \"en_core_web_sm\". Small models are more compact and computationally efficient than the medium, large, or transformer-based models, but less accurate and they do not come with pretrained embeddings. You can see which models are available, along with exact details of each model and performance statistics, here: https://spacy.io/models/ (As of this writing, there are pretrained models for 22 languages: Catalan, Chinese, Croatian, Danish, Dutch, English, Finnish, French, German, Greek, Italian, Japanese, Korean, Lithuanian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Spanish, and Swedish), as well as a multi-language support model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL806jdivHoe"
      },
      "source": [
        "### Named entities\n",
        "\n",
        "As we saw above, the spacy pipeline includes a named entity recognizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-cyMQOdzSnN",
        "outputId": "7f15a8f3-6301-423e-b433-11bb7b729fcc"
      },
      "source": [
        "for ent in annotated_doc_spacy.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joel Embiid 0 11 PERSON\n",
            "2022 33 37 DATE\n",
            "NBA MVP 38 45 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3_TaWB5lfM"
      },
      "source": [
        "For comparison with the other NLP pipelines discussed below and in other notebooks, we can see what named entities spaCy extracts from an excerpt of Ketanji Brown Jackson's acceptance speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXkO_fDuziZW"
      },
      "source": [
        "kbj = \"I have spent years toiling away in the relative solitude of my chambers, with just my law clerks, in isolation. So, it's been somewhat overwhelming, in a good way, to recently be flooded with thousands of notes and cards and photos expressing just how much this moment means to so many people.\\nThe notes that I've received from children are particularly cute and especially meaningful because, more than anything, they speak directly to the hope and promise of America.\\nIt has taken 232 years and 115 prior appointments for a Black woman to be selected to serve on the Supreme Court of the United States.\\nBut we've made it. We've made it, all of us. All of us.\\nAnd our children are telling me that they see now, more than ever, that, here in America, anything is possible.\\nThey also tell me that I'm a role model, which I take both as an opportunity and as a huge responsibility. I am feeling up to the task, primarily because I know that I am not alone. I am standing on the shoulders of my own role models, generations of Americans who never had anything close to this kind of opportunity but who got up every day and went to work believing in the promise of America, showing others through their determination and, yes, their perseverance that good -- good things can be done in this great country -- from my grandparents on both sides who had only a grade-school education but instilled in my parents the importance of learning, to my parents who went to racially segregated schools growing up and were the first in their families to have the chance to go to college.\\nI am also ever buoyed by the leadership of generations past who helped to light the way: Dr. Martin Luther King Jr., Justice Thurgood Marshall, and my personal heroine, Judge Constance Baker Motley. They, and so many others, did the heavy lifting that made this day possible. And for all of the talk of this historic nomination and now confirmation, I think of them as the true pathbreakers. I am just the very lucky first inheritor of the dream of liberty and justice for all.\\nTo be sure, I have worked hard to get to this point in my career, and I have now achieved something far beyond anything my grandparents could've possibly ever imagined. But no one does this on their own. The path was cleared for me so that I might rise to this occasion. And in the poetic words of Dr. Maya Angelou, I do so now, while 'bringing the gifts...my ancestors gave.'  'I am the dream and the hope of the slave.' So as I take on this new role, I strongly believe that this is a moment in which all Americans can take great pride. We have come a long way toward perfecting our union. In my family, it took just one generation to go from segregation to the Supreme Court of the United States. And it is an honor -- the honor of a lifetime -- for me to have this chance to join the Court, to promote the rule of law at the highest level, and to do my part to carry our shared project of democracy and equal justice under law forward, into the future. Thank you, again, Mr. President and members of the Senate for this incredible honor.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjgHBZfOzueh",
        "outputId": "c0ac7162-54a0-4f3d-b0ac-50acb0833e96"
      },
      "source": [
        "kbj_ann_spacy = nlp_spacy(kbj) \n",
        "for ent in kbj_ann_spacy.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "years 13 18 DATE\n",
            "thousands 192 201 CARDINAL\n",
            "America 461 468 GPE\n",
            "232 years 483 492 DATE\n",
            "115 497 500 CARDINAL\n",
            "the Supreme Court 565 582 ORG\n",
            "the United States 586 603 GPE\n",
            "America 742 749 GPE\n",
            "Americans 1024 1033 NORP\n",
            "America 1161 1168 GPE\n",
            "first 1511 1516 ORDINAL\n",
            "Martin Luther King Jr. 1665 1687 PERSON\n",
            "Thurgood Marshall 1697 1714 PERSON\n",
            "Constance Baker Motley 1747 1769 PERSON\n",
            "this day 1829 1837 DATE\n",
            "first 1989 1994 ORDINAL\n",
            "Maya Angelou 2352 2364 PERSON\n",
            "Americans 2557 2566 NORP\n",
            "the Supreme Court 2710 2727 ORG\n",
            "the United States 2731 2748 GPE\n",
            "Court 2838 2843 ORG\n",
            "Senate 3058 3064 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv_5ZsFo5vzo"
      },
      "source": [
        "### Morphological features\n",
        "\n",
        "A lemma can be *inflected* with **morphological features** to produce a \"surface form\". Examples of morpholological features include case, number, verb form, tense, person and mood. Spacy conducts morphological analysis as part of its pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KaicrD30HpG",
        "outputId": "9c65fd67-8698-4d0c-aa9f-709e585bcffb"
      },
      "source": [
        "ann2 = nlp_spacy(\"I walked the dog yesterday.\")\n",
        "print(ann2[1],ann2[1].lemma_,ann2[1].pos_,ann2[1].tag_, [mf for mf in ann2[1].morph])\n",
        "ann3 = nlp_spacy(\"I will walk the dog tomorrow.\")\n",
        "print(ann3[2],ann3[2].lemma_,ann3[2].pos_,ann3[1].tag_,[mf for mf in ann3[2].morph])\n",
        "ann4 = nlp_spacy(\"I am walking the dog.\")\n",
        "print(ann4[2],ann4[2].lemma_,ann4[2].pos_,ann4[2].tag_,[mf for mf in ann4[2].morph])\n",
        "ann5 = nlp_spacy(\"I was walking the dog.\")\n",
        "print(ann5[2],ann5[2].lemma_,ann5[2].pos_,ann5[2].tag_,[mf for mf in ann5[2].morph])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walked walk VERB VBD ['Tense=Past', 'VerbForm=Fin']\n",
            "walk walk VERB MD ['VerbForm=Inf']\n",
            "walking walk VERB VBG ['Aspect=Prog', 'Tense=Pres', 'VerbForm=Part']\n",
            "walking walk VERB VBG ['Aspect=Prog', 'Tense=Pres', 'VerbForm=Part']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43s1VR0255cK"
      },
      "source": [
        "I invite the grammar afficionados among you to assess whether those were all as expected. The first two are tagged as finite present and past tense respectively. The third is labeled as a modal verb in infinitive form. The last two are labeled identically -- with detailed POS tag of VBG (\"gerund\") -- and progressive/ongoing present participle (despite the last being past).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOwqQA52u_Y_"
      },
      "source": [
        "### Noun phrases\n",
        " As part of its dependency parsing, Spacy will isolate **noun chunks** or **noun phrases**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anTGYjbe2vDE",
        "outputId": "3de3f877-f0d6-4860-b86d-c91830ef0af8"
      },
      "source": [
        "docnp = nlp_spacy(\"Joel Embiid should have been the 2022 NBA MVP, but instead Nikola Jokic won the award.\")\n",
        "for chunk in docnp.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joel Embiid Embiid nsubj been\n",
            "the 2022 NBA MVP MVP attr been\n",
            "Nikola Jokic Jokic nsubj won\n",
            "the award award dobj won\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Ll7yi26A2s"
      },
      "source": [
        "### Dependency parse trees\n",
        "\n",
        "Dependency parsing is a complex subject we'll discuss in more detail separately. For present purposes, I'll just show some basics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namRctZC5Ny6"
      },
      "source": [
        "Spacy provides a visualization tool for its dependency parse, called **displacy**. So we need to load that up. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy's dependency parse is a tree that can be navigated like one. Every word has exactly one head, one word (or \"root\") that points to it via an arc. The example from the documentation looks like this:\n"
      ],
      "metadata": {
        "id": "DrCulXNR9VRJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "pi1QUHWj27yN",
        "outputId": "06e2d8ef-19ca-4583-ed27-ee985c646841"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "docnp = nlp_spacy(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "displacy.render(docnp, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cf4120e6dc304856995e7c736c0322b4-0\" class=\"displacy\" width=\"680\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Autonomous</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">cars</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">shift</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">insurance</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">liability</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">toward</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">manufacturers</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-2\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-3\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M405.0,139.0 L413.0,127.0 397.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-4\" stroke-width=\"2px\" d=\"M250,137.0 C250,2.0 500.0,2.0 500.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M500.0,139.0 L508.0,127.0 492.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cf4120e6dc304856995e7c736c0322b4-0-5\" stroke-width=\"2px\" d=\"M520,137.0 C520,92.0 580.0,92.0 580.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cf4120e6dc304856995e7c736c0322b4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M580.0,139.0 L588.0,127.0 572.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQtPHAUb6Kbs"
      },
      "source": [
        "So we iterate over words to find an arc of interest \"from below.\" Specifically, in this example, we search for a verb that has a subject (a verb with an \"nsubj\" arc leading from it) like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezm-E8WP3DUG",
        "outputId": "b5b5cad2-5f17-4613-c720-534257f56655"
      },
      "source": [
        "from spacy.symbols import nsubj, VERB\n",
        "# Finding a verb with a subject from below — good\n",
        "verbs = set()\n",
        "for possible_subject in docnp:\n",
        "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "        verbs.add(possible_subject.head)\n",
        "print(verbs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{shift}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaoKneeB6QtV"
      },
      "source": [
        "The verb \"shift\" is the only verb with a subject (\"cars\"). (The core subject-verb-object construction is \"cars shift liability.\")\n",
        "\n",
        "Spacy provides attributes that can be used to traverse the tree. For example attribute `lefts` contains the children nodes to the left of a given node, and `rights` contains the children nodes to the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXSayLmY3S4n",
        "outputId": "e30fd8b2-f384-4600-b54c-d0fdaff6673d"
      },
      "source": [
        "for left in docnp[2].lefts:\n",
        "  print(\"left\", left,left.dep_)\n",
        "for right in docnp[2].rights:\n",
        "  print(\"right\",right,right.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "left cars nsubj\n",
            "right liability dobj\n",
            "right toward prep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynSNQ-iy6ZQu"
      },
      "source": [
        "So, \"shift\" has three children, the subject \"cars\" to its left, the direct object \"liability\" to its right, and the preposition \"toward\" to its right.\n",
        "\n",
        "Dependency parsing is the basic foundation for many information extraction applications, such as political event data production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3qUUIRT6dL9"
      },
      "source": [
        "## Stanza (formerly Stanford NLP) and coreNLP\n",
        "\n",
        "\n",
        "Stanza -- formerly StanfordNLP -- is a Python library from the Stanford NLP group. Stanza provides a wrapper to coreNLP, the research group's Java library, and it inherits coreNLP functionality.  \n",
        "\n",
        "The official description:\n",
        "\n",
        "> Stanza is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, to give a syntactic structure dependency parse, and to recognize named entities. The toolkit is designed to be parallel among more than 70 languages, using the Universal Dependencies formalism.\n",
        "\n",
        "> Stanza is built with highly accurate neural network components that also enable efficient training and evaluation with your own annotated data. The modules are built on top of the PyTorch library. You will get much faster performance if you run the software on a GPU-enabled machine.\n",
        "\n",
        "> In addition, Stanza includes a Python interface to the CoreNLP Java package and inherits additional functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.\n",
        "\n",
        "> To summarize, Stanza features:\n",
        "\n",
        "> Native Python implementation requiring minimal efforts to set up;\n",
        "\n",
        "> Full neural network pipeline for robust text analytics, including tokenization, multi-word token (MWT) expansion, lemmatization, part-of-speech (POS) and morphological features tagging, dependency parsing, and named entity recognition;\n",
        "\n",
        "> Pretrained neural models supporting 66 (human) languages;\n",
        "\n",
        "> A stable, officially maintained Python interface to CoreNLP.\n",
        "\n",
        "> Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.\n",
        "\n",
        "Stanford's **coreNLP** is generally pretty close to the gold-standard NLP engine. From its official page (https://stanfordnlp.github.io/CoreNLP/): \n",
        "\n",
        "> CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish.\n",
        "\n",
        "Stanza is now *the* way to access coreNLP through Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX0k3GJFjJ33",
        "outputId": "6af09ff7-2b5d-4578-bb47-663ae3bf2f01"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.12.0+cu113)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->stanza) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->stanza) (3.8.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=a0454722c4be307d2949a80c226872d62c5a25b9ba08535509434d48ddab3984\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, emoji, stanza\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed emoji-1.7.0 huggingface-hub-0.8.1 pyyaml-6.0 stanza-1.4.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDe0cZbuwW3q"
      },
      "source": [
        "### The standard Stanza pipeline (tokens, pos, lemmas, dependency parse, sentiment, ner)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as we did before, we need to begin by loading the model. We'll load the \"en\" (or English) model. You can find the full list of models [here](https://stanfordnlp.github.io/stanza/available_models.html). There are more than 60 models listed there, so you have a lot of options depending on your need."
      ],
      "metadata": {
        "id": "n7429EuaBccG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614,
          "referenced_widgets": [
            "c2151f6847584a708f63e29ca760085e",
            "7eb67aa30a014975806ef273d0cc8202",
            "733b094ca18e4ad5b21e4be2502fdffd",
            "9d447109aaac4e24998c8e9a12cef9b5",
            "9ef288a437564a399c025bc2e6b581a0",
            "151fb9def52c4f988b3d968d75d0a4c8",
            "2e5052e4c56444cba08cdde917634514",
            "8c25489c5fb841289056e86cde89a2fa",
            "83dce59560d8409eb422fb623068cccf",
            "589a1cd5375c4209bdfe7198f17096c9",
            "2cab8f79be484bdda060963d05a3d403",
            "3672e1726fc14cd4b7145a7482e84f22",
            "0c618ff070404346860a1d22daeb449a",
            "7124b30c350e412aa895827282b91621",
            "127045425a6e4d0a977d4eaa5b738d78",
            "58c94a865bdc4f72bb08374c0f6eddf4",
            "a2da2351b37a44fca34559e5a7906ddd",
            "4e6540bfa6e4481a8e860822954d0cd4",
            "a2bd1b4e84354167af511b3ea245732a",
            "0d095493aab5403fb3163b4dafa466ef",
            "0667fb2613274c1d910801e0d84046e8",
            "068c0eccd70e403ca5924f10aa6e7fbb",
            "345f180de0274f3a8cea8b78fed3c1da",
            "c6672a17e4ac4074b20ad8a175b6a862",
            "ad0e80b93f1b4cfbbdc5b44297e1802d",
            "c79984bc46e14e38bffe291d8cbda72f",
            "5958c33d5179442485bcf2c40c440f4c",
            "f2c2b84f83e94bad98da2c8af98a1bb8",
            "4c514703d0f04aa48b7b971fee013af7",
            "df7d44ed331c49399f14bf5a09952abe",
            "1a136e2eadbf4305a31dbfc04e413075",
            "a68da0e8eb2141269dd0c0e7c6066f9e",
            "576cdfc042d347339fc06fa0cbb93ec3"
          ]
        },
        "id": "DPAwNpYgjpj3",
        "outputId": "ab000627-74b6-448c-be8d-6777fdf17974"
      },
      "source": [
        "import stanza\n",
        "\n",
        "stanza.download('en')\n",
        "nlp_stanza = stanza.Pipeline('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2151f6847584a708f63e29ca760085e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-13 01:42:34 INFO: Downloading default packages for language: en (English)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/default.zip:   0%|          | 0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3672e1726fc14cd4b7145a7482e84f22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-13 01:42:52 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "345f180de0274f3a8cea8b78fed3c1da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-13 01:42:54 INFO: Loading these models for language: en (English):\n",
            "============================\n",
            "| Processor    | Package   |\n",
            "----------------------------\n",
            "| tokenize     | combined  |\n",
            "| pos          | combined  |\n",
            "| lemma        | combined  |\n",
            "| depparse     | combined  |\n",
            "| sentiment    | sstplus   |\n",
            "| constituency | wsj       |\n",
            "| ner          | ontonotes |\n",
            "============================\n",
            "\n",
            "2022-07-13 01:42:54 INFO: Use device: cpu\n",
            "2022-07-13 01:42:54 INFO: Loading: tokenize\n",
            "2022-07-13 01:42:54 INFO: Loading: pos\n",
            "2022-07-13 01:42:54 INFO: Loading: lemma\n",
            "2022-07-13 01:42:54 INFO: Loading: depparse\n",
            "2022-07-13 01:42:54 INFO: Loading: sentiment\n",
            "2022-07-13 01:42:54 INFO: Loading: constituency\n",
            "2022-07-13 01:42:55 INFO: Loading: ner\n",
            "2022-07-13 01:42:56 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbv4rfSvmKD3"
      },
      "source": [
        "Note that we downloaded the \"default\" processors for the NLP pipeline. You can specify *which* specific processors to use for any given task (at least if options exist in that particular language model) as well as substitute your own.\n",
        "\n",
        "The default pipeline includes a tokenizer, a POS tagger, a lemmatizer, a dependency parser, a sentiment analyzer, a constituency parser, and a named-entity recognizer.\n",
        "\n",
        "We apply the pipeline to our text and assign it to a Document object, which will include the annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SiB0-JDkiDX"
      },
      "source": [
        "annotated_doc_stanza = nlp_stanza(\"Joel Embiid should have been the 2022 NBA MVP. Instead, Nikola Jokic won the award.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfhiFpmnS0d"
      },
      "source": [
        "The Document now has attributes including `sentences`, a list of Sentence objects. Sentence objects have attributes that include a list of `tokens`, `words`, entities (`ents`), `dependencies`, and `sentiment` (if there was a sentiment processor).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcPvZXFlkvPF",
        "outputId": "db2f5118-5c03-4853-b1c5-045f910c721d"
      },
      "source": [
        "for sentence in annotated_doc_stanza.sentences:\n",
        "    for word in sentence.words:\n",
        "        print(word.text, word.lemma, word.pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joel Joel PROPN\n",
            "Embiid Embiid PROPN\n",
            "should should AUX\n",
            "have have AUX\n",
            "been be AUX\n",
            "the the DET\n",
            "2022 2022 NUM\n",
            "NBA NBA PROPN\n",
            "MVP MVP NOUN\n",
            ". . PUNCT\n",
            "Instead instead ADV\n",
            ", , PUNCT\n",
            "Nikola Nikola PROPN\n",
            "Jokic Jokic PROPN\n",
            "won win VERB\n",
            "the the DET\n",
            "award award NOUN\n",
            ". . PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw6_60-xRA5"
      },
      "source": [
        "### Named entities and dependency parse trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAtdLFr3z4kI"
      },
      "source": [
        "Entities and dependencies are provided in lists of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtZFHQcNkzqj",
        "outputId": "76090e37-4398-430f-b27d-db0e93f7535d"
      },
      "source": [
        "for sentence in annotated_doc_stanza.sentences:\n",
        "    print(sentence.ents)\n",
        "    print(sentence.dependencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\n",
            "  \"text\": \"Joel Embiid\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 0,\n",
            "  \"end_char\": 11\n",
            "}, {\n",
            "  \"text\": \"2022\",\n",
            "  \"type\": \"DATE\",\n",
            "  \"start_char\": 33,\n",
            "  \"end_char\": 37\n",
            "}, {\n",
            "  \"text\": \"NBA\",\n",
            "  \"type\": \"ORG\",\n",
            "  \"start_char\": 38,\n",
            "  \"end_char\": 41\n",
            "}]\n",
            "[({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'nsubj', {\n",
            "  \"id\": 1,\n",
            "  \"text\": \"Joel\",\n",
            "  \"lemma\": \"Joel\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"nsubj\",\n",
            "  \"start_char\": 0,\n",
            "  \"end_char\": 4\n",
            "}), ({\n",
            "  \"id\": 1,\n",
            "  \"text\": \"Joel\",\n",
            "  \"lemma\": \"Joel\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"nsubj\",\n",
            "  \"start_char\": 0,\n",
            "  \"end_char\": 4\n",
            "}, 'flat', {\n",
            "  \"id\": 2,\n",
            "  \"text\": \"Embiid\",\n",
            "  \"lemma\": \"Embiid\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 1,\n",
            "  \"deprel\": \"flat\",\n",
            "  \"start_char\": 5,\n",
            "  \"end_char\": 11\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'aux', {\n",
            "  \"id\": 3,\n",
            "  \"text\": \"should\",\n",
            "  \"lemma\": \"should\",\n",
            "  \"upos\": \"AUX\",\n",
            "  \"xpos\": \"MD\",\n",
            "  \"feats\": \"VerbForm=Fin\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"aux\",\n",
            "  \"start_char\": 12,\n",
            "  \"end_char\": 18\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'aux', {\n",
            "  \"id\": 4,\n",
            "  \"text\": \"have\",\n",
            "  \"lemma\": \"have\",\n",
            "  \"upos\": \"AUX\",\n",
            "  \"xpos\": \"VB\",\n",
            "  \"feats\": \"VerbForm=Inf\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"aux\",\n",
            "  \"start_char\": 19,\n",
            "  \"end_char\": 23\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'cop', {\n",
            "  \"id\": 5,\n",
            "  \"text\": \"been\",\n",
            "  \"lemma\": \"be\",\n",
            "  \"upos\": \"AUX\",\n",
            "  \"xpos\": \"VBN\",\n",
            "  \"feats\": \"Tense=Past|VerbForm=Part\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"cop\",\n",
            "  \"start_char\": 24,\n",
            "  \"end_char\": 28\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'det', {\n",
            "  \"id\": 6,\n",
            "  \"text\": \"the\",\n",
            "  \"lemma\": \"the\",\n",
            "  \"upos\": \"DET\",\n",
            "  \"xpos\": \"DT\",\n",
            "  \"feats\": \"Definite=Def|PronType=Art\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"det\",\n",
            "  \"start_char\": 29,\n",
            "  \"end_char\": 32\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'compound', {\n",
            "  \"id\": 7,\n",
            "  \"text\": \"2022\",\n",
            "  \"lemma\": \"2022\",\n",
            "  \"upos\": \"NUM\",\n",
            "  \"xpos\": \"CD\",\n",
            "  \"feats\": \"NumType=Card\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"compound\",\n",
            "  \"start_char\": 33,\n",
            "  \"end_char\": 37\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'compound', {\n",
            "  \"id\": 8,\n",
            "  \"text\": \"NBA\",\n",
            "  \"lemma\": \"NBA\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"compound\",\n",
            "  \"start_char\": 38,\n",
            "  \"end_char\": 41\n",
            "}), ({\n",
            "  \"id\": 0,\n",
            "  \"text\": \"ROOT\"\n",
            "}, 'root', {\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}), ({\n",
            "  \"id\": 9,\n",
            "  \"text\": \"MVP\",\n",
            "  \"lemma\": \"MVP\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 42,\n",
            "  \"end_char\": 45\n",
            "}, 'punct', {\n",
            "  \"id\": 10,\n",
            "  \"text\": \".\",\n",
            "  \"lemma\": \".\",\n",
            "  \"upos\": \"PUNCT\",\n",
            "  \"xpos\": \".\",\n",
            "  \"head\": 9,\n",
            "  \"deprel\": \"punct\",\n",
            "  \"start_char\": 45,\n",
            "  \"end_char\": 46\n",
            "})]\n",
            "[{\n",
            "  \"text\": \"Nikola Jokic\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 56,\n",
            "  \"end_char\": 68\n",
            "}]\n",
            "[({\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}, 'advmod', {\n",
            "  \"id\": 1,\n",
            "  \"text\": \"Instead\",\n",
            "  \"lemma\": \"instead\",\n",
            "  \"upos\": \"ADV\",\n",
            "  \"xpos\": \"RB\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"advmod\",\n",
            "  \"start_char\": 47,\n",
            "  \"end_char\": 54\n",
            "}), ({\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}, 'punct', {\n",
            "  \"id\": 2,\n",
            "  \"text\": \",\",\n",
            "  \"lemma\": \",\",\n",
            "  \"upos\": \"PUNCT\",\n",
            "  \"xpos\": \",\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"punct\",\n",
            "  \"start_char\": 54,\n",
            "  \"end_char\": 55\n",
            "}), ({\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}, 'nsubj', {\n",
            "  \"id\": 3,\n",
            "  \"text\": \"Nikola\",\n",
            "  \"lemma\": \"Nikola\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"nsubj\",\n",
            "  \"start_char\": 56,\n",
            "  \"end_char\": 62\n",
            "}), ({\n",
            "  \"id\": 3,\n",
            "  \"text\": \"Nikola\",\n",
            "  \"lemma\": \"Nikola\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"nsubj\",\n",
            "  \"start_char\": 56,\n",
            "  \"end_char\": 62\n",
            "}, 'flat', {\n",
            "  \"id\": 4,\n",
            "  \"text\": \"Jokic\",\n",
            "  \"lemma\": \"Jokic\",\n",
            "  \"upos\": \"PROPN\",\n",
            "  \"xpos\": \"NNP\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 3,\n",
            "  \"deprel\": \"flat\",\n",
            "  \"start_char\": 63,\n",
            "  \"end_char\": 68\n",
            "}), ({\n",
            "  \"id\": 0,\n",
            "  \"text\": \"ROOT\"\n",
            "}, 'root', {\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}), ({\n",
            "  \"id\": 7,\n",
            "  \"text\": \"award\",\n",
            "  \"lemma\": \"award\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"obj\",\n",
            "  \"start_char\": 77,\n",
            "  \"end_char\": 82\n",
            "}, 'det', {\n",
            "  \"id\": 6,\n",
            "  \"text\": \"the\",\n",
            "  \"lemma\": \"the\",\n",
            "  \"upos\": \"DET\",\n",
            "  \"xpos\": \"DT\",\n",
            "  \"feats\": \"Definite=Def|PronType=Art\",\n",
            "  \"head\": 7,\n",
            "  \"deprel\": \"det\",\n",
            "  \"start_char\": 73,\n",
            "  \"end_char\": 76\n",
            "}), ({\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}, 'obj', {\n",
            "  \"id\": 7,\n",
            "  \"text\": \"award\",\n",
            "  \"lemma\": \"award\",\n",
            "  \"upos\": \"NOUN\",\n",
            "  \"xpos\": \"NN\",\n",
            "  \"feats\": \"Number=Sing\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"obj\",\n",
            "  \"start_char\": 77,\n",
            "  \"end_char\": 82\n",
            "}), ({\n",
            "  \"id\": 5,\n",
            "  \"text\": \"won\",\n",
            "  \"lemma\": \"win\",\n",
            "  \"upos\": \"VERB\",\n",
            "  \"xpos\": \"VBD\",\n",
            "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
            "  \"head\": 0,\n",
            "  \"deprel\": \"root\",\n",
            "  \"start_char\": 69,\n",
            "  \"end_char\": 72\n",
            "}, 'punct', {\n",
            "  \"id\": 8,\n",
            "  \"text\": \".\",\n",
            "  \"lemma\": \".\",\n",
            "  \"upos\": \"PUNCT\",\n",
            "  \"xpos\": \".\",\n",
            "  \"head\": 5,\n",
            "  \"deprel\": \"punct\",\n",
            "  \"start_char\": 82,\n",
            "  \"end_char\": 83\n",
            "})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lduZEt7q747d"
      },
      "source": [
        "As a comparison with spaCy, let's look at the entities we've identified from Ketanji Brown Jackson's speech with Stanza, and compare those to ones identified with spaCy. First, we need to run the `kbj` speech through our Stanza pipeline. Note that this takes a minute or so to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLzctuuQ779i"
      },
      "source": [
        "kbj_ann_stanza = nlp_stanza(kbj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDNSpNyv8OWn",
        "outputId": "add790fa-8eea-42a9-98b5-4dcacb6c3adf"
      },
      "source": [
        "for sentence in kbj_ann_stanza.sentences:\n",
        "    print(sentence.ents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\n",
            "  \"text\": \"years\",\n",
            "  \"type\": \"DATE\",\n",
            "  \"start_char\": 13,\n",
            "  \"end_char\": 18\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"thousands\",\n",
            "  \"type\": \"CARDINAL\",\n",
            "  \"start_char\": 192,\n",
            "  \"end_char\": 201\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"America\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 461,\n",
            "  \"end_char\": 468\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"232 years\",\n",
            "  \"type\": \"DATE\",\n",
            "  \"start_char\": 483,\n",
            "  \"end_char\": 492\n",
            "}, {\n",
            "  \"text\": \"115\",\n",
            "  \"type\": \"CARDINAL\",\n",
            "  \"start_char\": 497,\n",
            "  \"end_char\": 500\n",
            "}, {\n",
            "  \"text\": \"Black\",\n",
            "  \"type\": \"NORP\",\n",
            "  \"start_char\": 526,\n",
            "  \"end_char\": 531\n",
            "}, {\n",
            "  \"text\": \"the Supreme Court\",\n",
            "  \"type\": \"ORG\",\n",
            "  \"start_char\": 565,\n",
            "  \"end_char\": 582\n",
            "}, {\n",
            "  \"text\": \"the United States\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 586,\n",
            "  \"end_char\": 603\n",
            "}]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"America\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 742,\n",
            "  \"end_char\": 749\n",
            "}]\n",
            "[]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"Americans\",\n",
            "  \"type\": \"NORP\",\n",
            "  \"start_char\": 1024,\n",
            "  \"end_char\": 1033\n",
            "}, {\n",
            "  \"text\": \"America\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 1161,\n",
            "  \"end_char\": 1168\n",
            "}, {\n",
            "  \"text\": \"first\",\n",
            "  \"type\": \"ORDINAL\",\n",
            "  \"start_char\": 1511,\n",
            "  \"end_char\": 1516\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"Martin Luther King Jr.\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 1665,\n",
            "  \"end_char\": 1687\n",
            "}, {\n",
            "  \"text\": \"Thurgood Marshall\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 1697,\n",
            "  \"end_char\": 1714\n",
            "}, {\n",
            "  \"text\": \"Constance Baker Motley\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 1747,\n",
            "  \"end_char\": 1769\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"this day\",\n",
            "  \"type\": \"DATE\",\n",
            "  \"start_char\": 1829,\n",
            "  \"end_char\": 1837\n",
            "}]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"first\",\n",
            "  \"type\": \"ORDINAL\",\n",
            "  \"start_char\": 1989,\n",
            "  \"end_char\": 1994\n",
            "}]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"Maya Angelou\",\n",
            "  \"type\": \"PERSON\",\n",
            "  \"start_char\": 2352,\n",
            "  \"end_char\": 2364\n",
            "}]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"Americans\",\n",
            "  \"type\": \"NORP\",\n",
            "  \"start_char\": 2557,\n",
            "  \"end_char\": 2566\n",
            "}]\n",
            "[]\n",
            "[{\n",
            "  \"text\": \"one\",\n",
            "  \"type\": \"CARDINAL\",\n",
            "  \"start_char\": 2669,\n",
            "  \"end_char\": 2672\n",
            "}, {\n",
            "  \"text\": \"the Supreme Court\",\n",
            "  \"type\": \"ORG\",\n",
            "  \"start_char\": 2710,\n",
            "  \"end_char\": 2727\n",
            "}, {\n",
            "  \"text\": \"the United States\",\n",
            "  \"type\": \"GPE\",\n",
            "  \"start_char\": 2731,\n",
            "  \"end_char\": 2748\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"Court\",\n",
            "  \"type\": \"ORG\",\n",
            "  \"start_char\": 2838,\n",
            "  \"end_char\": 2843\n",
            "}]\n",
            "[{\n",
            "  \"text\": \"Senate\",\n",
            "  \"type\": \"ORG\",\n",
            "  \"start_char\": 3058,\n",
            "  \"end_char\": 3064\n",
            "}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, here's what we found with spaCy:"
      ],
      "metadata": {
        "id": "Utm_H7W8GWmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in kbj_ann_spacy.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mWQq5icGkyP",
        "outputId": "2525bde6-d1dd-4bf4-befe-bcb681d27c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "years 13 18 DATE\n",
            "thousands 192 201 CARDINAL\n",
            "America 461 468 GPE\n",
            "232 years 483 492 DATE\n",
            "115 497 500 CARDINAL\n",
            "the Supreme Court 565 582 ORG\n",
            "the United States 586 603 GPE\n",
            "America 742 749 GPE\n",
            "Americans 1024 1033 NORP\n",
            "America 1161 1168 GPE\n",
            "first 1511 1516 ORDINAL\n",
            "Martin Luther King Jr. 1665 1687 PERSON\n",
            "Thurgood Marshall 1697 1714 PERSON\n",
            "Constance Baker Motley 1747 1769 PERSON\n",
            "this day 1829 1837 DATE\n",
            "first 1989 1994 ORDINAL\n",
            "Maya Angelou 2352 2364 PERSON\n",
            "Americans 2557 2566 NORP\n",
            "the Supreme Court 2710 2727 ORG\n",
            "the United States 2731 2748 GPE\n",
            "Court 2838 2843 ORG\n",
            "Senate 3058 3064 ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNEGBBft-B8z"
      },
      "source": [
        "## NLTK (Natural Language Toolkit)\n",
        "\n",
        "NLTK is the longest established NLP library. It has lots of tools for lots of NLP tasks in lots of languages, including classification, tokenization, stemming, tagging, parsing, semantic reasoning. It interfaces to “over 50 corpora and lexical resources such as WordNet\" (many standard corpora are available directly from NLTK). It is easier to tweak / modify / extend functionality in NLTK than spaCy and there is a large user community, so it is easy to find lots of examples, etc.  There is a free book that serves as most people's entree into NLTK: Steven Bird, Ewan Klein, and Edward Loper. “Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit” updated for Python 3 and NLTK3: http://www.nltk.org/book.\n",
        "\n",
        "NLTK is still widely used, but it is not integrated with neural network / word embedding approaches and definitely not as hip anymore.\n",
        "\n",
        "There isn't a generic \"pipeline\" command that does a default series of sequence labeling tasks, as there is with spaCy and Stanza. You need to download and apply models/resources for different tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ-SG5Kzp1TE",
        "outputId": "af6b7901-a554-4827-8543-be0228e1da6e"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kKipwOX0cA2"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "There are roughly 20 tokenizers available in NLTK. The  generic sounding `word_tokenize` and `sent_tokenize` commands load NLTK's default recommended `punkt` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjjMWnPPrB7L",
        "outputId": "f4acafe8-9e91-44e5-9fbf-e89c59636239"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import word_tokenize, sent_tokenize\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcLirmL4rJHb",
        "outputId": "e96bf5e1-5a74-440e-e9fe-f4e202f32ae3"
      },
      "source": [
        "sent = \"Joel Embiid should have been the 2022 NBA MVP. Instead, Nikola Jokic won the award.\"\n",
        "tok_nltk = word_tokenize(sent)\n",
        "print(tok_nltk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Joel', 'Embiid', 'should', 'have', 'been', 'the', '2022', 'NBA', 'MVP', '.', 'Instead', ',', 'Nikola', 'Jokic', 'won', 'the', 'award', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI6zL9Rq0x9x"
      },
      "source": [
        "### POS tagging\n",
        "\n",
        "There are about a dozen different taggers available in the `nltk.tag` module. The one that seems to be used in most examples is `pos_tag`. It is applied to tokenized text, so we begin to see a pipeline forming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL9aEofPsWIK",
        "outputId": "051ffeb0-ce8e-475f-e4c2-9f684f4a9677"
      },
      "source": [
        "from nltk import pos_tag \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tagged = pos_tag(tok_nltk)                 \n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Joel', 'NNP'), ('Embiid', 'NNP'), ('should', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('the', 'DT'), ('2022', 'CD'), ('NBA', 'NNP'), ('MVP', 'NNP'), ('.', '.'), ('Instead', 'RB'), (',', ','), ('Nikola', 'NNP'), ('Jokic', 'NNP'), ('won', 'VBD'), ('the', 'DT'), ('award', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY71SZl24Z8j"
      },
      "source": [
        "### Named entities\n",
        "\n",
        "In turn, the tagged object can be passed to a named entity \"chunker.\" A chunker divides the tokens into \"chunks\" -- non-overlapping sequences of tokens. This is also known as shallow parsing. The recommended NLTK named entity chunker is accessed through the `ne_chunk` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BmNMmIyKaJO",
        "outputId": "ed5d21a3-5f75-4147-cdbc-c1333022d367"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "named_ents = nltk.ne_chunk(tagged, binary=False)\n",
        "print(named_ents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Joel/NNP)\n",
            "  (PERSON Embiid/NNP)\n",
            "  should/MD\n",
            "  have/VB\n",
            "  been/VBN\n",
            "  the/DT\n",
            "  2022/CD\n",
            "  (ORGANIZATION NBA/NNP)\n",
            "  MVP/NNP\n",
            "  ./.\n",
            "  Instead/RB\n",
            "  ,/,\n",
            "  (PERSON Nikola/NNP Jokic/NNP)\n",
            "  won/VBD\n",
            "  the/DT\n",
            "  award/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuQjVAFuZjXc"
      },
      "source": [
        "It identifies \"Joel\" and \"Embiid\" as two different people, but correctly identifies \"Nikola Jokic\" as a person.  That's not great . The `binary=False` option asks for these classifications into types of named entities -- person, GPE, etc. The default of `binary=True` just returns an indication that something is a named entity.\n",
        "\n",
        "Note that this returns an nltk Tree object, which needs to be traversed in a tree-like way for some purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TGqLERtX0vg"
      },
      "source": [
        "Let's return to the Ketanji Brown Jackson speech example one more time, and create our own pipeline to fit these different pieces together. We will first apply the sentence tokenizer, then the word tokenizer, then the POS tagger, then the named entity chunker. The Tree object output by the chunker has tokens *not* in named entities as leaves of the tree as well. These don't have the named entity label, though, so we'll just barrel through the Tree object brute force, look at every leaf, check for that label and output only those that have it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLfVYDFSXKHQ",
        "outputId": "829bd149-6c70-4f58-a99e-334ea72d8ef4"
      },
      "source": [
        "for sent in nltk.sent_tokenize(kbj):\n",
        "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "      if hasattr(chunk, 'label'):\n",
        "         print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE America\n",
            "ORGANIZATION Supreme Court\n",
            "GPE United States\n",
            "GPE America\n",
            "GPE America\n",
            "PERSON Martin Luther\n",
            "ORGANIZATION Justice Thurgood Marshall\n",
            "ORGANIZATION Supreme Court\n",
            "GPE United States\n",
            "PERSON Mr.\n",
            "ORGANIZATION Senate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOe0c-pLbRYK"
      },
      "source": [
        "This one misses some entities (Maya Angelou) and creates an odd one (\"Mr.\"). It's pretty clearly not working quite as well as the other two pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHXJGAb_6w5h"
      },
      "source": [
        "### Noun phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_TpyYhff3qF"
      },
      "source": [
        "Speaking of noun phrases ... noun phrase chunking in nltk requires you to define a pattern of parts of speech that you consider to be a noun phrase and then parse using regular expressions. There are lots and lots of patterns that folks consider noun phrases. A couple of them are demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUy_SFKadp8v",
        "outputId": "229e3973-3627-4c60-ef44-ba62f16313b3"
      },
      "source": [
        "sent = \"Joel Embiid should have been the 2022 NBA MVP. Instead, Nikola Jokic won the award.\"\n",
        "tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "\n",
        "NPpattern1 = r\"\"\"NP: {(<V\\w+>|<NN\\w?>)+.*<NN\\w?>}\"\"\"\n",
        "chunkParser = nltk.RegexpParser(NPpattern1)\n",
        "chunked_sent = chunkParser.parse(tagged_sent)\n",
        "print(chunked_sent)\n",
        "\n",
        "NPpattern2 = r\"\"\"\n",
        "    NP: {<JJ>*<NN>+}\n",
        "    {<JJ>*<NN><CC>*<NN>+}\n",
        "    \"\"\"\n",
        "chunkParser = nltk.RegexpParser(NPpattern2)\n",
        "chunked_sent = chunkParser.parse(tagged_sent)\n",
        "print(chunked_sent)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP Joel/NNP Embiid/NNP)\n",
            "  should/MD\n",
            "  have/VB\n",
            "  been/VBN\n",
            "  the/DT\n",
            "  2022/CD\n",
            "  (NP NBA/NNP MVP/NNP)\n",
            "  ./.\n",
            "  Instead/RB\n",
            "  ,/,\n",
            "  (NP Nikola/NNP Jokic/NNP)\n",
            "  won/VBD\n",
            "  the/DT\n",
            "  award/NN\n",
            "  ./.)\n",
            "(S\n",
            "  Joel/NNP\n",
            "  Embiid/NNP\n",
            "  should/MD\n",
            "  have/VB\n",
            "  been/VBN\n",
            "  the/DT\n",
            "  2022/CD\n",
            "  NBA/NNP\n",
            "  MVP/NNP\n",
            "  ./.\n",
            "  Instead/RB\n",
            "  ,/,\n",
            "  Nikola/NNP\n",
            "  Jokic/NNP\n",
            "  won/VBD\n",
            "  the/DT\n",
            "  (NP award/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXCKRe7S7_pO"
      },
      "source": [
        "### Dependency parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IJ7LpEWUT8E"
      },
      "source": [
        "Dependency parsing is also a little convoluted in NLTK. Generally, NLTK calls the Stanford CoreNLP dependency parser. But that gets Java involved, which is not something you probably want to do. If for some reason you do, you can check out the documentation: http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.corenlp. (If you just want to use coreNLP, I recommend you just use it through Stanza.) "
      ]
    }
  ]
}