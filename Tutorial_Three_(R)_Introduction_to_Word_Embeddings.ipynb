{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial Three (R): Introduction to Word Embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzo-crippa/3M_NLP_ESS_2022/blob/main/Tutorial_Three_(R)_Introduction_to_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy2ejTX933iI"
      },
      "source": [
        "# Introduction to Word Embeddings\n",
        "\n",
        "## Douglas Rice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls794rAM39Pb"
      },
      "source": [
        "In this notebook, we'll estimate our first word embedding model, then go through a series of analyses of the estimated embeddings. After completing this notebook, you should be familar with:\n",
        "\n",
        "\n",
        "1. Preparing a corpus for estimating word embeddings\n",
        "2. Estimating a (static) word embedding model\n",
        "3. Analyzing output of (static) word embedding model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBIvCScZBn_g"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM5RD3CF7G77"
      },
      "source": [
        "We'll be using the [`text2vec`](http://text2vec.org/index.html) package. `text2vec` was one of the first implementations of  word embeddings functionality in R, and is designed to run *fast*, relatively speaking. Still, it's important to remember that our computational complexity is amping up here, so don't expect immediate results. \n",
        "\n",
        "`text2vec` implements the \"Global Vectors\" (or GloVe) approach for estimating embeddings. Stanford University's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/) is an approach to estimating a distributional representation of a word. GloVe is based, essentially, on factorizing a huge term co-occurrence matrix. \n",
        "\n",
        "The distributional representation of words means that each term is represented as a distribution over some number of dimensions (say, 3 dimensions, where the values are 0.6, 0.3, and 0.1). This stands in stark contrast to the work we've done to this point, which has effectively encoded each word as being effectively just present (1) or not (0). \n",
        "\n",
        "Perhaps unsurprisingly, the distributional representation better captures semantic meaning than the one-hot encoding. This opens up a world of possibilities for us as researchers. Indeed, this has been a major leap forward for research in Text-as-Data. \n",
        "\n",
        "As an example, we can see how similar one word is to other words by measuring the distance between their distributions. Even more interestingly, we can capture really specific phenomena from text with some simple arithmetic based on word distributions. Consider the following canonical example:\n",
        "\n",
        "- <h2> king - man + woman = queen </h2>\n",
        "\n",
        "Ponder this equation for a moment. From the vector representation of  **king**, we subtract the vector representation of **man**. Then, we add the vector representation of **woman**. The end result of that should be a vector that is very similar to the vector representation of  **queen**. \n",
        "\n",
        "In what follows, we'll work through some examples to see how well this works. I want to caution, though, that the models we are training here are probably too small for us to have too much confidence in the trained models. Nevertheless, you'll see that even with this small set we'll recover really interesting dynamics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka390bNZWhQJ"
      },
      "source": [
        "## Front-end Matters\n",
        "\n",
        "First, let's install the `text2vec` package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OerFHJNPWnHF",
        "outputId": "cc36d0df-5aa4-41e4-e862-c0251e103d6d"
      },
      "source": [
        "# Installs text2vec package (might take a while)\n",
        "install.packages('text2vec')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘MatrixExtra’, ‘float’, ‘RhpcBLASctl’, ‘RcppArmadillo’, ‘Rcpp’, ‘rsparse’, ‘mlapi’, ‘lgr’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARQ4BeMWyNi"
      },
      "source": [
        "And load the library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C00d-gpUWz7u"
      },
      "source": [
        "library(text2vec)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8Kb23T9Bmu"
      },
      "source": [
        "## PoKi Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfrjO_nHBySr"
      },
      "source": [
        "We'll be using [PoKi](https://github.com/whipson/PoKi-Poems-by-Kids), a corpus of poems written by children and teenagers from grades 1 to 12.\n",
        "\n",
        "One thing to flag right off the bat is the really interesting dynamics related to *who* is writing these posts. We need to keep in mind that the children writing these texts are going to use less formal writing and more imaginative stories. Given this, we'll focus on analogies that are more appropriate for this context; here, we'll aim to create word embeddings that can recreate these two equations:\n",
        "\n",
        "- <h2> cat - meow + bark = dog </h2>\n",
        "\n",
        "- <h2> mom - girl + boy = dad </h2>\n",
        "\n",
        "By the end, we should hopefully be able to recreate these by creating and fitting our GloVe models. But first, let's perform the necessary pre-processing steps before creating our embedding models. \n",
        "\n",
        "Let's download and read in the data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT7BDwBwCJ5p"
      },
      "source": [
        "# Creates file\n",
        "temp <- tempfile()\n",
        "\n",
        "# Downloads and unzips file to a text8_file variable if it does not exist\n",
        "download.file(\"https://raw.githubusercontent.com/whipson/PoKi-Poems-by-Kids/master/poki.csv\", temp)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "BJ_6aeU0DVwl",
        "outputId": "21db2a6b-6627-4c2f-91c4-9b0098633535"
      },
      "source": [
        "# Reads in downloaded file\n",
        "poem <- read.csv(temp)\n",
        "\n",
        "# First ten rows\n",
        "head(poem, 10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 10 × 6</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>id</th><th scope=col>title</th><th scope=col>author</th><th scope=col>grade</th><th scope=col>text</th><th scope=col>char</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>104987</td><td>I Love The Zoo                  </td><td>            </td><td>1</td><td>roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td> 62</td></tr>\n",
              "\t<tr><th scope=row>2</th><td> 67185</td><td>The scary forest.               </td><td>            </td><td>1</td><td>the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                        </td><td> 87</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>103555</td><td>A Hike At School                </td><td>1st grade-wh</td><td>1</td><td>i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                           </td><td>324</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>112483</td><td>Computer                        </td><td>a           </td><td>1</td><td>you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td>106</td></tr>\n",
              "\t<tr><th scope=row>5</th><td> 74516</td><td>Angel                           </td><td>aab         </td><td>1</td><td>angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                            </td><td>164</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>114693</td><td>Nature Nature and Nature        </td><td>aadhya      </td><td>1</td><td>look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future. </td><td>491</td></tr>\n",
              "\t<tr><th scope=row>7</th><td> 46453</td><td>Jack                            </td><td>aaliyah     </td><td>1</td><td>dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td><td> 74</td></tr>\n",
              "\t<tr><th scope=row>8</th><td> 57397</td><td>When I awoke one morning        </td><td>aanna       </td><td>1</td><td>when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                      </td><td>325</td></tr>\n",
              "\t<tr><th scope=row>9</th><td> 77201</td><td>My Blue Berries and  My Cherries</td><td>aarathi     </td><td>1</td><td>i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                 </td><td>143</td></tr>\n",
              "\t<tr><th scope=row>10</th><td> 40520</td><td>A snowy day                     </td><td>ab.         </td><td>1</td><td>one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                     </td><td>199</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 10 × 6\n\n| <!--/--> | id &lt;int&gt; | title &lt;chr&gt; | author &lt;chr&gt; | grade &lt;int&gt; | text &lt;chr&gt; | char &lt;int&gt; |\n|---|---|---|---|---|---|---|\n| 1 | 104987 | I Love The Zoo                   | <!----> | 1 | roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |  62 |\n| 2 |  67185 | The scary forest.                | <!----> | 1 | the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                         |  87 |\n| 3 | 103555 | A Hike At School                 | 1st grade-wh | 1 | i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                            | 324 |\n| 4 | 112483 | Computer                         | a            | 1 | you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                      | 106 |\n| 5 |  74516 | Angel                            | aab          | 1 | angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                             | 164 |\n| 6 | 114693 | Nature Nature and Nature         | aadhya       | 1 | look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future.  | 491 |\n| 7 |  46453 | Jack                             | aaliyah      | 1 | dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                    |  74 |\n| 8 |  57397 | When I awoke one morning         | aanna        | 1 | when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                       | 325 |\n| 9 |  77201 | My Blue Berries and  My Cherries | aarathi      | 1 | i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                  | 143 |\n| 10 |  40520 | A snowy day                      | ab.          | 1 | one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                      | 199 |\n\n",
            "text/latex": "A data.frame: 10 × 6\n\\begin{tabular}{r|llllll}\n  & id & title & author & grade & text & char\\\\\n  & <int> & <chr> & <chr> & <int> & <chr> & <int>\\\\\n\\hline\n\t1 & 104987 & I Love The Zoo                   &              & 1 & roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &  62\\\\\n\t2 &  67185 & The scary forest.                &              & 1 & the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                         &  87\\\\\n\t3 & 103555 & A Hike At School                 & 1st grade-wh & 1 & i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                            & 324\\\\\n\t4 & 112483 & Computer                         & a            & 1 & you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                      & 106\\\\\n\t5 &  74516 & Angel                            & aab          & 1 & angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                             & 164\\\\\n\t6 & 114693 & Nature Nature and Nature         & aadhya       & 1 & look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future.  & 491\\\\\n\t7 &  46453 & Jack                             & aaliyah      & 1 & dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                    &  74\\\\\n\t8 &  57397 & When I awoke one morning         & aanna        & 1 & when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                       & 325\\\\\n\t9 &  77201 & My Blue Berries and  My Cherries & aarathi      & 1 & i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                  & 143\\\\\n\t10 &  40520 & A snowy day                      & ab.          & 1 & one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                      & 199\\\\\n\\end{tabular}\n",
            "text/plain": [
              "   id     title                            author       grade\n",
              "1  104987 I Love The Zoo                                1    \n",
              "2   67185 The scary forest.                             1    \n",
              "3  103555 A Hike At School                 1st grade-wh 1    \n",
              "4  112483 Computer                         a            1    \n",
              "5   74516 Angel                            aab          1    \n",
              "6  114693 Nature Nature and Nature         aadhya       1    \n",
              "7   46453 Jack                             aaliyah      1    \n",
              "8   57397 When I awoke one morning         aanna        1    \n",
              "9   77201 My Blue Berries and  My Cherries aarathi      1    \n",
              "10  40520 A snowy day                      ab.          1    \n",
              "   text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
              "1  roses are red,  violets are blue.   i love the zoo.   do you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
              "2  the forest is really haunted.  i believe it to be so.  but then we are going camping.                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
              "3  i took a hike at school today  and this is what i saw     bouncing balls      girls chatting against the walls     kids climbing on monkey bars     i even saw some teachers' cars     the wind was blowing my hair in my face     i saw a mud puddle,  but just a trace all of these things i noticed just now on my little hike.                                                                                                                                                                                           \n",
              "4  you  can  do  what  you  want  you  can play a  game    you can do many things,   you can read and write                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
              "5  angel oh angle you spin like a top angel oh angel you will never stop can't you feel the air  as it blows through your hair angel oh angel itisto bad your a mop!                                                                                                                                                                                                                                                                                                                                                            \n",
              "6  look at the sun, what a beautiful day.  under the trees, we can run and play.  beauty of nature, we love to see,  from tiny insect to exotic tree.  it is a place to sit and think,  nature and human share the deepest link.  nature has ocean, which is in motion.  nature has tree, nature has river.  if we destroy the nature we would never be free.  our nature keeps us alive,  we must protect it, for society to thrive.  we spoil the nature, we spoil the future.  go along with nature, for your better future. \n",
              "7  dog  playful,  energetic running,  jumping,  tackling my is my friend jack                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "8  when i awoke one morning,  a dog was on my  head.  i asked ,  ''what are you doing there?' it looked at me and said  ''woof!'' ''wouldn't you like to be outside playing?''said the man ''i'm staying here and playing here. '' said the dog he played all night and day. he came inside his new house and played inside a wet wet day.                                                                                                                                                                                      \n",
              "9  i went to my blue berry tree they were no blue berries found i went to another tree to get some more free but found none but cherries round.                                                                                                                                                                                                                                                                                                                                                                                 \n",
              "10 one snowy day the children went outside to play in the snow.  they threw snowballs,  went sledding and made a snowman.  afterwards they went inside to drink warm hot chocolate.  it was a fun snowy day                                                                                                                                                                                                                                                                                                                     \n",
              "   char\n",
              "1   62 \n",
              "2   87 \n",
              "3  324 \n",
              "4  106 \n",
              "5  164 \n",
              "6  491 \n",
              "7   74 \n",
              "8  325 \n",
              "9  143 \n",
              "10 199 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HPpZMH5kXnsd",
        "outputId": "b2d74160-330e-48e1-d491-5043685f4198"
      },
      "source": [
        "# Checks dimensions\n",
        "dim(poem) # 61,508 poems"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>61508</li><li>6</li></ol>\n"
            ],
            "text/markdown": "1. 61508\n2. 6\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 61508\n\\item 6\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 61508     6"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_flemCsX4L7"
      },
      "source": [
        "We want the poems themselves, so we'll use the column 'text' for tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMhYOwTu866T"
      },
      "source": [
        "## Tokenization and Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfqH5NO2I7CB"
      },
      "source": [
        "We start with `text2vec` by creating a tokenized iterator and vectorized vocabulary first. This time, there's no need to lowercase our words since the downloaded dataset is already lowercased.\n",
        "\n",
        "Let's tokenize the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "HgpMIqt2JPxL",
        "outputId": "f9ddeea9-9937-40c7-91bb-b46b76ff3158"
      },
      "source": [
        "# Tokenization\n",
        "tokens <- word_tokenizer(poem$text)\n",
        "\n",
        "# First five rows tokenized\n",
        "head(tokens, 5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<ol>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'roses'</li><li>'are'</li><li>'red'</li><li>'violets'</li><li>'are'</li><li>'blue'</li><li>'i'</li><li>'love'</li><li>'the'</li><li>'zoo'</li><li>'do'</li><li>'you'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'the'</li><li>'forest'</li><li>'is'</li><li>'really'</li><li>'haunted'</li><li>'i'</li><li>'believe'</li><li>'it'</li><li>'to'</li><li>'be'</li><li>'so'</li><li>'but'</li><li>'then'</li><li>'we'</li><li>'are'</li><li>'going'</li><li>'camping'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'i'</li><li>'took'</li><li>'a'</li><li>'hike'</li><li>'at'</li><li>'school'</li><li>'today'</li><li>'and'</li><li>'this'</li><li>'is'</li><li>'what'</li><li>'i'</li><li>'saw'</li><li>'bouncing'</li><li>'balls'</li><li>'girls'</li><li>'chatting'</li><li>'against'</li><li>'the'</li><li>'walls'</li><li>'kids'</li><li>'climbing'</li><li>'on'</li><li>'monkey'</li><li>'bars'</li><li>'i'</li><li>'even'</li><li>'saw'</li><li>'some'</li><li>'teachers'</li><li>'cars'</li><li>'the'</li><li>'wind'</li><li>'was'</li><li>'blowing'</li><li>'my'</li><li>'hair'</li><li>'in'</li><li>'my'</li><li>'face'</li><li>'i'</li><li>'saw'</li><li>'a'</li><li>'mud'</li><li>'puddle'</li><li>'but'</li><li>'just'</li><li>'a'</li><li>'trace'</li><li>'all'</li><li>'of'</li><li>'these'</li><li>'things'</li><li>'i'</li><li>'noticed'</li><li>'just'</li><li>'now'</li><li>'on'</li><li>'my'</li><li>'little'</li><li>'hike'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'you'</li><li>'can'</li><li>'do'</li><li>'what'</li><li>'you'</li><li>'want'</li><li>'you'</li><li>'can'</li><li>'play'</li><li>'a'</li><li>'game'</li><li>'you'</li><li>'can'</li><li>'do'</li><li>'many'</li><li>'things'</li><li>'you'</li><li>'can'</li><li>'read'</li><li>'and'</li><li>'write'</li></ol>\n",
              "</li>\n",
              "\t<li><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'angel'</li><li>'oh'</li><li>'angle'</li><li>'you'</li><li>'spin'</li><li>'like'</li><li>'a'</li><li>'top'</li><li>'angel'</li><li>'oh'</li><li>'angel'</li><li>'you'</li><li>'will'</li><li>'never'</li><li>'stop'</li><li>'can\\'t'</li><li>'you'</li><li>'feel'</li><li>'the'</li><li>'air'</li><li>'as'</li><li>'it'</li><li>'blows'</li><li>'through'</li><li>'your'</li><li>'hair'</li><li>'angel'</li><li>'oh'</li><li>'angel'</li><li>'itisto'</li><li>'bad'</li><li>'your'</li><li>'a'</li><li>'mop'</li></ol>\n",
              "</li>\n",
              "</ol>\n"
            ],
            "text/markdown": "1. 1. 'roses'\n2. 'are'\n3. 'red'\n4. 'violets'\n5. 'are'\n6. 'blue'\n7. 'i'\n8. 'love'\n9. 'the'\n10. 'zoo'\n11. 'do'\n12. 'you'\n\n\n\n2. 1. 'the'\n2. 'forest'\n3. 'is'\n4. 'really'\n5. 'haunted'\n6. 'i'\n7. 'believe'\n8. 'it'\n9. 'to'\n10. 'be'\n11. 'so'\n12. 'but'\n13. 'then'\n14. 'we'\n15. 'are'\n16. 'going'\n17. 'camping'\n\n\n\n3. 1. 'i'\n2. 'took'\n3. 'a'\n4. 'hike'\n5. 'at'\n6. 'school'\n7. 'today'\n8. 'and'\n9. 'this'\n10. 'is'\n11. 'what'\n12. 'i'\n13. 'saw'\n14. 'bouncing'\n15. 'balls'\n16. 'girls'\n17. 'chatting'\n18. 'against'\n19. 'the'\n20. 'walls'\n21. 'kids'\n22. 'climbing'\n23. 'on'\n24. 'monkey'\n25. 'bars'\n26. 'i'\n27. 'even'\n28. 'saw'\n29. 'some'\n30. 'teachers'\n31. 'cars'\n32. 'the'\n33. 'wind'\n34. 'was'\n35. 'blowing'\n36. 'my'\n37. 'hair'\n38. 'in'\n39. 'my'\n40. 'face'\n41. 'i'\n42. 'saw'\n43. 'a'\n44. 'mud'\n45. 'puddle'\n46. 'but'\n47. 'just'\n48. 'a'\n49. 'trace'\n50. 'all'\n51. 'of'\n52. 'these'\n53. 'things'\n54. 'i'\n55. 'noticed'\n56. 'just'\n57. 'now'\n58. 'on'\n59. 'my'\n60. 'little'\n61. 'hike'\n\n\n\n4. 1. 'you'\n2. 'can'\n3. 'do'\n4. 'what'\n5. 'you'\n6. 'want'\n7. 'you'\n8. 'can'\n9. 'play'\n10. 'a'\n11. 'game'\n12. 'you'\n13. 'can'\n14. 'do'\n15. 'many'\n16. 'things'\n17. 'you'\n18. 'can'\n19. 'read'\n20. 'and'\n21. 'write'\n\n\n\n5. 1. 'angel'\n2. 'oh'\n3. 'angle'\n4. 'you'\n5. 'spin'\n6. 'like'\n7. 'a'\n8. 'top'\n9. 'angel'\n10. 'oh'\n11. 'angel'\n12. 'you'\n13. 'will'\n14. 'never'\n15. 'stop'\n16. 'can\\'t'\n17. 'you'\n18. 'feel'\n19. 'the'\n20. 'air'\n21. 'as'\n22. 'it'\n23. 'blows'\n24. 'through'\n25. 'your'\n26. 'hair'\n27. 'angel'\n28. 'oh'\n29. 'angel'\n30. 'itisto'\n31. 'bad'\n32. 'your'\n33. 'a'\n34. 'mop'\n\n\n\n\n\n",
            "text/latex": "\\begin{enumerate}\n\\item \\begin{enumerate*}\n\\item 'roses'\n\\item 'are'\n\\item 'red'\n\\item 'violets'\n\\item 'are'\n\\item 'blue'\n\\item 'i'\n\\item 'love'\n\\item 'the'\n\\item 'zoo'\n\\item 'do'\n\\item 'you'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'the'\n\\item 'forest'\n\\item 'is'\n\\item 'really'\n\\item 'haunted'\n\\item 'i'\n\\item 'believe'\n\\item 'it'\n\\item 'to'\n\\item 'be'\n\\item 'so'\n\\item 'but'\n\\item 'then'\n\\item 'we'\n\\item 'are'\n\\item 'going'\n\\item 'camping'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'i'\n\\item 'took'\n\\item 'a'\n\\item 'hike'\n\\item 'at'\n\\item 'school'\n\\item 'today'\n\\item 'and'\n\\item 'this'\n\\item 'is'\n\\item 'what'\n\\item 'i'\n\\item 'saw'\n\\item 'bouncing'\n\\item 'balls'\n\\item 'girls'\n\\item 'chatting'\n\\item 'against'\n\\item 'the'\n\\item 'walls'\n\\item 'kids'\n\\item 'climbing'\n\\item 'on'\n\\item 'monkey'\n\\item 'bars'\n\\item 'i'\n\\item 'even'\n\\item 'saw'\n\\item 'some'\n\\item 'teachers'\n\\item 'cars'\n\\item 'the'\n\\item 'wind'\n\\item 'was'\n\\item 'blowing'\n\\item 'my'\n\\item 'hair'\n\\item 'in'\n\\item 'my'\n\\item 'face'\n\\item 'i'\n\\item 'saw'\n\\item 'a'\n\\item 'mud'\n\\item 'puddle'\n\\item 'but'\n\\item 'just'\n\\item 'a'\n\\item 'trace'\n\\item 'all'\n\\item 'of'\n\\item 'these'\n\\item 'things'\n\\item 'i'\n\\item 'noticed'\n\\item 'just'\n\\item 'now'\n\\item 'on'\n\\item 'my'\n\\item 'little'\n\\item 'hike'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'you'\n\\item 'can'\n\\item 'do'\n\\item 'what'\n\\item 'you'\n\\item 'want'\n\\item 'you'\n\\item 'can'\n\\item 'play'\n\\item 'a'\n\\item 'game'\n\\item 'you'\n\\item 'can'\n\\item 'do'\n\\item 'many'\n\\item 'things'\n\\item 'you'\n\\item 'can'\n\\item 'read'\n\\item 'and'\n\\item 'write'\n\\end{enumerate*}\n\n\\item \\begin{enumerate*}\n\\item 'angel'\n\\item 'oh'\n\\item 'angle'\n\\item 'you'\n\\item 'spin'\n\\item 'like'\n\\item 'a'\n\\item 'top'\n\\item 'angel'\n\\item 'oh'\n\\item 'angel'\n\\item 'you'\n\\item 'will'\n\\item 'never'\n\\item 'stop'\n\\item 'can\\textbackslash{}'t'\n\\item 'you'\n\\item 'feel'\n\\item 'the'\n\\item 'air'\n\\item 'as'\n\\item 'it'\n\\item 'blows'\n\\item 'through'\n\\item 'your'\n\\item 'hair'\n\\item 'angel'\n\\item 'oh'\n\\item 'angel'\n\\item 'itisto'\n\\item 'bad'\n\\item 'your'\n\\item 'a'\n\\item 'mop'\n\\end{enumerate*}\n\n\\end{enumerate}\n",
            "text/plain": [
              "[[1]]\n",
              " [1] \"roses\"   \"are\"     \"red\"     \"violets\" \"are\"     \"blue\"    \"i\"      \n",
              " [8] \"love\"    \"the\"     \"zoo\"     \"do\"      \"you\"    \n",
              "\n",
              "[[2]]\n",
              " [1] \"the\"     \"forest\"  \"is\"      \"really\"  \"haunted\" \"i\"       \"believe\"\n",
              " [8] \"it\"      \"to\"      \"be\"      \"so\"      \"but\"     \"then\"    \"we\"     \n",
              "[15] \"are\"     \"going\"   \"camping\"\n",
              "\n",
              "[[3]]\n",
              " [1] \"i\"        \"took\"     \"a\"        \"hike\"     \"at\"       \"school\"  \n",
              " [7] \"today\"    \"and\"      \"this\"     \"is\"       \"what\"     \"i\"       \n",
              "[13] \"saw\"      \"bouncing\" \"balls\"    \"girls\"    \"chatting\" \"against\" \n",
              "[19] \"the\"      \"walls\"    \"kids\"     \"climbing\" \"on\"       \"monkey\"  \n",
              "[25] \"bars\"     \"i\"        \"even\"     \"saw\"      \"some\"     \"teachers\"\n",
              "[31] \"cars\"     \"the\"      \"wind\"     \"was\"      \"blowing\"  \"my\"      \n",
              "[37] \"hair\"     \"in\"       \"my\"       \"face\"     \"i\"        \"saw\"     \n",
              "[43] \"a\"        \"mud\"      \"puddle\"   \"but\"      \"just\"     \"a\"       \n",
              "[49] \"trace\"    \"all\"      \"of\"       \"these\"    \"things\"   \"i\"       \n",
              "[55] \"noticed\"  \"just\"     \"now\"      \"on\"       \"my\"       \"little\"  \n",
              "[61] \"hike\"    \n",
              "\n",
              "[[4]]\n",
              " [1] \"you\"    \"can\"    \"do\"     \"what\"   \"you\"    \"want\"   \"you\"    \"can\"   \n",
              " [9] \"play\"   \"a\"      \"game\"   \"you\"    \"can\"    \"do\"     \"many\"   \"things\"\n",
              "[17] \"you\"    \"can\"    \"read\"   \"and\"    \"write\" \n",
              "\n",
              "[[5]]\n",
              " [1] \"angel\"   \"oh\"      \"angle\"   \"you\"     \"spin\"    \"like\"    \"a\"      \n",
              " [8] \"top\"     \"angel\"   \"oh\"      \"angel\"   \"you\"     \"will\"    \"never\"  \n",
              "[15] \"stop\"    \"can't\"   \"you\"     \"feel\"    \"the\"     \"air\"     \"as\"     \n",
              "[22] \"it\"      \"blows\"   \"through\" \"your\"    \"hair\"    \"angel\"   \"oh\"     \n",
              "[29] \"angel\"   \"itisto\"  \"bad\"     \"your\"    \"a\"       \"mop\"    \n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-hQjA1-YXsO"
      },
      "source": [
        "Create an iterator object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKLUOG6xYZo1",
        "outputId": "fe7e913e-9c20-4bfb-af50-001f6a6cef35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Create iterator object \n",
        "it <- itoken(tokens, progressbar = FALSE)\n",
        "it"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<itoken>\n",
              "  Inherits from: <CallbackIterator>\n",
              "  Public:\n",
              "    callback: function (x) \n",
              "    clone: function (deep = FALSE) \n",
              "    initialize: function (x, callback = identity) \n",
              "    is_complete: active binding\n",
              "    length: active binding\n",
              "    move_cursor: function () \n",
              "    nextElem: function () \n",
              "    x: GenericIterator, iterator, R6"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw_phDKOY7P5"
      },
      "source": [
        "Build the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#?create_vocabulary"
      ],
      "metadata": {
        "id": "IgqdbgYkgn6_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyIXeVobZGz4",
        "outputId": "b62fc6be-facd-4316-9eb4-770c7a299266"
      },
      "source": [
        "# Build vocabulary\n",
        "vocab <- create_vocabulary(it)\n",
        "\n",
        "# Vocabulary\n",
        "vocab # we have term, count for the terms overall, and the number of documents it occurs in"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A text2vec_vocabulary: 56474 × 3</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>term</th><th scope=col>term_count</th><th scope=col>doc_count</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>0000     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0000000  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0000001  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00a:m    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00he     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00o'clock</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>00p      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>02       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>04       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>05at     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>05â’n    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>080      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0f       </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0ften    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0min     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0nce     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0ne      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0sec     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>0â’colck </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>1'000'000</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>10000    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100000   </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>1000000  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100k     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100miles </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100s     </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>100th    </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>103      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>104      </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>10foot2  </td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td>your</td><td> 14900</td><td> 8083</td></tr>\n",
              "\t<tr><td>she </td><td> 14949</td><td> 5126</td></tr>\n",
              "\t<tr><td>love</td><td> 15086</td><td> 8373</td></tr>\n",
              "\t<tr><td>they</td><td> 16807</td><td> 8281</td></tr>\n",
              "\t<tr><td>all </td><td> 17526</td><td>12199</td></tr>\n",
              "\t<tr><td>as  </td><td> 17526</td><td> 8394</td></tr>\n",
              "\t<tr><td>for </td><td> 17641</td><td>11241</td></tr>\n",
              "\t<tr><td>be  </td><td> 18190</td><td>11224</td></tr>\n",
              "\t<tr><td>when</td><td> 19313</td><td>12871</td></tr>\n",
              "\t<tr><td>with</td><td> 19938</td><td>13445</td></tr>\n",
              "\t<tr><td>so  </td><td> 21107</td><td>13870</td></tr>\n",
              "\t<tr><td>on  </td><td> 21518</td><td>14943</td></tr>\n",
              "\t<tr><td>was </td><td> 21549</td><td>10842</td></tr>\n",
              "\t<tr><td>but </td><td> 22095</td><td>14606</td></tr>\n",
              "\t<tr><td>he  </td><td> 23323</td><td> 8521</td></tr>\n",
              "\t<tr><td>like</td><td> 24288</td><td>14832</td></tr>\n",
              "\t<tr><td>that</td><td> 24999</td><td>14575</td></tr>\n",
              "\t<tr><td>are </td><td> 26681</td><td>14887</td></tr>\n",
              "\t<tr><td>me  </td><td> 29851</td><td>15628</td></tr>\n",
              "\t<tr><td>of  </td><td> 34025</td><td>18832</td></tr>\n",
              "\t<tr><td>in  </td><td> 37990</td><td>22586</td></tr>\n",
              "\t<tr><td>it  </td><td> 39444</td><td>19604</td></tr>\n",
              "\t<tr><td>you </td><td> 58337</td><td>19627</td></tr>\n",
              "\t<tr><td>my  </td><td> 58604</td><td>24914</td></tr>\n",
              "\t<tr><td>is  </td><td> 58608</td><td>27119</td></tr>\n",
              "\t<tr><td>to  </td><td> 69175</td><td>30347</td></tr>\n",
              "\t<tr><td>and </td><td> 80863</td><td>34798</td></tr>\n",
              "\t<tr><td>a   </td><td> 92765</td><td>37607</td></tr>\n",
              "\t<tr><td>the </td><td>120677</td><td>37676</td></tr>\n",
              "\t<tr><td>i   </td><td>124832</td><td>32777</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA text2vec_vocabulary: 56474 × 3\n\n| term &lt;chr&gt; | term_count &lt;int&gt; | doc_count &lt;int&gt; |\n|---|---|---|\n| 0000      | 1 | 1 |\n| 0000000   | 1 | 1 |\n| 0000001   | 1 | 1 |\n| 00a:m     | 1 | 1 |\n| 00he      | 1 | 1 |\n| 00o'clock | 1 | 1 |\n| 00p       | 1 | 1 |\n| 02        | 1 | 1 |\n| 04        | 1 | 1 |\n| 05at      | 1 | 1 |\n| 05â’n     | 1 | 1 |\n| 080       | 1 | 1 |\n| 0f        | 1 | 1 |\n| 0ften     | 1 | 1 |\n| 0min      | 1 | 1 |\n| 0nce      | 1 | 1 |\n| 0ne       | 1 | 1 |\n| 0sec      | 1 | 1 |\n| 0â’colck  | 1 | 1 |\n| 1'000'000 | 1 | 1 |\n| 10000     | 1 | 1 |\n| 100000    | 1 | 1 |\n| 1000000   | 1 | 1 |\n| 100k      | 1 | 1 |\n| 100miles  | 1 | 1 |\n| 100s      | 1 | 1 |\n| 100th     | 1 | 1 |\n| 103       | 1 | 1 |\n| 104       | 1 | 1 |\n| 10foot2   | 1 | 1 |\n| ⋮ | ⋮ | ⋮ |\n| your |  14900 |  8083 |\n| she  |  14949 |  5126 |\n| love |  15086 |  8373 |\n| they |  16807 |  8281 |\n| all  |  17526 | 12199 |\n| as   |  17526 |  8394 |\n| for  |  17641 | 11241 |\n| be   |  18190 | 11224 |\n| when |  19313 | 12871 |\n| with |  19938 | 13445 |\n| so   |  21107 | 13870 |\n| on   |  21518 | 14943 |\n| was  |  21549 | 10842 |\n| but  |  22095 | 14606 |\n| he   |  23323 |  8521 |\n| like |  24288 | 14832 |\n| that |  24999 | 14575 |\n| are  |  26681 | 14887 |\n| me   |  29851 | 15628 |\n| of   |  34025 | 18832 |\n| in   |  37990 | 22586 |\n| it   |  39444 | 19604 |\n| you  |  58337 | 19627 |\n| my   |  58604 | 24914 |\n| is   |  58608 | 27119 |\n| to   |  69175 | 30347 |\n| and  |  80863 | 34798 |\n| a    |  92765 | 37607 |\n| the  | 120677 | 37676 |\n| i    | 124832 | 32777 |\n\n",
            "text/latex": "A text2vec\\_vocabulary: 56474 × 3\n\\begin{tabular}{lll}\n term & term\\_count & doc\\_count\\\\\n <chr> & <int> & <int>\\\\\n\\hline\n\t 0000      & 1 & 1\\\\\n\t 0000000   & 1 & 1\\\\\n\t 0000001   & 1 & 1\\\\\n\t 00a:m     & 1 & 1\\\\\n\t 00he      & 1 & 1\\\\\n\t 00o'clock & 1 & 1\\\\\n\t 00p       & 1 & 1\\\\\n\t 02        & 1 & 1\\\\\n\t 04        & 1 & 1\\\\\n\t 05at      & 1 & 1\\\\\n\t 05â’n     & 1 & 1\\\\\n\t 080       & 1 & 1\\\\\n\t 0f        & 1 & 1\\\\\n\t 0ften     & 1 & 1\\\\\n\t 0min      & 1 & 1\\\\\n\t 0nce      & 1 & 1\\\\\n\t 0ne       & 1 & 1\\\\\n\t 0sec      & 1 & 1\\\\\n\t 0â’colck  & 1 & 1\\\\\n\t 1'000'000 & 1 & 1\\\\\n\t 10000     & 1 & 1\\\\\n\t 100000    & 1 & 1\\\\\n\t 1000000   & 1 & 1\\\\\n\t 100k      & 1 & 1\\\\\n\t 100miles  & 1 & 1\\\\\n\t 100s      & 1 & 1\\\\\n\t 100th     & 1 & 1\\\\\n\t 103       & 1 & 1\\\\\n\t 104       & 1 & 1\\\\\n\t 10foot2   & 1 & 1\\\\\n\t ⋮ & ⋮ & ⋮\\\\\n\t your &  14900 &  8083\\\\\n\t she  &  14949 &  5126\\\\\n\t love &  15086 &  8373\\\\\n\t they &  16807 &  8281\\\\\n\t all  &  17526 & 12199\\\\\n\t as   &  17526 &  8394\\\\\n\t for  &  17641 & 11241\\\\\n\t be   &  18190 & 11224\\\\\n\t when &  19313 & 12871\\\\\n\t with &  19938 & 13445\\\\\n\t so   &  21107 & 13870\\\\\n\t on   &  21518 & 14943\\\\\n\t was  &  21549 & 10842\\\\\n\t but  &  22095 & 14606\\\\\n\t he   &  23323 &  8521\\\\\n\t like &  24288 & 14832\\\\\n\t that &  24999 & 14575\\\\\n\t are  &  26681 & 14887\\\\\n\t me   &  29851 & 15628\\\\\n\t of   &  34025 & 18832\\\\\n\t in   &  37990 & 22586\\\\\n\t it   &  39444 & 19604\\\\\n\t you  &  58337 & 19627\\\\\n\t my   &  58604 & 24914\\\\\n\t is   &  58608 & 27119\\\\\n\t to   &  69175 & 30347\\\\\n\t and  &  80863 & 34798\\\\\n\t a    &  92765 & 37607\\\\\n\t the  & 120677 & 37676\\\\\n\t i    & 124832 & 32777\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      term      term_count doc_count\n",
              "1     0000      1          1        \n",
              "2     0000000   1          1        \n",
              "3     0000001   1          1        \n",
              "4     00a:m     1          1        \n",
              "5     00he      1          1        \n",
              "6     00o'clock 1          1        \n",
              "7     00p       1          1        \n",
              "8     02        1          1        \n",
              "9     04        1          1        \n",
              "10    05at      1          1        \n",
              "11    05â’n     1          1        \n",
              "12    080       1          1        \n",
              "13    0f        1          1        \n",
              "14    0ften     1          1        \n",
              "15    0min      1          1        \n",
              "16    0nce      1          1        \n",
              "17    0ne       1          1        \n",
              "18    0sec      1          1        \n",
              "19    0â’colck  1          1        \n",
              "20    1'000'000 1          1        \n",
              "21    10000     1          1        \n",
              "22    100000    1          1        \n",
              "23    1000000   1          1        \n",
              "24    100k      1          1        \n",
              "25    100miles  1          1        \n",
              "26    100s      1          1        \n",
              "27    100th     1          1        \n",
              "28    103       1          1        \n",
              "29    104       1          1        \n",
              "30    10foot2   1          1        \n",
              "⋮     ⋮         ⋮          ⋮        \n",
              "56445 your       14900      8083    \n",
              "56446 she        14949      5126    \n",
              "56447 love       15086      8373    \n",
              "56448 they       16807      8281    \n",
              "56449 all        17526     12199    \n",
              "56450 as         17526      8394    \n",
              "56451 for        17641     11241    \n",
              "56452 be         18190     11224    \n",
              "56453 when       19313     12871    \n",
              "56454 with       19938     13445    \n",
              "56455 so         21107     13870    \n",
              "56456 on         21518     14943    \n",
              "56457 was        21549     10842    \n",
              "56458 but        22095     14606    \n",
              "56459 he         23323      8521    \n",
              "56460 like       24288     14832    \n",
              "56461 that       24999     14575    \n",
              "56462 are        26681     14887    \n",
              "56463 me         29851     15628    \n",
              "56464 of         34025     18832    \n",
              "56465 in         37990     22586    \n",
              "56466 it         39444     19604    \n",
              "56467 you        58337     19627    \n",
              "56468 my         58604     24914    \n",
              "56469 is         58608     27119    \n",
              "56470 to         69175     30347    \n",
              "56471 and        80863     34798    \n",
              "56472 a          92765     37607    \n",
              "56473 the       120677     37676    \n",
              "56474 i         124832     32777    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "w9fpfk4WZYrM",
        "outputId": "f6a2c753-fde9-4d6a-d4ac-b13e068000f8"
      },
      "source": [
        "# Check dimensions\n",
        "dim(vocab) # the vocabulary has 56,474 types. We have word count and document counts"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>56474</li><li>3</li></ol>\n"
            ],
            "text/markdown": "1. 56474\n2. 3\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 56474\n\\item 3\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 56474     3"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qye3rnkZIVT"
      },
      "source": [
        "And prune and vectorize it. We'll keep the terms that occur at least 5 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hgLAjnr-ZLAR",
        "outputId": "4ecd9fb4-df0a-4ed9-f347-e427eb3a77be"
      },
      "source": [
        "# Prune vocabulary\n",
        "vocab <- prune_vocabulary(vocab, term_count_min = 5)\n",
        "\n",
        "# Check dimensions\n",
        "dim(vocab) # many many fewer words: 14k, about\n",
        "\n",
        "# Vectorize\n",
        "vectorizer <- vocab_vectorizer(vocab) # vectorizer of the vocabulary that we care about"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>14267</li><li>3</li></ol>\n"
            ],
            "text/markdown": "1. 14267\n2. 3\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 14267\n\\item 3\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 14267     3"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMpN2L5IZseV"
      },
      "source": [
        "As we can see, pruning our vocabulary deleted over 40 thousand words. I want to reiterate that this is a *very small* corpus from the perspective of traditional word embedding models. When we are working with word representations trained with these smaller corpora, we should be really cautious in our approach. \n",
        "\n",
        "Moving on, we can create out term-co-occurence matrix (TCM). We can achieve different results by experimenting with the `skip_grams_window` and other parameters. The definition of whether two words occur together is arbitrary, so we definitely want to play around with the parameters to see the different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI6x2HUXaDUp"
      },
      "source": [
        "# use window of 5 for context words\n",
        "tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L) \n",
        "# so we need: the iterator we created earlier (notice: unpruned at this point), \n",
        "# the vectorizer (defining the *pruned* vocabulary we care about), and the\n",
        "# size of the skip-grams window we want (in this case, 5 words)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugs_AQsF1BEy"
      },
      "source": [
        "## Creating and fitting the GloVe model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz9ajgpRaJ3j"
      },
      "source": [
        "Now we have a TCM matrix and can factorize it via the GloVe algorithm. We'll use the method `$new` to `GlobalVectors` to create our GloVe model. \n",
        "\n",
        "[Here](https://www.rdocumentation.org/packages/text2vec/versions/0.5.0/topics/GlobalVectors) is documentation for related functions and methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "0VG8YLfWaSuj",
        "outputId": "c7083bd8-00f0-497e-ba05-1dd6b5613732"
      },
      "source": [
        "# Creating new GloVe model\n",
        "glove <- GlobalVectors$new(rank = 50, x_max = 10) # rank = 50 determines the number of dimension of the final model (number of weights per word)\n",
        "# notice that this function works in an object-oriented fashion. We take the attribute new() from GlobalVectors\n",
        "\n",
        "# Checking GloVe methods\n",
        "glove"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<GloVe>\n",
              "  Public:\n",
              "    bias_i: NULL\n",
              "    bias_j: NULL\n",
              "    clone: function (deep = FALSE) \n",
              "    components: NULL\n",
              "    fit_transform: function (x, n_iter = 10L, convergence_tol = -1, n_threads = getOption(\"rsparse_omp_threads\", \n",
              "    get_history: function () \n",
              "    initialize: function (rank, x_max, learning_rate = 0.15, alpha = 0.75, lambda = 0, \n",
              "    shuffle: FALSE\n",
              "  Private:\n",
              "    alpha: 0.75\n",
              "    b_i: NULL\n",
              "    b_j: NULL\n",
              "    cost_history: \n",
              "    fitted: FALSE\n",
              "    glove_fitter: NULL\n",
              "    initial: NULL\n",
              "    lambda: 0\n",
              "    learning_rate: 0.15\n",
              "    rank: 50\n",
              "    w_i: NULL\n",
              "    w_j: NULL\n",
              "    x_max: 10"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SJK_pqTa29w"
      },
      "source": [
        "Note that you'll only be able to access the public methods.\n",
        "\n",
        "We can fit our model using `$fit_transform` to our `glove` variable. This may take several minutes to fit! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfTV2TWxbF_0",
        "outputId": "182e7f4d-06f4-4eba-f847-18a4301aab4b"
      },
      "source": [
        "# Fitting model\n",
        "wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)\n",
        "# advices on default options:\n",
        "# n_iter:\n",
        "# convergence_tol:\n",
        "# n_threads: "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO  [16:20:26.670] epoch 1, loss 0.1998 \n",
            "INFO  [16:20:29.406] epoch 2, loss 0.1303 \n",
            "INFO  [16:20:32.127] epoch 3, loss 0.1127 \n",
            "INFO  [16:20:34.836] epoch 4, loss 0.1018 \n",
            "INFO  [16:20:37.473] epoch 5, loss 0.0944 \n",
            "INFO  [16:20:40.122] epoch 6, loss 0.0889 \n",
            "INFO  [16:20:42.753] epoch 7, loss 0.0848 \n",
            "INFO  [16:20:45.371] epoch 8, loss 0.0816 \n",
            "INFO  [16:20:48.145] epoch 9, loss 0.0790 \n",
            "INFO  [16:20:50.908] epoch 10, loss 0.0768 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Yir-NHHXbPkU",
        "outputId": "26558949-52ae-4741-f64b-5dd42bc75db9"
      },
      "source": [
        "# Checking dimensions\n",
        "dim(wv_main)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>14267</li><li>50</li></ol>\n"
            ],
            "text/markdown": "1. 14267\n2. 50\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 14267\n\\item 50\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 14267    50"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1-Vml6-bWkv"
      },
      "source": [
        "Note that model learns *two* sets of word vectors - **target** and **context**. We can think of our word of interest as the target in this environment, and all the other words as the context inside the window. For both, word vectors are learned. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6q2rxO3ObaIU",
        "outputId": "bab457e8-028c-4c23-c780-1d16ea8d0a9b"
      },
      "source": [
        "wv_context <- glove$components\n",
        "dim(wv_context)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>50</li><li>14267</li></ol>\n"
            ],
            "text/markdown": "1. 50\n2. 14267\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 50\n\\item 14267\n\\end{enumerate*}\n",
            "text/plain": [
              "[1]    50 14267"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1MguP3biAE"
      },
      "source": [
        "While both of word-vectors matrices can be used as result, the creators recommends to average or take a sum of main and context vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouOY85abilR"
      },
      "source": [
        "word_vectors <- wv_main + t(wv_context)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFa079_Nyfwi"
      },
      "source": [
        "Here's a preview of the word vector matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hu2BV-Unbn--",
        "outputId": "3610b739-df27-42e5-856f-e80e9f6a052f"
      },
      "source": [
        "word_vectors"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 14267 × 50 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1837</th><td>-0.63303339</td><td> 0.30040185</td><td>-0.16941897</td><td> 0.19313955</td><td> 0.33992052</td><td> 0.10447346</td><td>-0.61349224</td><td>-0.51546688</td><td> 1.143208193</td><td> 0.50096603</td><td>⋯</td><td>-0.207672794</td><td>-0.68709990</td><td>-0.16135047</td><td>-0.27323195</td><td> 0.074283333</td><td>-0.440089791</td><td>-0.294238013</td><td> 0.10534551</td><td> 0.6299827557</td><td> 0.528825835</td></tr>\n",
              "\t<tr><th scope=row>1841</th><td>-0.64363169</td><td> 0.39464305</td><td>-0.51663717</td><td>-0.26036606</td><td> 0.17839496</td><td> 0.28172289</td><td> 0.35276825</td><td> 0.04594110</td><td>-0.085550141</td><td> 0.60269109</td><td>⋯</td><td>-0.078028838</td><td>-0.84906721</td><td>-0.74034014</td><td> 0.22099173</td><td>-0.005637184</td><td>-0.196269850</td><td>-0.490606502</td><td>-0.14352095</td><td>-0.5746922442</td><td> 0.484796474</td></tr>\n",
              "\t<tr><th scope=row>1881</th><td>-0.16064160</td><td> 0.63842416</td><td>-0.15512472</td><td> 0.21920492</td><td>-0.40337032</td><td> 0.08934645</td><td> 0.05465616</td><td>-0.68041639</td><td> 0.486641403</td><td> 0.33557877</td><td>⋯</td><td> 0.523296084</td><td> 0.10808258</td><td>-0.14250742</td><td>-0.30209468</td><td>-0.347134508</td><td>-0.131963484</td><td> 0.199057038</td><td>-0.46297884</td><td> 0.3299895984</td><td>-0.023089008</td></tr>\n",
              "\t<tr><th scope=row>2005</th><td> 0.30330476</td><td> 0.46340260</td><td> 1.07200098</td><td> 0.64906417</td><td>-0.09673991</td><td> 0.24447156</td><td>-0.22427952</td><td> 0.05807113</td><td> 0.176181114</td><td> 0.77190048</td><td>⋯</td><td> 0.217931912</td><td>-0.23449325</td><td>-0.54430110</td><td> 0.74694629</td><td> 0.378682670</td><td>-0.414246425</td><td>-0.251819016</td><td> 0.09975650</td><td>-0.3260630406</td><td> 0.313982341</td></tr>\n",
              "\t<tr><th scope=row>36</th><td>-1.04820731</td><td> 0.18680421</td><td>-0.36206242</td><td>-0.31433877</td><td>-0.16047504</td><td> 0.02559040</td><td>-0.52615126</td><td> 0.30309286</td><td> 0.594394198</td><td> 0.15275491</td><td>⋯</td><td>-0.432263762</td><td>-0.94327215</td><td>-0.86728761</td><td> 0.03697629</td><td> 0.023473632</td><td> 0.250202793</td><td>-0.225504402</td><td> 0.56372624</td><td> 0.3423216271</td><td> 0.170207423</td></tr>\n",
              "\t<tr><th scope=row>38</th><td> 0.23950902</td><td> 0.66162068</td><td> 0.40454963</td><td>-0.12451774</td><td>-0.27237319</td><td>-0.25242899</td><td>-0.32643207</td><td> 0.77775335</td><td> 0.008975544</td><td> 0.54310462</td><td>⋯</td><td> 0.002088803</td><td>-0.40748182</td><td> 0.14296547</td><td>-0.57684190</td><td>-0.405189757</td><td>-0.280860809</td><td>-0.102424417</td><td>-0.13971630</td><td> 0.1082684066</td><td> 0.846399960</td></tr>\n",
              "\t<tr><th scope=row>39</th><td>-0.45500920</td><td> 0.37761543</td><td> 0.12899650</td><td> 0.34855021</td><td> 0.95821330</td><td> 0.22457393</td><td> 0.27838264</td><td> 0.27735947</td><td> 0.175953891</td><td>-0.13082525</td><td>⋯</td><td>-0.223028636</td><td> 0.25634268</td><td>-0.61034492</td><td> 0.08320432</td><td>-0.007539595</td><td> 0.823494643</td><td>-0.504143283</td><td>-0.23752902</td><td> 0.5533741496</td><td> 0.046986448</td></tr>\n",
              "\t<tr><th scope=row>52</th><td>-0.05806557</td><td> 0.05131635</td><td> 1.13282643</td><td> 0.22139621</td><td> 0.68497688</td><td> 0.09284366</td><td>-0.25984258</td><td>-1.01686067</td><td>-0.238540322</td><td>-0.14508061</td><td>⋯</td><td>-0.016681551</td><td>-0.84191835</td><td> 0.13537092</td><td> 0.93538656</td><td> 0.331386129</td><td>-0.284657078</td><td> 0.037629748</td><td> 0.77218520</td><td> 0.0445297803</td><td>-0.179013608</td></tr>\n",
              "\t<tr><th scope=row>5â</th><td>-0.33385473</td><td> 0.15762580</td><td> 0.69672298</td><td> 0.06397768</td><td>-0.38596070</td><td> 0.44789943</td><td> 0.46258904</td><td> 0.51563506</td><td> 0.232720042</td><td> 0.36631229</td><td>⋯</td><td> 0.770611898</td><td>-0.51251799</td><td>-0.14484181</td><td> 0.03746398</td><td>-0.351322133</td><td>-0.531175056</td><td>-1.027938904</td><td>-0.24506040</td><td>-0.2788825331</td><td> 0.262615112</td></tr>\n",
              "\t<tr><th scope=row>600</th><td>-0.33060412</td><td> 0.50694422</td><td> 0.08683890</td><td>-0.07372704</td><td>-0.19431043</td><td>-0.51469502</td><td>-0.15987011</td><td>-0.83610260</td><td>-0.196220839</td><td> 0.08482536</td><td>⋯</td><td> 0.066924373</td><td>-0.81379241</td><td>-0.69483290</td><td>-0.94866798</td><td>-0.504177778</td><td>-0.240210791</td><td> 0.055614033</td><td> 0.22476259</td><td>-0.3597520838</td><td>-0.409118558</td></tr>\n",
              "\t<tr><th scope=row>abcdefg</th><td>-0.40157881</td><td> 0.01796207</td><td> 0.64565495</td><td>-0.70716491</td><td> 1.33364934</td><td>-0.31949992</td><td>-0.26349394</td><td>-0.31911708</td><td>-0.268414973</td><td> 0.57926403</td><td>⋯</td><td>-0.380683508</td><td>-0.56358408</td><td>-0.60721345</td><td> 0.37689570</td><td>-0.294437593</td><td>-0.349698620</td><td> 0.312965267</td><td>-0.34318367</td><td>-0.2756320172</td><td> 0.390639932</td></tr>\n",
              "\t<tr><th scope=row>abroad</th><td> 0.15488023</td><td> 0.27734733</td><td> 0.26413380</td><td>-0.12373690</td><td> 0.09538514</td><td> 0.30076671</td><td>-0.18368433</td><td>-0.94869674</td><td> 0.835425264</td><td> 0.86722846</td><td>⋯</td><td> 0.191768403</td><td>-0.57944446</td><td>-0.61749644</td><td> 0.21150241</td><td>-0.528324816</td><td> 0.073130100</td><td> 0.574808522</td><td> 0.84900215</td><td> 0.0412230305</td><td>-0.375758703</td></tr>\n",
              "\t<tr><th scope=row>abruptly</th><td>-0.14651066</td><td> 0.81145941</td><td> 1.07477592</td><td> 0.38995279</td><td> 0.94847820</td><td> 0.22388005</td><td>-0.25161239</td><td>-0.08105080</td><td> 0.222603359</td><td> 0.28664104</td><td>⋯</td><td> 0.416165364</td><td>-0.77895841</td><td>-0.20823782</td><td>-0.53372262</td><td> 0.079124525</td><td> 0.480021354</td><td>-0.307780197</td><td>-0.41896826</td><td>-0.0893551174</td><td> 0.595651565</td></tr>\n",
              "\t<tr><th scope=row>absorbing</th><td> 0.20015971</td><td> 0.77594940</td><td> 0.16698965</td><td>-0.37351282</td><td> 0.55434020</td><td>-0.62725826</td><td>-0.04442423</td><td>-0.42002240</td><td> 1.008285998</td><td> 0.78512673</td><td>⋯</td><td>-0.474354430</td><td>-0.56125171</td><td> 0.03619193</td><td> 0.02560654</td><td>-0.163977187</td><td>-0.465297438</td><td>-0.110314825</td><td>-0.72296376</td><td> 0.4077864150</td><td>-0.060930952</td></tr>\n",
              "\t<tr><th scope=row>accomplishments</th><td>-0.13764559</td><td> 0.50274629</td><td> 0.62661033</td><td> 0.02178542</td><td> 0.14756980</td><td>-0.47047875</td><td>-0.51140888</td><td>-0.15982545</td><td> 0.293659412</td><td> 0.39459262</td><td>⋯</td><td>-0.445194673</td><td>-0.07482632</td><td> 0.15309754</td><td>-0.25800632</td><td>-0.363646755</td><td> 0.558851918</td><td>-0.579230029</td><td> 0.15624892</td><td>-0.6153160090</td><td> 0.409156719</td></tr>\n",
              "\t<tr><th scope=row>accused</th><td>-0.61261534</td><td> 0.10095523</td><td> 0.12040146</td><td>-0.21564831</td><td>-0.13773283</td><td> 0.36379750</td><td> 0.46329951</td><td> 0.12105378</td><td> 0.431317256</td><td> 0.15956916</td><td>⋯</td><td>-0.683099987</td><td>-0.33268572</td><td>-0.76791701</td><td>-0.21520854</td><td>-0.370413862</td><td> 0.545322211</td><td> 0.335937806</td><td> 0.01036726</td><td> 0.7395227427</td><td> 0.211819633</td></tr>\n",
              "\t<tr><th scope=row>achievement</th><td>-0.25862395</td><td> 0.29322679</td><td> 0.10182493</td><td>-0.06985774</td><td> 0.18478602</td><td> 0.30963299</td><td> 0.43193835</td><td>-0.25541769</td><td> 0.750169145</td><td> 0.63206618</td><td>⋯</td><td>-0.585145216</td><td> 0.01526585</td><td> 0.32202048</td><td>-0.81241454</td><td> 0.210245851</td><td> 0.058727916</td><td>-0.359031967</td><td> 0.66510821</td><td> 0.2153829511</td><td> 0.379436211</td></tr>\n",
              "\t<tr><th scope=row>acquired</th><td>-0.34116270</td><td> 0.50027169</td><td> 0.47119684</td><td> 0.45904676</td><td> 0.57840933</td><td> 0.45674340</td><td>-0.33646089</td><td>-0.18665639</td><td> 0.508170088</td><td> 0.20306662</td><td>⋯</td><td> 0.266381253</td><td>-0.24012731</td><td>-0.73133292</td><td>-0.03649425</td><td>-0.779558511</td><td>-0.036389511</td><td>-0.075870580</td><td> 0.19411978</td><td> 0.6116579132</td><td> 0.846860814</td></tr>\n",
              "\t<tr><th scope=row>acres</th><td>-0.31485318</td><td> 0.30243760</td><td>-0.09304048</td><td> 0.83754469</td><td> 0.34108108</td><td>-0.09690345</td><td> 0.35804593</td><td>-0.16034645</td><td> 0.615634092</td><td> 0.72639915</td><td>⋯</td><td>-0.384969230</td><td>-0.52875394</td><td>-0.69982675</td><td>-0.68346097</td><td> 0.395895187</td><td> 0.286274512</td><td> 0.339776968</td><td>-0.04467395</td><td>-0.3479136521</td><td>-0.313874057</td></tr>\n",
              "\t<tr><th scope=row>adams</th><td>-0.69634439</td><td>-0.26658760</td><td> 0.90266824</td><td>-0.61493526</td><td> 0.78649574</td><td> 0.56622898</td><td> 0.14576546</td><td>-0.15590994</td><td>-0.152552419</td><td> 0.06719049</td><td>⋯</td><td> 0.255518293</td><td>-0.29020555</td><td> 0.16477009</td><td>-0.30498240</td><td>-0.217081124</td><td>-0.355825160</td><td>-0.005717112</td><td>-0.27495496</td><td>-0.0064399829</td><td> 0.692910905</td></tr>\n",
              "\t<tr><th scope=row>address</th><td>-0.18458682</td><td> 0.82754345</td><td> 0.40785055</td><td> 0.07160201</td><td> 0.29813581</td><td>-0.14680054</td><td>-0.05683479</td><td> 0.27725900</td><td> 0.776367427</td><td> 0.71574551</td><td>⋯</td><td>-0.425507604</td><td> 0.56925232</td><td>-0.77450787</td><td>-0.21968531</td><td>-0.393851543</td><td> 0.235527390</td><td>-0.222628464</td><td> 0.33149269</td><td>-0.1092724028</td><td>-0.277868435</td></tr>\n",
              "\t<tr><th scope=row>adjust</th><td> 0.02851715</td><td> 0.37271110</td><td> 0.72450049</td><td>-0.65003277</td><td> 0.46891784</td><td> 0.23665688</td><td> 0.62821302</td><td>-0.92325426</td><td>-0.282036458</td><td> 0.55982904</td><td>⋯</td><td>-0.534008096</td><td>-0.77349121</td><td>-0.52626828</td><td>-0.35360541</td><td>-0.246580776</td><td>-0.019781547</td><td> 0.246551178</td><td> 0.05270351</td><td> 0.1101698701</td><td>-0.590714979</td></tr>\n",
              "\t<tr><th scope=row>admires</th><td> 0.44918936</td><td> 0.56227903</td><td> 0.03074817</td><td> 0.36452636</td><td> 0.12465177</td><td>-0.44721982</td><td> 0.66251004</td><td>-0.43171553</td><td> 0.306017283</td><td> 0.42018114</td><td>⋯</td><td> 0.166355598</td><td> 0.25240886</td><td>-0.66343709</td><td>-0.08748625</td><td>-0.369577214</td><td>-0.067474622</td><td> 0.076654036</td><td>-0.01786241</td><td>-0.1047113229</td><td>-0.120111746</td></tr>\n",
              "\t<tr><th scope=row>admits</th><td> 0.29036073</td><td> 0.27498543</td><td> 0.49407554</td><td>-0.14200274</td><td> 0.47344389</td><td> 0.60071006</td><td> 0.30293527</td><td>-0.83453126</td><td> 0.165434325</td><td> 0.22509583</td><td>⋯</td><td> 0.148829458</td><td>-0.94952972</td><td>-0.55808713</td><td>-0.26700851</td><td> 0.254484310</td><td> 0.334849709</td><td>-0.201680989</td><td> 0.57556197</td><td>-0.0171481467</td><td>-0.004069417</td></tr>\n",
              "\t<tr><th scope=row>admitted</th><td> 0.47993942</td><td>-0.61493381</td><td> 0.12408015</td><td> 0.28097345</td><td> 0.72183445</td><td> 1.07804104</td><td>-0.20036122</td><td> 0.10829519</td><td> 0.404195819</td><td>-0.03653836</td><td>⋯</td><td> 0.065173675</td><td>-0.89852201</td><td>-0.74149526</td><td> 0.05404889</td><td>-1.033616420</td><td> 0.873615782</td><td> 0.108723448</td><td>-0.83790351</td><td> 0.5334340693</td><td>-0.146044437</td></tr>\n",
              "\t<tr><th scope=row>adorn</th><td> 0.25667473</td><td> 0.34523755</td><td> 0.28820475</td><td>-0.20070982</td><td> 0.08202452</td><td>-0.28895571</td><td> 0.07114694</td><td>-0.86296166</td><td> 0.379971986</td><td>-0.56725116</td><td>⋯</td><td>-0.921922461</td><td>-0.71610052</td><td>-0.88031595</td><td> 0.26788561</td><td> 0.034513968</td><td> 0.105264456</td><td>-0.408694046</td><td> 0.01248398</td><td> 0.1840377769</td><td>-0.337146807</td></tr>\n",
              "\t<tr><th scope=row>adrenalin</th><td>-0.26604361</td><td>-0.35490073</td><td> 1.00307782</td><td>-0.18668994</td><td> 0.44937947</td><td>-0.72531490</td><td> 0.30058889</td><td>-0.09830227</td><td>-0.047422765</td><td> 1.41489692</td><td>⋯</td><td> 0.442566064</td><td>-0.50471032</td><td> 0.04323064</td><td>-0.13455601</td><td> 0.058062792</td><td> 0.642578172</td><td>-0.617369036</td><td>-0.36684942</td><td> 0.0800636819</td><td>-0.558739711</td></tr>\n",
              "\t<tr><th scope=row>adress</th><td> 0.23531301</td><td> 0.53459220</td><td> 0.07612993</td><td> 0.37024559</td><td>-0.30720931</td><td> 0.81581563</td><td> 0.66167766</td><td> 0.61156169</td><td> 0.369798429</td><td>-0.15421523</td><td>⋯</td><td>-0.634295394</td><td> 0.14429949</td><td>-0.64907894</td><td> 0.12386252</td><td>-0.330147619</td><td> 0.073750553</td><td>-0.376343321</td><td> 0.05642954</td><td>-0.0118247254</td><td>-0.507969695</td></tr>\n",
              "\t<tr><th scope=row>adrift</th><td>-0.19592400</td><td> 0.60244978</td><td> 0.22078414</td><td> 0.49324494</td><td> 0.89550409</td><td>-0.39184929</td><td> 0.19876276</td><td>-0.49194338</td><td>-0.572287628</td><td> 0.01662615</td><td>⋯</td><td>-0.732657136</td><td> 0.09863665</td><td>-0.36016847</td><td>-0.17595746</td><td>-0.756126706</td><td>-0.002017151</td><td>-0.067666600</td><td> 0.15205261</td><td> 0.3512133517</td><td> 0.113470366</td></tr>\n",
              "\t<tr><th scope=row>advanced</th><td>-0.24623197</td><td> 0.60280456</td><td> 1.20168672</td><td> 0.85798190</td><td> 0.39978256</td><td> 0.16937385</td><td>-0.04049930</td><td>-0.09311009</td><td> 0.479889034</td><td> 0.47722388</td><td>⋯</td><td>-0.531313985</td><td>-0.46731587</td><td>-0.89766364</td><td> 0.71246716</td><td> 0.372421698</td><td>-0.111219321</td><td>-0.602362906</td><td> 0.34732118</td><td>-0.0008298921</td><td> 0.148789316</td></tr>\n",
              "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><th scope=row>your</th><td>-0.26367375</td><td>-0.79079137</td><td>-1.5063920</td><td> 0.621950800</td><td> 0.17502403</td><td>-0.935297702</td><td>-0.601180304</td><td> 0.01600121</td><td>-1.16517819</td><td>-0.1845625</td><td>⋯</td><td>-1.174381298</td><td>1.1423899</td><td>0.6179424</td><td> 0.62678728</td><td> 0.269599724</td><td> 0.625622458</td><td> 0.051591504</td><td>-0.483605119</td><td>-0.13837945</td><td>-0.59079055</td></tr>\n",
              "\t<tr><th scope=row>she</th><td> 0.69289123</td><td>-0.49871991</td><td>-0.5222423</td><td> 0.409830475</td><td>-0.12236762</td><td> 0.714752240</td><td>-0.015708922</td><td> 0.95570365</td><td>-0.47359760</td><td>-0.3749117</td><td>⋯</td><td> 0.721822721</td><td>1.0727351</td><td>1.1555149</td><td> 1.60043253</td><td> 0.613246704</td><td> 0.250095766</td><td> 0.456091981</td><td>-0.119821136</td><td>-0.07308069</td><td>-0.00315397</td></tr>\n",
              "\t<tr><th scope=row>love</th><td> 0.22250122</td><td>-0.17730073</td><td>-1.4340479</td><td>-0.034107139</td><td>-0.36885386</td><td>-0.585152325</td><td>-0.285122862</td><td> 0.56371378</td><td>-0.03731074</td><td>-0.7822942</td><td>⋯</td><td> 0.578649364</td><td>1.5367088</td><td>0.2989985</td><td> 1.45558025</td><td> 0.658917682</td><td> 0.044272550</td><td>-0.152453809</td><td> 0.555487312</td><td>-0.52112356</td><td> 0.10285955</td></tr>\n",
              "\t<tr><th scope=row>they</th><td> 0.58026652</td><td>-1.17959436</td><td>-1.1824842</td><td>-0.352939878</td><td>-0.26317892</td><td>-0.159141653</td><td> 0.003178093</td><td> 0.49906131</td><td>-0.40643702</td><td>-0.6424109</td><td>⋯</td><td>-0.536253460</td><td>0.9325739</td><td>0.3636713</td><td> 0.68045159</td><td> 0.317031096</td><td>-0.114155112</td><td> 0.596893721</td><td> 0.681431082</td><td> 0.07413305</td><td> 0.22283298</td></tr>\n",
              "\t<tr><th scope=row>all</th><td> 0.39592632</td><td>-0.67309049</td><td>-1.0172284</td><td> 0.684478344</td><td>-0.19585116</td><td>-0.017499164</td><td>-0.344259324</td><td> 0.71108178</td><td>-0.29756383</td><td>-0.7239884</td><td>⋯</td><td>-0.547088172</td><td>1.3387397</td><td>0.6095917</td><td> 0.60443375</td><td> 0.664492883</td><td>-0.779255753</td><td> 0.171481528</td><td> 0.470049161</td><td> 0.39555708</td><td> 0.12852380</td></tr>\n",
              "\t<tr><th scope=row>as</th><td> 0.39832854</td><td>-0.29186656</td><td>-1.4850306</td><td>-0.581164915</td><td>-1.09616048</td><td>-0.511131519</td><td>-0.685779204</td><td> 0.85771183</td><td>-1.06425903</td><td>-0.7679770</td><td>⋯</td><td> 0.096985115</td><td>0.7200969</td><td>1.4716890</td><td>-0.22949817</td><td> 0.429819727</td><td>-0.499004022</td><td> 1.190302261</td><td>-0.387476423</td><td> 0.18922229</td><td>-0.11379285</td></tr>\n",
              "\t<tr><th scope=row>for</th><td>-0.39839714</td><td>-0.41988219</td><td>-0.7809746</td><td> 0.512492233</td><td>-0.75993745</td><td> 0.029745997</td><td>-0.526605443</td><td> 0.75617495</td><td> 0.09088757</td><td>-0.4428188</td><td>⋯</td><td>-0.067477238</td><td>1.1690065</td><td>0.2871463</td><td> 0.87203142</td><td> 0.559696752</td><td>-0.052406673</td><td>-0.102448950</td><td> 1.007599076</td><td>-0.58417665</td><td>-0.82657403</td></tr>\n",
              "\t<tr><th scope=row>be</th><td> 0.28283749</td><td> 0.02185847</td><td>-1.4425185</td><td> 0.167710814</td><td>-0.44790750</td><td>-0.518003098</td><td> 0.051647847</td><td> 0.21476109</td><td>-0.87474988</td><td>-0.5868641</td><td>⋯</td><td>-0.688947469</td><td>0.4722453</td><td>0.6577129</td><td> 0.90697095</td><td> 1.017378164</td><td> 0.114565906</td><td> 0.728626254</td><td> 0.902513682</td><td> 0.05861234</td><td>-0.95253640</td></tr>\n",
              "\t<tr><th scope=row>when</th><td> 0.16942950</td><td>-0.69643404</td><td>-0.8785456</td><td>-0.044236829</td><td> 0.22389559</td><td> 0.218573021</td><td>-0.342530269</td><td> 0.70260641</td><td>-0.63261592</td><td>-1.0646387</td><td>⋯</td><td>-0.129041535</td><td>1.1645497</td><td>0.8748809</td><td> 0.61149677</td><td> 0.640061583</td><td> 0.374739556</td><td> 0.297211930</td><td>-0.179811189</td><td> 0.06769368</td><td>-0.19530544</td></tr>\n",
              "\t<tr><th scope=row>with</th><td> 0.51493375</td><td>-0.85476390</td><td>-1.3779862</td><td> 0.633571360</td><td>-0.31009490</td><td>-0.569237710</td><td>-0.453635308</td><td> 0.69090739</td><td>-0.30020950</td><td>-1.0686468</td><td>⋯</td><td> 0.546739769</td><td>1.3293900</td><td>0.9229314</td><td> 0.01843741</td><td> 0.882998687</td><td>-0.214145903</td><td>-0.234868390</td><td> 0.467989801</td><td>-0.20973266</td><td> 0.09242494</td></tr>\n",
              "\t<tr><th scope=row>so</th><td> 0.43518000</td><td>-0.61271702</td><td>-1.2576371</td><td>-0.370141735</td><td>-0.39944119</td><td>-0.206096288</td><td>-0.311205664</td><td> 0.69828094</td><td>-0.92427719</td><td>-0.7476003</td><td>⋯</td><td> 0.099608427</td><td>0.5871950</td><td>1.1117145</td><td> 0.39143774</td><td> 0.492687491</td><td> 0.353968411</td><td> 0.895911547</td><td> 0.121871380</td><td>-0.37343258</td><td>-0.08273767</td></tr>\n",
              "\t<tr><th scope=row>on</th><td> 0.59969940</td><td>-1.00557044</td><td>-2.0211294</td><td> 0.592511023</td><td> 0.11367573</td><td>-0.004992162</td><td>-0.691903344</td><td> 0.70014569</td><td>-1.11767509</td><td>-1.1833096</td><td>⋯</td><td> 0.695171589</td><td>1.0036004</td><td>1.3252801</td><td> 0.74257307</td><td> 0.066241981</td><td> 0.002643934</td><td>-0.534613554</td><td> 0.039622184</td><td> 0.73803651</td><td>-0.09712845</td></tr>\n",
              "\t<tr><th scope=row>was</th><td> 0.64905300</td><td>-0.98923659</td><td>-0.9852883</td><td> 0.910984863</td><td>-0.34462532</td><td> 0.425333470</td><td>-0.234567540</td><td> 1.34106202</td><td>-0.43065139</td><td>-0.3377714</td><td>⋯</td><td> 0.354142381</td><td>0.4853990</td><td>1.4333954</td><td> 0.45147687</td><td> 0.622032544</td><td> 0.647855629</td><td> 0.088629654</td><td>-0.061850423</td><td> 0.63683263</td><td>-0.38573138</td></tr>\n",
              "\t<tr><th scope=row>but</th><td> 0.18693633</td><td>-0.74668540</td><td>-1.2558590</td><td> 0.281841072</td><td>-0.47389408</td><td> 0.103364640</td><td>-0.026913124</td><td> 0.52866758</td><td>-0.54662059</td><td>-0.6586600</td><td>⋯</td><td>-0.466982217</td><td>0.6718686</td><td>0.9988895</td><td> 0.82054422</td><td> 0.736419112</td><td> 0.437368043</td><td> 0.488170107</td><td> 0.388788607</td><td>-0.08358231</td><td>-0.04387687</td></tr>\n",
              "\t<tr><th scope=row>he</th><td> 1.15394314</td><td>-1.10759222</td><td>-0.8475731</td><td> 0.635357755</td><td>-0.27859281</td><td> 1.107216159</td><td>-0.356926170</td><td> 0.92975387</td><td>-0.04188973</td><td>-0.7303973</td><td>⋯</td><td> 0.860222507</td><td>0.5588400</td><td>1.4958305</td><td> 1.24920355</td><td> 0.695390830</td><td> 0.734397965</td><td> 0.316446983</td><td> 0.039039399</td><td>-0.09124719</td><td> 0.01090295</td></tr>\n",
              "\t<tr><th scope=row>like</th><td> 0.13243629</td><td>-0.68196441</td><td>-1.3995908</td><td>-0.940976045</td><td>-0.80537181</td><td> 0.062827589</td><td>-0.114515711</td><td> 0.50544646</td><td>-0.45742890</td><td>-0.7841520</td><td>⋯</td><td> 0.817125986</td><td>1.1051883</td><td>1.2717990</td><td> 0.65548684</td><td> 0.644173537</td><td>-0.361841487</td><td>-0.472050497</td><td>-0.004153028</td><td> 0.15938925</td><td> 0.41821575</td></tr>\n",
              "\t<tr><th scope=row>that</th><td> 0.43750344</td><td>-0.58189489</td><td>-1.4702877</td><td> 0.687848056</td><td>-0.76543832</td><td> 0.165876761</td><td> 0.080006208</td><td> 0.56184661</td><td>-0.11855347</td><td>-0.4638706</td><td>⋯</td><td> 0.005276605</td><td>1.0304909</td><td>0.6683722</td><td> 1.01108274</td><td> 0.615448666</td><td> 0.484574855</td><td> 0.004167039</td><td>-0.218486076</td><td> 0.26499621</td><td>-0.21052289</td></tr>\n",
              "\t<tr><th scope=row>are</th><td>-0.14341924</td><td>-1.03757929</td><td>-1.2857334</td><td>-0.238915715</td><td> 0.02918028</td><td>-0.840337994</td><td>-0.233678114</td><td> 1.13795747</td><td>-1.04935078</td><td>-0.3944796</td><td>⋯</td><td>-0.695810579</td><td>1.3011044</td><td>0.7818273</td><td> 0.35312520</td><td> 0.572296360</td><td>-0.906472998</td><td>-0.438791042</td><td> 0.608495006</td><td>-0.03605581</td><td> 1.46582011</td></tr>\n",
              "\t<tr><th scope=row>me</th><td> 0.53929675</td><td>-0.71271830</td><td>-1.1580592</td><td>-0.135692760</td><td> 0.12916879</td><td> 0.309807293</td><td>-0.438567525</td><td> 0.26537195</td><td>-0.42383084</td><td>-0.2615553</td><td>⋯</td><td>-0.632752272</td><td>1.6250256</td><td>1.1259227</td><td> 1.60052831</td><td>-0.007514038</td><td> 1.041455442</td><td> 0.063508085</td><td>-0.013093116</td><td>-0.56689741</td><td>-1.06950911</td></tr>\n",
              "\t<tr><th scope=row>of</th><td>-1.24883526</td><td>-0.89486126</td><td>-1.4192157</td><td> 0.744012861</td><td>-1.50305425</td><td>-0.475032972</td><td>-0.144426718</td><td> 0.70020604</td><td>-0.75120661</td><td>-0.7190685</td><td>⋯</td><td>-0.450830970</td><td>1.6904811</td><td>0.7748471</td><td> 0.13082204</td><td> 0.995318940</td><td>-0.718575513</td><td>-0.240805340</td><td> 0.277899281</td><td>-0.06371220</td><td>-0.66647384</td></tr>\n",
              "\t<tr><th scope=row>in</th><td> 0.77309697</td><td>-0.72740194</td><td>-1.5559369</td><td> 0.459332303</td><td>-0.86835475</td><td>-0.364802701</td><td>-0.496057385</td><td> 0.26410878</td><td>-0.72646978</td><td>-0.7266337</td><td>⋯</td><td> 0.043670690</td><td>0.8980010</td><td>1.0808435</td><td> 0.14019658</td><td> 0.008163692</td><td>-0.764175252</td><td>-0.051513225</td><td>-0.077868104</td><td> 0.15978812</td><td>-0.92226165</td></tr>\n",
              "\t<tr><th scope=row>it</th><td> 0.34079067</td><td>-0.38734016</td><td>-1.3916619</td><td>-0.078257184</td><td>-0.69930216</td><td> 0.517317441</td><td>-0.429555592</td><td> 0.69321637</td><td>-0.11307488</td><td>-0.7576901</td><td>⋯</td><td> 0.185232829</td><td>1.4058403</td><td>1.0122437</td><td> 0.62448924</td><td> 0.551349422</td><td> 0.487480496</td><td> 0.161114430</td><td>-0.683544092</td><td> 0.22282358</td><td>-0.39265535</td></tr>\n",
              "\t<tr><th scope=row>you</th><td> 0.46096771</td><td>-0.51584745</td><td>-1.8084014</td><td>-0.285518484</td><td>-0.29062820</td><td>-0.063137734</td><td>-0.114242858</td><td> 0.17100351</td><td>-0.55435952</td><td>-0.4474065</td><td>⋯</td><td>-0.859708842</td><td>1.3703449</td><td>0.6416512</td><td> 1.13490020</td><td> 0.256059923</td><td> 0.773477748</td><td> 0.028206076</td><td> 0.404171039</td><td>-0.55528292</td><td>-1.12197184</td></tr>\n",
              "\t<tr><th scope=row>my</th><td> 0.06996694</td><td>-1.28749834</td><td>-1.6345217</td><td> 0.479953869</td><td> 0.23901260</td><td>-0.742588168</td><td>-0.354232280</td><td> 0.46311420</td><td>-1.45084674</td><td>-0.4734557</td><td>⋯</td><td>-0.451633652</td><td>1.4100172</td><td>1.3128105</td><td> 0.71639714</td><td> 0.082677530</td><td> 0.861428672</td><td>-0.051903571</td><td>-0.484869159</td><td>-0.39794365</td><td> 0.36894574</td></tr>\n",
              "\t<tr><th scope=row>is</th><td>-0.16791467</td><td> 0.03864938</td><td>-1.4806589</td><td>-0.004808392</td><td>-0.54753438</td><td>-0.050512281</td><td>-0.108310608</td><td> 1.06102805</td><td>-0.78905250</td><td>-0.5043514</td><td>⋯</td><td> 0.941339249</td><td>1.0334282</td><td>1.7182371</td><td> 0.98784481</td><td> 1.068900958</td><td>-0.385775921</td><td> 0.371381677</td><td>-0.340486566</td><td>-0.80676722</td><td> 0.12511416</td></tr>\n",
              "\t<tr><th scope=row>to</th><td> 0.43481309</td><td>-0.58222114</td><td>-1.3526620</td><td>-0.164314058</td><td> 0.04213467</td><td> 0.225188221</td><td> 0.124631109</td><td> 0.14982865</td><td>-0.14420387</td><td>-0.7668390</td><td>⋯</td><td>-0.736723974</td><td>0.7045938</td><td>0.7155747</td><td> 0.95134272</td><td> 0.411088902</td><td> 0.374227170</td><td> 0.892367856</td><td> 0.427908117</td><td>-0.62428465</td><td>-1.11231286</td></tr>\n",
              "\t<tr><th scope=row>and</th><td> 0.36225460</td><td>-1.01378561</td><td>-1.0322945</td><td>-0.358453057</td><td>-0.37041794</td><td>-0.085126377</td><td>-0.605695044</td><td> 0.78977045</td><td>-0.57183245</td><td>-1.3635476</td><td>⋯</td><td>-0.165092097</td><td>1.3466418</td><td>1.2457942</td><td> 0.42809506</td><td> 0.322931471</td><td>-0.293811885</td><td> 0.652186210</td><td> 0.267071963</td><td>-0.42989400</td><td> 0.03637533</td></tr>\n",
              "\t<tr><th scope=row>a</th><td> 0.37189141</td><td>-0.85665368</td><td>-1.5738692</td><td>-0.041131224</td><td>-1.91131374</td><td> 0.352998853</td><td> 0.170112566</td><td>-0.07298348</td><td>-0.54963493</td><td>-0.9005205</td><td>⋯</td><td> 1.253177568</td><td>1.1260915</td><td>1.8603940</td><td>-0.22164927</td><td> 0.788741974</td><td> 0.742962234</td><td> 0.372429451</td><td>-0.413793308</td><td> 0.12117929</td><td>-0.29739998</td></tr>\n",
              "\t<tr><th scope=row>the</th><td> 0.38891068</td><td>-0.88399918</td><td>-1.5074105</td><td>-0.343454444</td><td>-0.93805158</td><td>-0.007305140</td><td>-0.952921270</td><td> 0.77646064</td><td>-0.48269182</td><td>-0.7937678</td><td>⋯</td><td>-0.104269760</td><td>1.4607548</td><td>1.2679326</td><td>-0.28575237</td><td> 0.507531505</td><td>-0.860655346</td><td> 0.415291275</td><td>-1.053508614</td><td> 0.27167243</td><td>-0.51934954</td></tr>\n",
              "\t<tr><th scope=row>i</th><td> 0.71442600</td><td>-0.82348366</td><td>-1.9898728</td><td>-0.381494712</td><td>-0.42153281</td><td> 0.119676811</td><td> 0.078833015</td><td> 0.65684282</td><td>-0.88310721</td><td>-1.0057171</td><td>⋯</td><td>-0.268243873</td><td>1.0580800</td><td>1.0078055</td><td> 0.56378961</td><td> 0.286672299</td><td> 1.019334540</td><td>-0.173161490</td><td> 0.682597136</td><td>-0.32932463</td><td>-0.67716317</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 14267 × 50 of type dbl\n\n| 1837 | -0.63303339 |  0.30040185 | -0.16941897 |  0.19313955 |  0.33992052 |  0.10447346 | -0.61349224 | -0.51546688 |  1.143208193 |  0.50096603 | ⋯ | -0.207672794 | -0.68709990 | -0.16135047 | -0.27323195 |  0.074283333 | -0.440089791 | -0.294238013 |  0.10534551 |  0.6299827557 |  0.528825835 |\n| 1841 | -0.64363169 |  0.39464305 | -0.51663717 | -0.26036606 |  0.17839496 |  0.28172289 |  0.35276825 |  0.04594110 | -0.085550141 |  0.60269109 | ⋯ | -0.078028838 | -0.84906721 | -0.74034014 |  0.22099173 | -0.005637184 | -0.196269850 | -0.490606502 | -0.14352095 | -0.5746922442 |  0.484796474 |\n| 1881 | -0.16064160 |  0.63842416 | -0.15512472 |  0.21920492 | -0.40337032 |  0.08934645 |  0.05465616 | -0.68041639 |  0.486641403 |  0.33557877 | ⋯ |  0.523296084 |  0.10808258 | -0.14250742 | -0.30209468 | -0.347134508 | -0.131963484 |  0.199057038 | -0.46297884 |  0.3299895984 | -0.023089008 |\n| 2005 |  0.30330476 |  0.46340260 |  1.07200098 |  0.64906417 | -0.09673991 |  0.24447156 | -0.22427952 |  0.05807113 |  0.176181114 |  0.77190048 | ⋯ |  0.217931912 | -0.23449325 | -0.54430110 |  0.74694629 |  0.378682670 | -0.414246425 | -0.251819016 |  0.09975650 | -0.3260630406 |  0.313982341 |\n| 36 | -1.04820731 |  0.18680421 | -0.36206242 | -0.31433877 | -0.16047504 |  0.02559040 | -0.52615126 |  0.30309286 |  0.594394198 |  0.15275491 | ⋯ | -0.432263762 | -0.94327215 | -0.86728761 |  0.03697629 |  0.023473632 |  0.250202793 | -0.225504402 |  0.56372624 |  0.3423216271 |  0.170207423 |\n| 38 |  0.23950902 |  0.66162068 |  0.40454963 | -0.12451774 | -0.27237319 | -0.25242899 | -0.32643207 |  0.77775335 |  0.008975544 |  0.54310462 | ⋯ |  0.002088803 | -0.40748182 |  0.14296547 | -0.57684190 | -0.405189757 | -0.280860809 | -0.102424417 | -0.13971630 |  0.1082684066 |  0.846399960 |\n| 39 | -0.45500920 |  0.37761543 |  0.12899650 |  0.34855021 |  0.95821330 |  0.22457393 |  0.27838264 |  0.27735947 |  0.175953891 | -0.13082525 | ⋯ | -0.223028636 |  0.25634268 | -0.61034492 |  0.08320432 | -0.007539595 |  0.823494643 | -0.504143283 | -0.23752902 |  0.5533741496 |  0.046986448 |\n| 52 | -0.05806557 |  0.05131635 |  1.13282643 |  0.22139621 |  0.68497688 |  0.09284366 | -0.25984258 | -1.01686067 | -0.238540322 | -0.14508061 | ⋯ | -0.016681551 | -0.84191835 |  0.13537092 |  0.93538656 |  0.331386129 | -0.284657078 |  0.037629748 |  0.77218520 |  0.0445297803 | -0.179013608 |\n| 5â | -0.33385473 |  0.15762580 |  0.69672298 |  0.06397768 | -0.38596070 |  0.44789943 |  0.46258904 |  0.51563506 |  0.232720042 |  0.36631229 | ⋯ |  0.770611898 | -0.51251799 | -0.14484181 |  0.03746398 | -0.351322133 | -0.531175056 | -1.027938904 | -0.24506040 | -0.2788825331 |  0.262615112 |\n| 600 | -0.33060412 |  0.50694422 |  0.08683890 | -0.07372704 | -0.19431043 | -0.51469502 | -0.15987011 | -0.83610260 | -0.196220839 |  0.08482536 | ⋯ |  0.066924373 | -0.81379241 | -0.69483290 | -0.94866798 | -0.504177778 | -0.240210791 |  0.055614033 |  0.22476259 | -0.3597520838 | -0.409118558 |\n| abcdefg | -0.40157881 |  0.01796207 |  0.64565495 | -0.70716491 |  1.33364934 | -0.31949992 | -0.26349394 | -0.31911708 | -0.268414973 |  0.57926403 | ⋯ | -0.380683508 | -0.56358408 | -0.60721345 |  0.37689570 | -0.294437593 | -0.349698620 |  0.312965267 | -0.34318367 | -0.2756320172 |  0.390639932 |\n| abroad |  0.15488023 |  0.27734733 |  0.26413380 | -0.12373690 |  0.09538514 |  0.30076671 | -0.18368433 | -0.94869674 |  0.835425264 |  0.86722846 | ⋯ |  0.191768403 | -0.57944446 | -0.61749644 |  0.21150241 | -0.528324816 |  0.073130100 |  0.574808522 |  0.84900215 |  0.0412230305 | -0.375758703 |\n| abruptly | -0.14651066 |  0.81145941 |  1.07477592 |  0.38995279 |  0.94847820 |  0.22388005 | -0.25161239 | -0.08105080 |  0.222603359 |  0.28664104 | ⋯ |  0.416165364 | -0.77895841 | -0.20823782 | -0.53372262 |  0.079124525 |  0.480021354 | -0.307780197 | -0.41896826 | -0.0893551174 |  0.595651565 |\n| absorbing |  0.20015971 |  0.77594940 |  0.16698965 | -0.37351282 |  0.55434020 | -0.62725826 | -0.04442423 | -0.42002240 |  1.008285998 |  0.78512673 | ⋯ | -0.474354430 | -0.56125171 |  0.03619193 |  0.02560654 | -0.163977187 | -0.465297438 | -0.110314825 | -0.72296376 |  0.4077864150 | -0.060930952 |\n| accomplishments | -0.13764559 |  0.50274629 |  0.62661033 |  0.02178542 |  0.14756980 | -0.47047875 | -0.51140888 | -0.15982545 |  0.293659412 |  0.39459262 | ⋯ | -0.445194673 | -0.07482632 |  0.15309754 | -0.25800632 | -0.363646755 |  0.558851918 | -0.579230029 |  0.15624892 | -0.6153160090 |  0.409156719 |\n| accused | -0.61261534 |  0.10095523 |  0.12040146 | -0.21564831 | -0.13773283 |  0.36379750 |  0.46329951 |  0.12105378 |  0.431317256 |  0.15956916 | ⋯ | -0.683099987 | -0.33268572 | -0.76791701 | -0.21520854 | -0.370413862 |  0.545322211 |  0.335937806 |  0.01036726 |  0.7395227427 |  0.211819633 |\n| achievement | -0.25862395 |  0.29322679 |  0.10182493 | -0.06985774 |  0.18478602 |  0.30963299 |  0.43193835 | -0.25541769 |  0.750169145 |  0.63206618 | ⋯ | -0.585145216 |  0.01526585 |  0.32202048 | -0.81241454 |  0.210245851 |  0.058727916 | -0.359031967 |  0.66510821 |  0.2153829511 |  0.379436211 |\n| acquired | -0.34116270 |  0.50027169 |  0.47119684 |  0.45904676 |  0.57840933 |  0.45674340 | -0.33646089 | -0.18665639 |  0.508170088 |  0.20306662 | ⋯ |  0.266381253 | -0.24012731 | -0.73133292 | -0.03649425 | -0.779558511 | -0.036389511 | -0.075870580 |  0.19411978 |  0.6116579132 |  0.846860814 |\n| acres | -0.31485318 |  0.30243760 | -0.09304048 |  0.83754469 |  0.34108108 | -0.09690345 |  0.35804593 | -0.16034645 |  0.615634092 |  0.72639915 | ⋯ | -0.384969230 | -0.52875394 | -0.69982675 | -0.68346097 |  0.395895187 |  0.286274512 |  0.339776968 | -0.04467395 | -0.3479136521 | -0.313874057 |\n| adams | -0.69634439 | -0.26658760 |  0.90266824 | -0.61493526 |  0.78649574 |  0.56622898 |  0.14576546 | -0.15590994 | -0.152552419 |  0.06719049 | ⋯ |  0.255518293 | -0.29020555 |  0.16477009 | -0.30498240 | -0.217081124 | -0.355825160 | -0.005717112 | -0.27495496 | -0.0064399829 |  0.692910905 |\n| address | -0.18458682 |  0.82754345 |  0.40785055 |  0.07160201 |  0.29813581 | -0.14680054 | -0.05683479 |  0.27725900 |  0.776367427 |  0.71574551 | ⋯ | -0.425507604 |  0.56925232 | -0.77450787 | -0.21968531 | -0.393851543 |  0.235527390 | -0.222628464 |  0.33149269 | -0.1092724028 | -0.277868435 |\n| adjust |  0.02851715 |  0.37271110 |  0.72450049 | -0.65003277 |  0.46891784 |  0.23665688 |  0.62821302 | -0.92325426 | -0.282036458 |  0.55982904 | ⋯ | -0.534008096 | -0.77349121 | -0.52626828 | -0.35360541 | -0.246580776 | -0.019781547 |  0.246551178 |  0.05270351 |  0.1101698701 | -0.590714979 |\n| admires |  0.44918936 |  0.56227903 |  0.03074817 |  0.36452636 |  0.12465177 | -0.44721982 |  0.66251004 | -0.43171553 |  0.306017283 |  0.42018114 | ⋯ |  0.166355598 |  0.25240886 | -0.66343709 | -0.08748625 | -0.369577214 | -0.067474622 |  0.076654036 | -0.01786241 | -0.1047113229 | -0.120111746 |\n| admits |  0.29036073 |  0.27498543 |  0.49407554 | -0.14200274 |  0.47344389 |  0.60071006 |  0.30293527 | -0.83453126 |  0.165434325 |  0.22509583 | ⋯ |  0.148829458 | -0.94952972 | -0.55808713 | -0.26700851 |  0.254484310 |  0.334849709 | -0.201680989 |  0.57556197 | -0.0171481467 | -0.004069417 |\n| admitted |  0.47993942 | -0.61493381 |  0.12408015 |  0.28097345 |  0.72183445 |  1.07804104 | -0.20036122 |  0.10829519 |  0.404195819 | -0.03653836 | ⋯ |  0.065173675 | -0.89852201 | -0.74149526 |  0.05404889 | -1.033616420 |  0.873615782 |  0.108723448 | -0.83790351 |  0.5334340693 | -0.146044437 |\n| adorn |  0.25667473 |  0.34523755 |  0.28820475 | -0.20070982 |  0.08202452 | -0.28895571 |  0.07114694 | -0.86296166 |  0.379971986 | -0.56725116 | ⋯ | -0.921922461 | -0.71610052 | -0.88031595 |  0.26788561 |  0.034513968 |  0.105264456 | -0.408694046 |  0.01248398 |  0.1840377769 | -0.337146807 |\n| adrenalin | -0.26604361 | -0.35490073 |  1.00307782 | -0.18668994 |  0.44937947 | -0.72531490 |  0.30058889 | -0.09830227 | -0.047422765 |  1.41489692 | ⋯ |  0.442566064 | -0.50471032 |  0.04323064 | -0.13455601 |  0.058062792 |  0.642578172 | -0.617369036 | -0.36684942 |  0.0800636819 | -0.558739711 |\n| adress |  0.23531301 |  0.53459220 |  0.07612993 |  0.37024559 | -0.30720931 |  0.81581563 |  0.66167766 |  0.61156169 |  0.369798429 | -0.15421523 | ⋯ | -0.634295394 |  0.14429949 | -0.64907894 |  0.12386252 | -0.330147619 |  0.073750553 | -0.376343321 |  0.05642954 | -0.0118247254 | -0.507969695 |\n| adrift | -0.19592400 |  0.60244978 |  0.22078414 |  0.49324494 |  0.89550409 | -0.39184929 |  0.19876276 | -0.49194338 | -0.572287628 |  0.01662615 | ⋯ | -0.732657136 |  0.09863665 | -0.36016847 | -0.17595746 | -0.756126706 | -0.002017151 | -0.067666600 |  0.15205261 |  0.3512133517 |  0.113470366 |\n| advanced | -0.24623197 |  0.60280456 |  1.20168672 |  0.85798190 |  0.39978256 |  0.16937385 | -0.04049930 | -0.09311009 |  0.479889034 |  0.47722388 | ⋯ | -0.531313985 | -0.46731587 | -0.89766364 |  0.71246716 |  0.372421698 | -0.111219321 | -0.602362906 |  0.34732118 | -0.0008298921 |  0.148789316 |\n| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n| your | -0.26367375 | -0.79079137 | -1.5063920 |  0.621950800 |  0.17502403 | -0.935297702 | -0.601180304 |  0.01600121 | -1.16517819 | -0.1845625 | ⋯ | -1.174381298 | 1.1423899 | 0.6179424 |  0.62678728 |  0.269599724 |  0.625622458 |  0.051591504 | -0.483605119 | -0.13837945 | -0.59079055 |\n| she |  0.69289123 | -0.49871991 | -0.5222423 |  0.409830475 | -0.12236762 |  0.714752240 | -0.015708922 |  0.95570365 | -0.47359760 | -0.3749117 | ⋯ |  0.721822721 | 1.0727351 | 1.1555149 |  1.60043253 |  0.613246704 |  0.250095766 |  0.456091981 | -0.119821136 | -0.07308069 | -0.00315397 |\n| love |  0.22250122 | -0.17730073 | -1.4340479 | -0.034107139 | -0.36885386 | -0.585152325 | -0.285122862 |  0.56371378 | -0.03731074 | -0.7822942 | ⋯ |  0.578649364 | 1.5367088 | 0.2989985 |  1.45558025 |  0.658917682 |  0.044272550 | -0.152453809 |  0.555487312 | -0.52112356 |  0.10285955 |\n| they |  0.58026652 | -1.17959436 | -1.1824842 | -0.352939878 | -0.26317892 | -0.159141653 |  0.003178093 |  0.49906131 | -0.40643702 | -0.6424109 | ⋯ | -0.536253460 | 0.9325739 | 0.3636713 |  0.68045159 |  0.317031096 | -0.114155112 |  0.596893721 |  0.681431082 |  0.07413305 |  0.22283298 |\n| all |  0.39592632 | -0.67309049 | -1.0172284 |  0.684478344 | -0.19585116 | -0.017499164 | -0.344259324 |  0.71108178 | -0.29756383 | -0.7239884 | ⋯ | -0.547088172 | 1.3387397 | 0.6095917 |  0.60443375 |  0.664492883 | -0.779255753 |  0.171481528 |  0.470049161 |  0.39555708 |  0.12852380 |\n| as |  0.39832854 | -0.29186656 | -1.4850306 | -0.581164915 | -1.09616048 | -0.511131519 | -0.685779204 |  0.85771183 | -1.06425903 | -0.7679770 | ⋯ |  0.096985115 | 0.7200969 | 1.4716890 | -0.22949817 |  0.429819727 | -0.499004022 |  1.190302261 | -0.387476423 |  0.18922229 | -0.11379285 |\n| for | -0.39839714 | -0.41988219 | -0.7809746 |  0.512492233 | -0.75993745 |  0.029745997 | -0.526605443 |  0.75617495 |  0.09088757 | -0.4428188 | ⋯ | -0.067477238 | 1.1690065 | 0.2871463 |  0.87203142 |  0.559696752 | -0.052406673 | -0.102448950 |  1.007599076 | -0.58417665 | -0.82657403 |\n| be |  0.28283749 |  0.02185847 | -1.4425185 |  0.167710814 | -0.44790750 | -0.518003098 |  0.051647847 |  0.21476109 | -0.87474988 | -0.5868641 | ⋯ | -0.688947469 | 0.4722453 | 0.6577129 |  0.90697095 |  1.017378164 |  0.114565906 |  0.728626254 |  0.902513682 |  0.05861234 | -0.95253640 |\n| when |  0.16942950 | -0.69643404 | -0.8785456 | -0.044236829 |  0.22389559 |  0.218573021 | -0.342530269 |  0.70260641 | -0.63261592 | -1.0646387 | ⋯ | -0.129041535 | 1.1645497 | 0.8748809 |  0.61149677 |  0.640061583 |  0.374739556 |  0.297211930 | -0.179811189 |  0.06769368 | -0.19530544 |\n| with |  0.51493375 | -0.85476390 | -1.3779862 |  0.633571360 | -0.31009490 | -0.569237710 | -0.453635308 |  0.69090739 | -0.30020950 | -1.0686468 | ⋯ |  0.546739769 | 1.3293900 | 0.9229314 |  0.01843741 |  0.882998687 | -0.214145903 | -0.234868390 |  0.467989801 | -0.20973266 |  0.09242494 |\n| so |  0.43518000 | -0.61271702 | -1.2576371 | -0.370141735 | -0.39944119 | -0.206096288 | -0.311205664 |  0.69828094 | -0.92427719 | -0.7476003 | ⋯ |  0.099608427 | 0.5871950 | 1.1117145 |  0.39143774 |  0.492687491 |  0.353968411 |  0.895911547 |  0.121871380 | -0.37343258 | -0.08273767 |\n| on |  0.59969940 | -1.00557044 | -2.0211294 |  0.592511023 |  0.11367573 | -0.004992162 | -0.691903344 |  0.70014569 | -1.11767509 | -1.1833096 | ⋯ |  0.695171589 | 1.0036004 | 1.3252801 |  0.74257307 |  0.066241981 |  0.002643934 | -0.534613554 |  0.039622184 |  0.73803651 | -0.09712845 |\n| was |  0.64905300 | -0.98923659 | -0.9852883 |  0.910984863 | -0.34462532 |  0.425333470 | -0.234567540 |  1.34106202 | -0.43065139 | -0.3377714 | ⋯ |  0.354142381 | 0.4853990 | 1.4333954 |  0.45147687 |  0.622032544 |  0.647855629 |  0.088629654 | -0.061850423 |  0.63683263 | -0.38573138 |\n| but |  0.18693633 | -0.74668540 | -1.2558590 |  0.281841072 | -0.47389408 |  0.103364640 | -0.026913124 |  0.52866758 | -0.54662059 | -0.6586600 | ⋯ | -0.466982217 | 0.6718686 | 0.9988895 |  0.82054422 |  0.736419112 |  0.437368043 |  0.488170107 |  0.388788607 | -0.08358231 | -0.04387687 |\n| he |  1.15394314 | -1.10759222 | -0.8475731 |  0.635357755 | -0.27859281 |  1.107216159 | -0.356926170 |  0.92975387 | -0.04188973 | -0.7303973 | ⋯ |  0.860222507 | 0.5588400 | 1.4958305 |  1.24920355 |  0.695390830 |  0.734397965 |  0.316446983 |  0.039039399 | -0.09124719 |  0.01090295 |\n| like |  0.13243629 | -0.68196441 | -1.3995908 | -0.940976045 | -0.80537181 |  0.062827589 | -0.114515711 |  0.50544646 | -0.45742890 | -0.7841520 | ⋯ |  0.817125986 | 1.1051883 | 1.2717990 |  0.65548684 |  0.644173537 | -0.361841487 | -0.472050497 | -0.004153028 |  0.15938925 |  0.41821575 |\n| that |  0.43750344 | -0.58189489 | -1.4702877 |  0.687848056 | -0.76543832 |  0.165876761 |  0.080006208 |  0.56184661 | -0.11855347 | -0.4638706 | ⋯ |  0.005276605 | 1.0304909 | 0.6683722 |  1.01108274 |  0.615448666 |  0.484574855 |  0.004167039 | -0.218486076 |  0.26499621 | -0.21052289 |\n| are | -0.14341924 | -1.03757929 | -1.2857334 | -0.238915715 |  0.02918028 | -0.840337994 | -0.233678114 |  1.13795747 | -1.04935078 | -0.3944796 | ⋯ | -0.695810579 | 1.3011044 | 0.7818273 |  0.35312520 |  0.572296360 | -0.906472998 | -0.438791042 |  0.608495006 | -0.03605581 |  1.46582011 |\n| me |  0.53929675 | -0.71271830 | -1.1580592 | -0.135692760 |  0.12916879 |  0.309807293 | -0.438567525 |  0.26537195 | -0.42383084 | -0.2615553 | ⋯ | -0.632752272 | 1.6250256 | 1.1259227 |  1.60052831 | -0.007514038 |  1.041455442 |  0.063508085 | -0.013093116 | -0.56689741 | -1.06950911 |\n| of | -1.24883526 | -0.89486126 | -1.4192157 |  0.744012861 | -1.50305425 | -0.475032972 | -0.144426718 |  0.70020604 | -0.75120661 | -0.7190685 | ⋯ | -0.450830970 | 1.6904811 | 0.7748471 |  0.13082204 |  0.995318940 | -0.718575513 | -0.240805340 |  0.277899281 | -0.06371220 | -0.66647384 |\n| in |  0.77309697 | -0.72740194 | -1.5559369 |  0.459332303 | -0.86835475 | -0.364802701 | -0.496057385 |  0.26410878 | -0.72646978 | -0.7266337 | ⋯ |  0.043670690 | 0.8980010 | 1.0808435 |  0.14019658 |  0.008163692 | -0.764175252 | -0.051513225 | -0.077868104 |  0.15978812 | -0.92226165 |\n| it |  0.34079067 | -0.38734016 | -1.3916619 | -0.078257184 | -0.69930216 |  0.517317441 | -0.429555592 |  0.69321637 | -0.11307488 | -0.7576901 | ⋯ |  0.185232829 | 1.4058403 | 1.0122437 |  0.62448924 |  0.551349422 |  0.487480496 |  0.161114430 | -0.683544092 |  0.22282358 | -0.39265535 |\n| you |  0.46096771 | -0.51584745 | -1.8084014 | -0.285518484 | -0.29062820 | -0.063137734 | -0.114242858 |  0.17100351 | -0.55435952 | -0.4474065 | ⋯ | -0.859708842 | 1.3703449 | 0.6416512 |  1.13490020 |  0.256059923 |  0.773477748 |  0.028206076 |  0.404171039 | -0.55528292 | -1.12197184 |\n| my |  0.06996694 | -1.28749834 | -1.6345217 |  0.479953869 |  0.23901260 | -0.742588168 | -0.354232280 |  0.46311420 | -1.45084674 | -0.4734557 | ⋯ | -0.451633652 | 1.4100172 | 1.3128105 |  0.71639714 |  0.082677530 |  0.861428672 | -0.051903571 | -0.484869159 | -0.39794365 |  0.36894574 |\n| is | -0.16791467 |  0.03864938 | -1.4806589 | -0.004808392 | -0.54753438 | -0.050512281 | -0.108310608 |  1.06102805 | -0.78905250 | -0.5043514 | ⋯ |  0.941339249 | 1.0334282 | 1.7182371 |  0.98784481 |  1.068900958 | -0.385775921 |  0.371381677 | -0.340486566 | -0.80676722 |  0.12511416 |\n| to |  0.43481309 | -0.58222114 | -1.3526620 | -0.164314058 |  0.04213467 |  0.225188221 |  0.124631109 |  0.14982865 | -0.14420387 | -0.7668390 | ⋯ | -0.736723974 | 0.7045938 | 0.7155747 |  0.95134272 |  0.411088902 |  0.374227170 |  0.892367856 |  0.427908117 | -0.62428465 | -1.11231286 |\n| and |  0.36225460 | -1.01378561 | -1.0322945 | -0.358453057 | -0.37041794 | -0.085126377 | -0.605695044 |  0.78977045 | -0.57183245 | -1.3635476 | ⋯ | -0.165092097 | 1.3466418 | 1.2457942 |  0.42809506 |  0.322931471 | -0.293811885 |  0.652186210 |  0.267071963 | -0.42989400 |  0.03637533 |\n| a |  0.37189141 | -0.85665368 | -1.5738692 | -0.041131224 | -1.91131374 |  0.352998853 |  0.170112566 | -0.07298348 | -0.54963493 | -0.9005205 | ⋯ |  1.253177568 | 1.1260915 | 1.8603940 | -0.22164927 |  0.788741974 |  0.742962234 |  0.372429451 | -0.413793308 |  0.12117929 | -0.29739998 |\n| the |  0.38891068 | -0.88399918 | -1.5074105 | -0.343454444 | -0.93805158 | -0.007305140 | -0.952921270 |  0.77646064 | -0.48269182 | -0.7937678 | ⋯ | -0.104269760 | 1.4607548 | 1.2679326 | -0.28575237 |  0.507531505 | -0.860655346 |  0.415291275 | -1.053508614 |  0.27167243 | -0.51934954 |\n| i |  0.71442600 | -0.82348366 | -1.9898728 | -0.381494712 | -0.42153281 |  0.119676811 |  0.078833015 |  0.65684282 | -0.88310721 | -1.0057171 | ⋯ | -0.268243873 | 1.0580800 | 1.0078055 |  0.56378961 |  0.286672299 |  1.019334540 | -0.173161490 |  0.682597136 | -0.32932463 | -0.67716317 |\n\n",
            "text/latex": "A matrix: 14267 × 50 of type dbl\n\\begin{tabular}{r|lllllllllllllllllllll}\n\t1837 & -0.63303339 &  0.30040185 & -0.16941897 &  0.19313955 &  0.33992052 &  0.10447346 & -0.61349224 & -0.51546688 &  1.143208193 &  0.50096603 & ⋯ & -0.207672794 & -0.68709990 & -0.16135047 & -0.27323195 &  0.074283333 & -0.440089791 & -0.294238013 &  0.10534551 &  0.6299827557 &  0.528825835\\\\\n\t1841 & -0.64363169 &  0.39464305 & -0.51663717 & -0.26036606 &  0.17839496 &  0.28172289 &  0.35276825 &  0.04594110 & -0.085550141 &  0.60269109 & ⋯ & -0.078028838 & -0.84906721 & -0.74034014 &  0.22099173 & -0.005637184 & -0.196269850 & -0.490606502 & -0.14352095 & -0.5746922442 &  0.484796474\\\\\n\t1881 & -0.16064160 &  0.63842416 & -0.15512472 &  0.21920492 & -0.40337032 &  0.08934645 &  0.05465616 & -0.68041639 &  0.486641403 &  0.33557877 & ⋯ &  0.523296084 &  0.10808258 & -0.14250742 & -0.30209468 & -0.347134508 & -0.131963484 &  0.199057038 & -0.46297884 &  0.3299895984 & -0.023089008\\\\\n\t2005 &  0.30330476 &  0.46340260 &  1.07200098 &  0.64906417 & -0.09673991 &  0.24447156 & -0.22427952 &  0.05807113 &  0.176181114 &  0.77190048 & ⋯ &  0.217931912 & -0.23449325 & -0.54430110 &  0.74694629 &  0.378682670 & -0.414246425 & -0.251819016 &  0.09975650 & -0.3260630406 &  0.313982341\\\\\n\t36 & -1.04820731 &  0.18680421 & -0.36206242 & -0.31433877 & -0.16047504 &  0.02559040 & -0.52615126 &  0.30309286 &  0.594394198 &  0.15275491 & ⋯ & -0.432263762 & -0.94327215 & -0.86728761 &  0.03697629 &  0.023473632 &  0.250202793 & -0.225504402 &  0.56372624 &  0.3423216271 &  0.170207423\\\\\n\t38 &  0.23950902 &  0.66162068 &  0.40454963 & -0.12451774 & -0.27237319 & -0.25242899 & -0.32643207 &  0.77775335 &  0.008975544 &  0.54310462 & ⋯ &  0.002088803 & -0.40748182 &  0.14296547 & -0.57684190 & -0.405189757 & -0.280860809 & -0.102424417 & -0.13971630 &  0.1082684066 &  0.846399960\\\\\n\t39 & -0.45500920 &  0.37761543 &  0.12899650 &  0.34855021 &  0.95821330 &  0.22457393 &  0.27838264 &  0.27735947 &  0.175953891 & -0.13082525 & ⋯ & -0.223028636 &  0.25634268 & -0.61034492 &  0.08320432 & -0.007539595 &  0.823494643 & -0.504143283 & -0.23752902 &  0.5533741496 &  0.046986448\\\\\n\t52 & -0.05806557 &  0.05131635 &  1.13282643 &  0.22139621 &  0.68497688 &  0.09284366 & -0.25984258 & -1.01686067 & -0.238540322 & -0.14508061 & ⋯ & -0.016681551 & -0.84191835 &  0.13537092 &  0.93538656 &  0.331386129 & -0.284657078 &  0.037629748 &  0.77218520 &  0.0445297803 & -0.179013608\\\\\n\t5â & -0.33385473 &  0.15762580 &  0.69672298 &  0.06397768 & -0.38596070 &  0.44789943 &  0.46258904 &  0.51563506 &  0.232720042 &  0.36631229 & ⋯ &  0.770611898 & -0.51251799 & -0.14484181 &  0.03746398 & -0.351322133 & -0.531175056 & -1.027938904 & -0.24506040 & -0.2788825331 &  0.262615112\\\\\n\t600 & -0.33060412 &  0.50694422 &  0.08683890 & -0.07372704 & -0.19431043 & -0.51469502 & -0.15987011 & -0.83610260 & -0.196220839 &  0.08482536 & ⋯ &  0.066924373 & -0.81379241 & -0.69483290 & -0.94866798 & -0.504177778 & -0.240210791 &  0.055614033 &  0.22476259 & -0.3597520838 & -0.409118558\\\\\n\tabcdefg & -0.40157881 &  0.01796207 &  0.64565495 & -0.70716491 &  1.33364934 & -0.31949992 & -0.26349394 & -0.31911708 & -0.268414973 &  0.57926403 & ⋯ & -0.380683508 & -0.56358408 & -0.60721345 &  0.37689570 & -0.294437593 & -0.349698620 &  0.312965267 & -0.34318367 & -0.2756320172 &  0.390639932\\\\\n\tabroad &  0.15488023 &  0.27734733 &  0.26413380 & -0.12373690 &  0.09538514 &  0.30076671 & -0.18368433 & -0.94869674 &  0.835425264 &  0.86722846 & ⋯ &  0.191768403 & -0.57944446 & -0.61749644 &  0.21150241 & -0.528324816 &  0.073130100 &  0.574808522 &  0.84900215 &  0.0412230305 & -0.375758703\\\\\n\tabruptly & -0.14651066 &  0.81145941 &  1.07477592 &  0.38995279 &  0.94847820 &  0.22388005 & -0.25161239 & -0.08105080 &  0.222603359 &  0.28664104 & ⋯ &  0.416165364 & -0.77895841 & -0.20823782 & -0.53372262 &  0.079124525 &  0.480021354 & -0.307780197 & -0.41896826 & -0.0893551174 &  0.595651565\\\\\n\tabsorbing &  0.20015971 &  0.77594940 &  0.16698965 & -0.37351282 &  0.55434020 & -0.62725826 & -0.04442423 & -0.42002240 &  1.008285998 &  0.78512673 & ⋯ & -0.474354430 & -0.56125171 &  0.03619193 &  0.02560654 & -0.163977187 & -0.465297438 & -0.110314825 & -0.72296376 &  0.4077864150 & -0.060930952\\\\\n\taccomplishments & -0.13764559 &  0.50274629 &  0.62661033 &  0.02178542 &  0.14756980 & -0.47047875 & -0.51140888 & -0.15982545 &  0.293659412 &  0.39459262 & ⋯ & -0.445194673 & -0.07482632 &  0.15309754 & -0.25800632 & -0.363646755 &  0.558851918 & -0.579230029 &  0.15624892 & -0.6153160090 &  0.409156719\\\\\n\taccused & -0.61261534 &  0.10095523 &  0.12040146 & -0.21564831 & -0.13773283 &  0.36379750 &  0.46329951 &  0.12105378 &  0.431317256 &  0.15956916 & ⋯ & -0.683099987 & -0.33268572 & -0.76791701 & -0.21520854 & -0.370413862 &  0.545322211 &  0.335937806 &  0.01036726 &  0.7395227427 &  0.211819633\\\\\n\tachievement & -0.25862395 &  0.29322679 &  0.10182493 & -0.06985774 &  0.18478602 &  0.30963299 &  0.43193835 & -0.25541769 &  0.750169145 &  0.63206618 & ⋯ & -0.585145216 &  0.01526585 &  0.32202048 & -0.81241454 &  0.210245851 &  0.058727916 & -0.359031967 &  0.66510821 &  0.2153829511 &  0.379436211\\\\\n\tacquired & -0.34116270 &  0.50027169 &  0.47119684 &  0.45904676 &  0.57840933 &  0.45674340 & -0.33646089 & -0.18665639 &  0.508170088 &  0.20306662 & ⋯ &  0.266381253 & -0.24012731 & -0.73133292 & -0.03649425 & -0.779558511 & -0.036389511 & -0.075870580 &  0.19411978 &  0.6116579132 &  0.846860814\\\\\n\tacres & -0.31485318 &  0.30243760 & -0.09304048 &  0.83754469 &  0.34108108 & -0.09690345 &  0.35804593 & -0.16034645 &  0.615634092 &  0.72639915 & ⋯ & -0.384969230 & -0.52875394 & -0.69982675 & -0.68346097 &  0.395895187 &  0.286274512 &  0.339776968 & -0.04467395 & -0.3479136521 & -0.313874057\\\\\n\tadams & -0.69634439 & -0.26658760 &  0.90266824 & -0.61493526 &  0.78649574 &  0.56622898 &  0.14576546 & -0.15590994 & -0.152552419 &  0.06719049 & ⋯ &  0.255518293 & -0.29020555 &  0.16477009 & -0.30498240 & -0.217081124 & -0.355825160 & -0.005717112 & -0.27495496 & -0.0064399829 &  0.692910905\\\\\n\taddress & -0.18458682 &  0.82754345 &  0.40785055 &  0.07160201 &  0.29813581 & -0.14680054 & -0.05683479 &  0.27725900 &  0.776367427 &  0.71574551 & ⋯ & -0.425507604 &  0.56925232 & -0.77450787 & -0.21968531 & -0.393851543 &  0.235527390 & -0.222628464 &  0.33149269 & -0.1092724028 & -0.277868435\\\\\n\tadjust &  0.02851715 &  0.37271110 &  0.72450049 & -0.65003277 &  0.46891784 &  0.23665688 &  0.62821302 & -0.92325426 & -0.282036458 &  0.55982904 & ⋯ & -0.534008096 & -0.77349121 & -0.52626828 & -0.35360541 & -0.246580776 & -0.019781547 &  0.246551178 &  0.05270351 &  0.1101698701 & -0.590714979\\\\\n\tadmires &  0.44918936 &  0.56227903 &  0.03074817 &  0.36452636 &  0.12465177 & -0.44721982 &  0.66251004 & -0.43171553 &  0.306017283 &  0.42018114 & ⋯ &  0.166355598 &  0.25240886 & -0.66343709 & -0.08748625 & -0.369577214 & -0.067474622 &  0.076654036 & -0.01786241 & -0.1047113229 & -0.120111746\\\\\n\tadmits &  0.29036073 &  0.27498543 &  0.49407554 & -0.14200274 &  0.47344389 &  0.60071006 &  0.30293527 & -0.83453126 &  0.165434325 &  0.22509583 & ⋯ &  0.148829458 & -0.94952972 & -0.55808713 & -0.26700851 &  0.254484310 &  0.334849709 & -0.201680989 &  0.57556197 & -0.0171481467 & -0.004069417\\\\\n\tadmitted &  0.47993942 & -0.61493381 &  0.12408015 &  0.28097345 &  0.72183445 &  1.07804104 & -0.20036122 &  0.10829519 &  0.404195819 & -0.03653836 & ⋯ &  0.065173675 & -0.89852201 & -0.74149526 &  0.05404889 & -1.033616420 &  0.873615782 &  0.108723448 & -0.83790351 &  0.5334340693 & -0.146044437\\\\\n\tadorn &  0.25667473 &  0.34523755 &  0.28820475 & -0.20070982 &  0.08202452 & -0.28895571 &  0.07114694 & -0.86296166 &  0.379971986 & -0.56725116 & ⋯ & -0.921922461 & -0.71610052 & -0.88031595 &  0.26788561 &  0.034513968 &  0.105264456 & -0.408694046 &  0.01248398 &  0.1840377769 & -0.337146807\\\\\n\tadrenalin & -0.26604361 & -0.35490073 &  1.00307782 & -0.18668994 &  0.44937947 & -0.72531490 &  0.30058889 & -0.09830227 & -0.047422765 &  1.41489692 & ⋯ &  0.442566064 & -0.50471032 &  0.04323064 & -0.13455601 &  0.058062792 &  0.642578172 & -0.617369036 & -0.36684942 &  0.0800636819 & -0.558739711\\\\\n\tadress &  0.23531301 &  0.53459220 &  0.07612993 &  0.37024559 & -0.30720931 &  0.81581563 &  0.66167766 &  0.61156169 &  0.369798429 & -0.15421523 & ⋯ & -0.634295394 &  0.14429949 & -0.64907894 &  0.12386252 & -0.330147619 &  0.073750553 & -0.376343321 &  0.05642954 & -0.0118247254 & -0.507969695\\\\\n\tadrift & -0.19592400 &  0.60244978 &  0.22078414 &  0.49324494 &  0.89550409 & -0.39184929 &  0.19876276 & -0.49194338 & -0.572287628 &  0.01662615 & ⋯ & -0.732657136 &  0.09863665 & -0.36016847 & -0.17595746 & -0.756126706 & -0.002017151 & -0.067666600 &  0.15205261 &  0.3512133517 &  0.113470366\\\\\n\tadvanced & -0.24623197 &  0.60280456 &  1.20168672 &  0.85798190 &  0.39978256 &  0.16937385 & -0.04049930 & -0.09311009 &  0.479889034 &  0.47722388 & ⋯ & -0.531313985 & -0.46731587 & -0.89766364 &  0.71246716 &  0.372421698 & -0.111219321 & -0.602362906 &  0.34732118 & -0.0008298921 &  0.148789316\\\\\n\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n\tyour & -0.26367375 & -0.79079137 & -1.5063920 &  0.621950800 &  0.17502403 & -0.935297702 & -0.601180304 &  0.01600121 & -1.16517819 & -0.1845625 & ⋯ & -1.174381298 & 1.1423899 & 0.6179424 &  0.62678728 &  0.269599724 &  0.625622458 &  0.051591504 & -0.483605119 & -0.13837945 & -0.59079055\\\\\n\tshe &  0.69289123 & -0.49871991 & -0.5222423 &  0.409830475 & -0.12236762 &  0.714752240 & -0.015708922 &  0.95570365 & -0.47359760 & -0.3749117 & ⋯ &  0.721822721 & 1.0727351 & 1.1555149 &  1.60043253 &  0.613246704 &  0.250095766 &  0.456091981 & -0.119821136 & -0.07308069 & -0.00315397\\\\\n\tlove &  0.22250122 & -0.17730073 & -1.4340479 & -0.034107139 & -0.36885386 & -0.585152325 & -0.285122862 &  0.56371378 & -0.03731074 & -0.7822942 & ⋯ &  0.578649364 & 1.5367088 & 0.2989985 &  1.45558025 &  0.658917682 &  0.044272550 & -0.152453809 &  0.555487312 & -0.52112356 &  0.10285955\\\\\n\tthey &  0.58026652 & -1.17959436 & -1.1824842 & -0.352939878 & -0.26317892 & -0.159141653 &  0.003178093 &  0.49906131 & -0.40643702 & -0.6424109 & ⋯ & -0.536253460 & 0.9325739 & 0.3636713 &  0.68045159 &  0.317031096 & -0.114155112 &  0.596893721 &  0.681431082 &  0.07413305 &  0.22283298\\\\\n\tall &  0.39592632 & -0.67309049 & -1.0172284 &  0.684478344 & -0.19585116 & -0.017499164 & -0.344259324 &  0.71108178 & -0.29756383 & -0.7239884 & ⋯ & -0.547088172 & 1.3387397 & 0.6095917 &  0.60443375 &  0.664492883 & -0.779255753 &  0.171481528 &  0.470049161 &  0.39555708 &  0.12852380\\\\\n\tas &  0.39832854 & -0.29186656 & -1.4850306 & -0.581164915 & -1.09616048 & -0.511131519 & -0.685779204 &  0.85771183 & -1.06425903 & -0.7679770 & ⋯ &  0.096985115 & 0.7200969 & 1.4716890 & -0.22949817 &  0.429819727 & -0.499004022 &  1.190302261 & -0.387476423 &  0.18922229 & -0.11379285\\\\\n\tfor & -0.39839714 & -0.41988219 & -0.7809746 &  0.512492233 & -0.75993745 &  0.029745997 & -0.526605443 &  0.75617495 &  0.09088757 & -0.4428188 & ⋯ & -0.067477238 & 1.1690065 & 0.2871463 &  0.87203142 &  0.559696752 & -0.052406673 & -0.102448950 &  1.007599076 & -0.58417665 & -0.82657403\\\\\n\tbe &  0.28283749 &  0.02185847 & -1.4425185 &  0.167710814 & -0.44790750 & -0.518003098 &  0.051647847 &  0.21476109 & -0.87474988 & -0.5868641 & ⋯ & -0.688947469 & 0.4722453 & 0.6577129 &  0.90697095 &  1.017378164 &  0.114565906 &  0.728626254 &  0.902513682 &  0.05861234 & -0.95253640\\\\\n\twhen &  0.16942950 & -0.69643404 & -0.8785456 & -0.044236829 &  0.22389559 &  0.218573021 & -0.342530269 &  0.70260641 & -0.63261592 & -1.0646387 & ⋯ & -0.129041535 & 1.1645497 & 0.8748809 &  0.61149677 &  0.640061583 &  0.374739556 &  0.297211930 & -0.179811189 &  0.06769368 & -0.19530544\\\\\n\twith &  0.51493375 & -0.85476390 & -1.3779862 &  0.633571360 & -0.31009490 & -0.569237710 & -0.453635308 &  0.69090739 & -0.30020950 & -1.0686468 & ⋯ &  0.546739769 & 1.3293900 & 0.9229314 &  0.01843741 &  0.882998687 & -0.214145903 & -0.234868390 &  0.467989801 & -0.20973266 &  0.09242494\\\\\n\tso &  0.43518000 & -0.61271702 & -1.2576371 & -0.370141735 & -0.39944119 & -0.206096288 & -0.311205664 &  0.69828094 & -0.92427719 & -0.7476003 & ⋯ &  0.099608427 & 0.5871950 & 1.1117145 &  0.39143774 &  0.492687491 &  0.353968411 &  0.895911547 &  0.121871380 & -0.37343258 & -0.08273767\\\\\n\ton &  0.59969940 & -1.00557044 & -2.0211294 &  0.592511023 &  0.11367573 & -0.004992162 & -0.691903344 &  0.70014569 & -1.11767509 & -1.1833096 & ⋯ &  0.695171589 & 1.0036004 & 1.3252801 &  0.74257307 &  0.066241981 &  0.002643934 & -0.534613554 &  0.039622184 &  0.73803651 & -0.09712845\\\\\n\twas &  0.64905300 & -0.98923659 & -0.9852883 &  0.910984863 & -0.34462532 &  0.425333470 & -0.234567540 &  1.34106202 & -0.43065139 & -0.3377714 & ⋯ &  0.354142381 & 0.4853990 & 1.4333954 &  0.45147687 &  0.622032544 &  0.647855629 &  0.088629654 & -0.061850423 &  0.63683263 & -0.38573138\\\\\n\tbut &  0.18693633 & -0.74668540 & -1.2558590 &  0.281841072 & -0.47389408 &  0.103364640 & -0.026913124 &  0.52866758 & -0.54662059 & -0.6586600 & ⋯ & -0.466982217 & 0.6718686 & 0.9988895 &  0.82054422 &  0.736419112 &  0.437368043 &  0.488170107 &  0.388788607 & -0.08358231 & -0.04387687\\\\\n\the &  1.15394314 & -1.10759222 & -0.8475731 &  0.635357755 & -0.27859281 &  1.107216159 & -0.356926170 &  0.92975387 & -0.04188973 & -0.7303973 & ⋯ &  0.860222507 & 0.5588400 & 1.4958305 &  1.24920355 &  0.695390830 &  0.734397965 &  0.316446983 &  0.039039399 & -0.09124719 &  0.01090295\\\\\n\tlike &  0.13243629 & -0.68196441 & -1.3995908 & -0.940976045 & -0.80537181 &  0.062827589 & -0.114515711 &  0.50544646 & -0.45742890 & -0.7841520 & ⋯ &  0.817125986 & 1.1051883 & 1.2717990 &  0.65548684 &  0.644173537 & -0.361841487 & -0.472050497 & -0.004153028 &  0.15938925 &  0.41821575\\\\\n\tthat &  0.43750344 & -0.58189489 & -1.4702877 &  0.687848056 & -0.76543832 &  0.165876761 &  0.080006208 &  0.56184661 & -0.11855347 & -0.4638706 & ⋯ &  0.005276605 & 1.0304909 & 0.6683722 &  1.01108274 &  0.615448666 &  0.484574855 &  0.004167039 & -0.218486076 &  0.26499621 & -0.21052289\\\\\n\tare & -0.14341924 & -1.03757929 & -1.2857334 & -0.238915715 &  0.02918028 & -0.840337994 & -0.233678114 &  1.13795747 & -1.04935078 & -0.3944796 & ⋯ & -0.695810579 & 1.3011044 & 0.7818273 &  0.35312520 &  0.572296360 & -0.906472998 & -0.438791042 &  0.608495006 & -0.03605581 &  1.46582011\\\\\n\tme &  0.53929675 & -0.71271830 & -1.1580592 & -0.135692760 &  0.12916879 &  0.309807293 & -0.438567525 &  0.26537195 & -0.42383084 & -0.2615553 & ⋯ & -0.632752272 & 1.6250256 & 1.1259227 &  1.60052831 & -0.007514038 &  1.041455442 &  0.063508085 & -0.013093116 & -0.56689741 & -1.06950911\\\\\n\tof & -1.24883526 & -0.89486126 & -1.4192157 &  0.744012861 & -1.50305425 & -0.475032972 & -0.144426718 &  0.70020604 & -0.75120661 & -0.7190685 & ⋯ & -0.450830970 & 1.6904811 & 0.7748471 &  0.13082204 &  0.995318940 & -0.718575513 & -0.240805340 &  0.277899281 & -0.06371220 & -0.66647384\\\\\n\tin &  0.77309697 & -0.72740194 & -1.5559369 &  0.459332303 & -0.86835475 & -0.364802701 & -0.496057385 &  0.26410878 & -0.72646978 & -0.7266337 & ⋯ &  0.043670690 & 0.8980010 & 1.0808435 &  0.14019658 &  0.008163692 & -0.764175252 & -0.051513225 & -0.077868104 &  0.15978812 & -0.92226165\\\\\n\tit &  0.34079067 & -0.38734016 & -1.3916619 & -0.078257184 & -0.69930216 &  0.517317441 & -0.429555592 &  0.69321637 & -0.11307488 & -0.7576901 & ⋯ &  0.185232829 & 1.4058403 & 1.0122437 &  0.62448924 &  0.551349422 &  0.487480496 &  0.161114430 & -0.683544092 &  0.22282358 & -0.39265535\\\\\n\tyou &  0.46096771 & -0.51584745 & -1.8084014 & -0.285518484 & -0.29062820 & -0.063137734 & -0.114242858 &  0.17100351 & -0.55435952 & -0.4474065 & ⋯ & -0.859708842 & 1.3703449 & 0.6416512 &  1.13490020 &  0.256059923 &  0.773477748 &  0.028206076 &  0.404171039 & -0.55528292 & -1.12197184\\\\\n\tmy &  0.06996694 & -1.28749834 & -1.6345217 &  0.479953869 &  0.23901260 & -0.742588168 & -0.354232280 &  0.46311420 & -1.45084674 & -0.4734557 & ⋯ & -0.451633652 & 1.4100172 & 1.3128105 &  0.71639714 &  0.082677530 &  0.861428672 & -0.051903571 & -0.484869159 & -0.39794365 &  0.36894574\\\\\n\tis & -0.16791467 &  0.03864938 & -1.4806589 & -0.004808392 & -0.54753438 & -0.050512281 & -0.108310608 &  1.06102805 & -0.78905250 & -0.5043514 & ⋯ &  0.941339249 & 1.0334282 & 1.7182371 &  0.98784481 &  1.068900958 & -0.385775921 &  0.371381677 & -0.340486566 & -0.80676722 &  0.12511416\\\\\n\tto &  0.43481309 & -0.58222114 & -1.3526620 & -0.164314058 &  0.04213467 &  0.225188221 &  0.124631109 &  0.14982865 & -0.14420387 & -0.7668390 & ⋯ & -0.736723974 & 0.7045938 & 0.7155747 &  0.95134272 &  0.411088902 &  0.374227170 &  0.892367856 &  0.427908117 & -0.62428465 & -1.11231286\\\\\n\tand &  0.36225460 & -1.01378561 & -1.0322945 & -0.358453057 & -0.37041794 & -0.085126377 & -0.605695044 &  0.78977045 & -0.57183245 & -1.3635476 & ⋯ & -0.165092097 & 1.3466418 & 1.2457942 &  0.42809506 &  0.322931471 & -0.293811885 &  0.652186210 &  0.267071963 & -0.42989400 &  0.03637533\\\\\n\ta &  0.37189141 & -0.85665368 & -1.5738692 & -0.041131224 & -1.91131374 &  0.352998853 &  0.170112566 & -0.07298348 & -0.54963493 & -0.9005205 & ⋯ &  1.253177568 & 1.1260915 & 1.8603940 & -0.22164927 &  0.788741974 &  0.742962234 &  0.372429451 & -0.413793308 &  0.12117929 & -0.29739998\\\\\n\tthe &  0.38891068 & -0.88399918 & -1.5074105 & -0.343454444 & -0.93805158 & -0.007305140 & -0.952921270 &  0.77646064 & -0.48269182 & -0.7937678 & ⋯ & -0.104269760 & 1.4607548 & 1.2679326 & -0.28575237 &  0.507531505 & -0.860655346 &  0.415291275 & -1.053508614 &  0.27167243 & -0.51934954\\\\\n\ti &  0.71442600 & -0.82348366 & -1.9898728 & -0.381494712 & -0.42153281 &  0.119676811 &  0.078833015 &  0.65684282 & -0.88310721 & -1.0057171 & ⋯ & -0.268243873 & 1.0580800 & 1.0078055 &  0.56378961 &  0.286672299 &  1.019334540 & -0.173161490 &  0.682597136 & -0.32932463 & -0.67716317\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                [,1]        [,2]        [,3]        [,4]         [,5]       \n",
              "1837            -0.63303339  0.30040185 -0.16941897  0.19313955   0.33992052\n",
              "1841            -0.64363169  0.39464305 -0.51663717 -0.26036606   0.17839496\n",
              "1881            -0.16064160  0.63842416 -0.15512472  0.21920492  -0.40337032\n",
              "2005             0.30330476  0.46340260  1.07200098  0.64906417  -0.09673991\n",
              "36              -1.04820731  0.18680421 -0.36206242 -0.31433877  -0.16047504\n",
              "38               0.23950902  0.66162068  0.40454963 -0.12451774  -0.27237319\n",
              "39              -0.45500920  0.37761543  0.12899650  0.34855021   0.95821330\n",
              "52              -0.05806557  0.05131635  1.13282643  0.22139621   0.68497688\n",
              "5â              -0.33385473  0.15762580  0.69672298  0.06397768  -0.38596070\n",
              "600             -0.33060412  0.50694422  0.08683890 -0.07372704  -0.19431043\n",
              "abcdefg         -0.40157881  0.01796207  0.64565495 -0.70716491   1.33364934\n",
              "abroad           0.15488023  0.27734733  0.26413380 -0.12373690   0.09538514\n",
              "abruptly        -0.14651066  0.81145941  1.07477592  0.38995279   0.94847820\n",
              "absorbing        0.20015971  0.77594940  0.16698965 -0.37351282   0.55434020\n",
              "accomplishments -0.13764559  0.50274629  0.62661033  0.02178542   0.14756980\n",
              "accused         -0.61261534  0.10095523  0.12040146 -0.21564831  -0.13773283\n",
              "achievement     -0.25862395  0.29322679  0.10182493 -0.06985774   0.18478602\n",
              "acquired        -0.34116270  0.50027169  0.47119684  0.45904676   0.57840933\n",
              "acres           -0.31485318  0.30243760 -0.09304048  0.83754469   0.34108108\n",
              "adams           -0.69634439 -0.26658760  0.90266824 -0.61493526   0.78649574\n",
              "address         -0.18458682  0.82754345  0.40785055  0.07160201   0.29813581\n",
              "adjust           0.02851715  0.37271110  0.72450049 -0.65003277   0.46891784\n",
              "admires          0.44918936  0.56227903  0.03074817  0.36452636   0.12465177\n",
              "admits           0.29036073  0.27498543  0.49407554 -0.14200274   0.47344389\n",
              "admitted         0.47993942 -0.61493381  0.12408015  0.28097345   0.72183445\n",
              "adorn            0.25667473  0.34523755  0.28820475 -0.20070982   0.08202452\n",
              "adrenalin       -0.26604361 -0.35490073  1.00307782 -0.18668994   0.44937947\n",
              "adress           0.23531301  0.53459220  0.07612993  0.37024559  -0.30720931\n",
              "adrift          -0.19592400  0.60244978  0.22078414  0.49324494   0.89550409\n",
              "advanced        -0.24623197  0.60280456  1.20168672  0.85798190   0.39978256\n",
              "⋮               ⋮           ⋮           ⋮           ⋮            ⋮          \n",
              "your            -0.26367375 -0.79079137 -1.5063920   0.621950800  0.17502403\n",
              "she              0.69289123 -0.49871991 -0.5222423   0.409830475 -0.12236762\n",
              "love             0.22250122 -0.17730073 -1.4340479  -0.034107139 -0.36885386\n",
              "they             0.58026652 -1.17959436 -1.1824842  -0.352939878 -0.26317892\n",
              "all              0.39592632 -0.67309049 -1.0172284   0.684478344 -0.19585116\n",
              "as               0.39832854 -0.29186656 -1.4850306  -0.581164915 -1.09616048\n",
              "for             -0.39839714 -0.41988219 -0.7809746   0.512492233 -0.75993745\n",
              "be               0.28283749  0.02185847 -1.4425185   0.167710814 -0.44790750\n",
              "when             0.16942950 -0.69643404 -0.8785456  -0.044236829  0.22389559\n",
              "with             0.51493375 -0.85476390 -1.3779862   0.633571360 -0.31009490\n",
              "so               0.43518000 -0.61271702 -1.2576371  -0.370141735 -0.39944119\n",
              "on               0.59969940 -1.00557044 -2.0211294   0.592511023  0.11367573\n",
              "was              0.64905300 -0.98923659 -0.9852883   0.910984863 -0.34462532\n",
              "but              0.18693633 -0.74668540 -1.2558590   0.281841072 -0.47389408\n",
              "he               1.15394314 -1.10759222 -0.8475731   0.635357755 -0.27859281\n",
              "like             0.13243629 -0.68196441 -1.3995908  -0.940976045 -0.80537181\n",
              "that             0.43750344 -0.58189489 -1.4702877   0.687848056 -0.76543832\n",
              "are             -0.14341924 -1.03757929 -1.2857334  -0.238915715  0.02918028\n",
              "me               0.53929675 -0.71271830 -1.1580592  -0.135692760  0.12916879\n",
              "of              -1.24883526 -0.89486126 -1.4192157   0.744012861 -1.50305425\n",
              "in               0.77309697 -0.72740194 -1.5559369   0.459332303 -0.86835475\n",
              "it               0.34079067 -0.38734016 -1.3916619  -0.078257184 -0.69930216\n",
              "you              0.46096771 -0.51584745 -1.8084014  -0.285518484 -0.29062820\n",
              "my               0.06996694 -1.28749834 -1.6345217   0.479953869  0.23901260\n",
              "is              -0.16791467  0.03864938 -1.4806589  -0.004808392 -0.54753438\n",
              "to               0.43481309 -0.58222114 -1.3526620  -0.164314058  0.04213467\n",
              "and              0.36225460 -1.01378561 -1.0322945  -0.358453057 -0.37041794\n",
              "a                0.37189141 -0.85665368 -1.5738692  -0.041131224 -1.91131374\n",
              "the              0.38891068 -0.88399918 -1.5074105  -0.343454444 -0.93805158\n",
              "i                0.71442600 -0.82348366 -1.9898728  -0.381494712 -0.42153281\n",
              "                [,6]         [,7]         [,8]        [,9]         [,10]      \n",
              "1837             0.10447346  -0.61349224  -0.51546688  1.143208193  0.50096603\n",
              "1841             0.28172289   0.35276825   0.04594110 -0.085550141  0.60269109\n",
              "1881             0.08934645   0.05465616  -0.68041639  0.486641403  0.33557877\n",
              "2005             0.24447156  -0.22427952   0.05807113  0.176181114  0.77190048\n",
              "36               0.02559040  -0.52615126   0.30309286  0.594394198  0.15275491\n",
              "38              -0.25242899  -0.32643207   0.77775335  0.008975544  0.54310462\n",
              "39               0.22457393   0.27838264   0.27735947  0.175953891 -0.13082525\n",
              "52               0.09284366  -0.25984258  -1.01686067 -0.238540322 -0.14508061\n",
              "5â               0.44789943   0.46258904   0.51563506  0.232720042  0.36631229\n",
              "600             -0.51469502  -0.15987011  -0.83610260 -0.196220839  0.08482536\n",
              "abcdefg         -0.31949992  -0.26349394  -0.31911708 -0.268414973  0.57926403\n",
              "abroad           0.30076671  -0.18368433  -0.94869674  0.835425264  0.86722846\n",
              "abruptly         0.22388005  -0.25161239  -0.08105080  0.222603359  0.28664104\n",
              "absorbing       -0.62725826  -0.04442423  -0.42002240  1.008285998  0.78512673\n",
              "accomplishments -0.47047875  -0.51140888  -0.15982545  0.293659412  0.39459262\n",
              "accused          0.36379750   0.46329951   0.12105378  0.431317256  0.15956916\n",
              "achievement      0.30963299   0.43193835  -0.25541769  0.750169145  0.63206618\n",
              "acquired         0.45674340  -0.33646089  -0.18665639  0.508170088  0.20306662\n",
              "acres           -0.09690345   0.35804593  -0.16034645  0.615634092  0.72639915\n",
              "adams            0.56622898   0.14576546  -0.15590994 -0.152552419  0.06719049\n",
              "address         -0.14680054  -0.05683479   0.27725900  0.776367427  0.71574551\n",
              "adjust           0.23665688   0.62821302  -0.92325426 -0.282036458  0.55982904\n",
              "admires         -0.44721982   0.66251004  -0.43171553  0.306017283  0.42018114\n",
              "admits           0.60071006   0.30293527  -0.83453126  0.165434325  0.22509583\n",
              "admitted         1.07804104  -0.20036122   0.10829519  0.404195819 -0.03653836\n",
              "adorn           -0.28895571   0.07114694  -0.86296166  0.379971986 -0.56725116\n",
              "adrenalin       -0.72531490   0.30058889  -0.09830227 -0.047422765  1.41489692\n",
              "adress           0.81581563   0.66167766   0.61156169  0.369798429 -0.15421523\n",
              "adrift          -0.39184929   0.19876276  -0.49194338 -0.572287628  0.01662615\n",
              "advanced         0.16937385  -0.04049930  -0.09311009  0.479889034  0.47722388\n",
              "⋮               ⋮            ⋮            ⋮           ⋮            ⋮          \n",
              "your            -0.935297702 -0.601180304  0.01600121 -1.16517819  -0.1845625 \n",
              "she              0.714752240 -0.015708922  0.95570365 -0.47359760  -0.3749117 \n",
              "love            -0.585152325 -0.285122862  0.56371378 -0.03731074  -0.7822942 \n",
              "they            -0.159141653  0.003178093  0.49906131 -0.40643702  -0.6424109 \n",
              "all             -0.017499164 -0.344259324  0.71108178 -0.29756383  -0.7239884 \n",
              "as              -0.511131519 -0.685779204  0.85771183 -1.06425903  -0.7679770 \n",
              "for              0.029745997 -0.526605443  0.75617495  0.09088757  -0.4428188 \n",
              "be              -0.518003098  0.051647847  0.21476109 -0.87474988  -0.5868641 \n",
              "when             0.218573021 -0.342530269  0.70260641 -0.63261592  -1.0646387 \n",
              "with            -0.569237710 -0.453635308  0.69090739 -0.30020950  -1.0686468 \n",
              "so              -0.206096288 -0.311205664  0.69828094 -0.92427719  -0.7476003 \n",
              "on              -0.004992162 -0.691903344  0.70014569 -1.11767509  -1.1833096 \n",
              "was              0.425333470 -0.234567540  1.34106202 -0.43065139  -0.3377714 \n",
              "but              0.103364640 -0.026913124  0.52866758 -0.54662059  -0.6586600 \n",
              "he               1.107216159 -0.356926170  0.92975387 -0.04188973  -0.7303973 \n",
              "like             0.062827589 -0.114515711  0.50544646 -0.45742890  -0.7841520 \n",
              "that             0.165876761  0.080006208  0.56184661 -0.11855347  -0.4638706 \n",
              "are             -0.840337994 -0.233678114  1.13795747 -1.04935078  -0.3944796 \n",
              "me               0.309807293 -0.438567525  0.26537195 -0.42383084  -0.2615553 \n",
              "of              -0.475032972 -0.144426718  0.70020604 -0.75120661  -0.7190685 \n",
              "in              -0.364802701 -0.496057385  0.26410878 -0.72646978  -0.7266337 \n",
              "it               0.517317441 -0.429555592  0.69321637 -0.11307488  -0.7576901 \n",
              "you             -0.063137734 -0.114242858  0.17100351 -0.55435952  -0.4474065 \n",
              "my              -0.742588168 -0.354232280  0.46311420 -1.45084674  -0.4734557 \n",
              "is              -0.050512281 -0.108310608  1.06102805 -0.78905250  -0.5043514 \n",
              "to               0.225188221  0.124631109  0.14982865 -0.14420387  -0.7668390 \n",
              "and             -0.085126377 -0.605695044  0.78977045 -0.57183245  -1.3635476 \n",
              "a                0.352998853  0.170112566 -0.07298348 -0.54963493  -0.9005205 \n",
              "the             -0.007305140 -0.952921270  0.77646064 -0.48269182  -0.7937678 \n",
              "i                0.119676811  0.078833015  0.65684282 -0.88310721  -1.0057171 \n",
              "                [,11] [,12]        [,13]       [,14]       [,15]      \n",
              "1837            ⋯     -0.207672794 -0.68709990 -0.16135047 -0.27323195\n",
              "1841            ⋯     -0.078028838 -0.84906721 -0.74034014  0.22099173\n",
              "1881            ⋯      0.523296084  0.10808258 -0.14250742 -0.30209468\n",
              "2005            ⋯      0.217931912 -0.23449325 -0.54430110  0.74694629\n",
              "36              ⋯     -0.432263762 -0.94327215 -0.86728761  0.03697629\n",
              "38              ⋯      0.002088803 -0.40748182  0.14296547 -0.57684190\n",
              "39              ⋯     -0.223028636  0.25634268 -0.61034492  0.08320432\n",
              "52              ⋯     -0.016681551 -0.84191835  0.13537092  0.93538656\n",
              "5â              ⋯      0.770611898 -0.51251799 -0.14484181  0.03746398\n",
              "600             ⋯      0.066924373 -0.81379241 -0.69483290 -0.94866798\n",
              "abcdefg         ⋯     -0.380683508 -0.56358408 -0.60721345  0.37689570\n",
              "abroad          ⋯      0.191768403 -0.57944446 -0.61749644  0.21150241\n",
              "abruptly        ⋯      0.416165364 -0.77895841 -0.20823782 -0.53372262\n",
              "absorbing       ⋯     -0.474354430 -0.56125171  0.03619193  0.02560654\n",
              "accomplishments ⋯     -0.445194673 -0.07482632  0.15309754 -0.25800632\n",
              "accused         ⋯     -0.683099987 -0.33268572 -0.76791701 -0.21520854\n",
              "achievement     ⋯     -0.585145216  0.01526585  0.32202048 -0.81241454\n",
              "acquired        ⋯      0.266381253 -0.24012731 -0.73133292 -0.03649425\n",
              "acres           ⋯     -0.384969230 -0.52875394 -0.69982675 -0.68346097\n",
              "adams           ⋯      0.255518293 -0.29020555  0.16477009 -0.30498240\n",
              "address         ⋯     -0.425507604  0.56925232 -0.77450787 -0.21968531\n",
              "adjust          ⋯     -0.534008096 -0.77349121 -0.52626828 -0.35360541\n",
              "admires         ⋯      0.166355598  0.25240886 -0.66343709 -0.08748625\n",
              "admits          ⋯      0.148829458 -0.94952972 -0.55808713 -0.26700851\n",
              "admitted        ⋯      0.065173675 -0.89852201 -0.74149526  0.05404889\n",
              "adorn           ⋯     -0.921922461 -0.71610052 -0.88031595  0.26788561\n",
              "adrenalin       ⋯      0.442566064 -0.50471032  0.04323064 -0.13455601\n",
              "adress          ⋯     -0.634295394  0.14429949 -0.64907894  0.12386252\n",
              "adrift          ⋯     -0.732657136  0.09863665 -0.36016847 -0.17595746\n",
              "advanced        ⋯     -0.531313985 -0.46731587 -0.89766364  0.71246716\n",
              "⋮               ⋱     ⋮            ⋮           ⋮           ⋮          \n",
              "your            ⋯     -1.174381298 1.1423899   0.6179424    0.62678728\n",
              "she             ⋯      0.721822721 1.0727351   1.1555149    1.60043253\n",
              "love            ⋯      0.578649364 1.5367088   0.2989985    1.45558025\n",
              "they            ⋯     -0.536253460 0.9325739   0.3636713    0.68045159\n",
              "all             ⋯     -0.547088172 1.3387397   0.6095917    0.60443375\n",
              "as              ⋯      0.096985115 0.7200969   1.4716890   -0.22949817\n",
              "for             ⋯     -0.067477238 1.1690065   0.2871463    0.87203142\n",
              "be              ⋯     -0.688947469 0.4722453   0.6577129    0.90697095\n",
              "when            ⋯     -0.129041535 1.1645497   0.8748809    0.61149677\n",
              "with            ⋯      0.546739769 1.3293900   0.9229314    0.01843741\n",
              "so              ⋯      0.099608427 0.5871950   1.1117145    0.39143774\n",
              "on              ⋯      0.695171589 1.0036004   1.3252801    0.74257307\n",
              "was             ⋯      0.354142381 0.4853990   1.4333954    0.45147687\n",
              "but             ⋯     -0.466982217 0.6718686   0.9988895    0.82054422\n",
              "he              ⋯      0.860222507 0.5588400   1.4958305    1.24920355\n",
              "like            ⋯      0.817125986 1.1051883   1.2717990    0.65548684\n",
              "that            ⋯      0.005276605 1.0304909   0.6683722    1.01108274\n",
              "are             ⋯     -0.695810579 1.3011044   0.7818273    0.35312520\n",
              "me              ⋯     -0.632752272 1.6250256   1.1259227    1.60052831\n",
              "of              ⋯     -0.450830970 1.6904811   0.7748471    0.13082204\n",
              "in              ⋯      0.043670690 0.8980010   1.0808435    0.14019658\n",
              "it              ⋯      0.185232829 1.4058403   1.0122437    0.62448924\n",
              "you             ⋯     -0.859708842 1.3703449   0.6416512    1.13490020\n",
              "my              ⋯     -0.451633652 1.4100172   1.3128105    0.71639714\n",
              "is              ⋯      0.941339249 1.0334282   1.7182371    0.98784481\n",
              "to              ⋯     -0.736723974 0.7045938   0.7155747    0.95134272\n",
              "and             ⋯     -0.165092097 1.3466418   1.2457942    0.42809506\n",
              "a               ⋯      1.253177568 1.1260915   1.8603940   -0.22164927\n",
              "the             ⋯     -0.104269760 1.4607548   1.2679326   -0.28575237\n",
              "i               ⋯     -0.268243873 1.0580800   1.0078055    0.56378961\n",
              "                [,16]        [,17]        [,18]        [,19]       \n",
              "1837             0.074283333 -0.440089791 -0.294238013  0.10534551 \n",
              "1841            -0.005637184 -0.196269850 -0.490606502 -0.14352095 \n",
              "1881            -0.347134508 -0.131963484  0.199057038 -0.46297884 \n",
              "2005             0.378682670 -0.414246425 -0.251819016  0.09975650 \n",
              "36               0.023473632  0.250202793 -0.225504402  0.56372624 \n",
              "38              -0.405189757 -0.280860809 -0.102424417 -0.13971630 \n",
              "39              -0.007539595  0.823494643 -0.504143283 -0.23752902 \n",
              "52               0.331386129 -0.284657078  0.037629748  0.77218520 \n",
              "5â              -0.351322133 -0.531175056 -1.027938904 -0.24506040 \n",
              "600             -0.504177778 -0.240210791  0.055614033  0.22476259 \n",
              "abcdefg         -0.294437593 -0.349698620  0.312965267 -0.34318367 \n",
              "abroad          -0.528324816  0.073130100  0.574808522  0.84900215 \n",
              "abruptly         0.079124525  0.480021354 -0.307780197 -0.41896826 \n",
              "absorbing       -0.163977187 -0.465297438 -0.110314825 -0.72296376 \n",
              "accomplishments -0.363646755  0.558851918 -0.579230029  0.15624892 \n",
              "accused         -0.370413862  0.545322211  0.335937806  0.01036726 \n",
              "achievement      0.210245851  0.058727916 -0.359031967  0.66510821 \n",
              "acquired        -0.779558511 -0.036389511 -0.075870580  0.19411978 \n",
              "acres            0.395895187  0.286274512  0.339776968 -0.04467395 \n",
              "adams           -0.217081124 -0.355825160 -0.005717112 -0.27495496 \n",
              "address         -0.393851543  0.235527390 -0.222628464  0.33149269 \n",
              "adjust          -0.246580776 -0.019781547  0.246551178  0.05270351 \n",
              "admires         -0.369577214 -0.067474622  0.076654036 -0.01786241 \n",
              "admits           0.254484310  0.334849709 -0.201680989  0.57556197 \n",
              "admitted        -1.033616420  0.873615782  0.108723448 -0.83790351 \n",
              "adorn            0.034513968  0.105264456 -0.408694046  0.01248398 \n",
              "adrenalin        0.058062792  0.642578172 -0.617369036 -0.36684942 \n",
              "adress          -0.330147619  0.073750553 -0.376343321  0.05642954 \n",
              "adrift          -0.756126706 -0.002017151 -0.067666600  0.15205261 \n",
              "advanced         0.372421698 -0.111219321 -0.602362906  0.34732118 \n",
              "⋮               ⋮            ⋮            ⋮            ⋮           \n",
              "your             0.269599724  0.625622458  0.051591504 -0.483605119\n",
              "she              0.613246704  0.250095766  0.456091981 -0.119821136\n",
              "love             0.658917682  0.044272550 -0.152453809  0.555487312\n",
              "they             0.317031096 -0.114155112  0.596893721  0.681431082\n",
              "all              0.664492883 -0.779255753  0.171481528  0.470049161\n",
              "as               0.429819727 -0.499004022  1.190302261 -0.387476423\n",
              "for              0.559696752 -0.052406673 -0.102448950  1.007599076\n",
              "be               1.017378164  0.114565906  0.728626254  0.902513682\n",
              "when             0.640061583  0.374739556  0.297211930 -0.179811189\n",
              "with             0.882998687 -0.214145903 -0.234868390  0.467989801\n",
              "so               0.492687491  0.353968411  0.895911547  0.121871380\n",
              "on               0.066241981  0.002643934 -0.534613554  0.039622184\n",
              "was              0.622032544  0.647855629  0.088629654 -0.061850423\n",
              "but              0.736419112  0.437368043  0.488170107  0.388788607\n",
              "he               0.695390830  0.734397965  0.316446983  0.039039399\n",
              "like             0.644173537 -0.361841487 -0.472050497 -0.004153028\n",
              "that             0.615448666  0.484574855  0.004167039 -0.218486076\n",
              "are              0.572296360 -0.906472998 -0.438791042  0.608495006\n",
              "me              -0.007514038  1.041455442  0.063508085 -0.013093116\n",
              "of               0.995318940 -0.718575513 -0.240805340  0.277899281\n",
              "in               0.008163692 -0.764175252 -0.051513225 -0.077868104\n",
              "it               0.551349422  0.487480496  0.161114430 -0.683544092\n",
              "you              0.256059923  0.773477748  0.028206076  0.404171039\n",
              "my               0.082677530  0.861428672 -0.051903571 -0.484869159\n",
              "is               1.068900958 -0.385775921  0.371381677 -0.340486566\n",
              "to               0.411088902  0.374227170  0.892367856  0.427908117\n",
              "and              0.322931471 -0.293811885  0.652186210  0.267071963\n",
              "a                0.788741974  0.742962234  0.372429451 -0.413793308\n",
              "the              0.507531505 -0.860655346  0.415291275 -1.053508614\n",
              "i                0.286672299  1.019334540 -0.173161490  0.682597136\n",
              "                [,20]         [,21]       \n",
              "1837             0.6299827557  0.528825835\n",
              "1841            -0.5746922442  0.484796474\n",
              "1881             0.3299895984 -0.023089008\n",
              "2005            -0.3260630406  0.313982341\n",
              "36               0.3423216271  0.170207423\n",
              "38               0.1082684066  0.846399960\n",
              "39               0.5533741496  0.046986448\n",
              "52               0.0445297803 -0.179013608\n",
              "5â              -0.2788825331  0.262615112\n",
              "600             -0.3597520838 -0.409118558\n",
              "abcdefg         -0.2756320172  0.390639932\n",
              "abroad           0.0412230305 -0.375758703\n",
              "abruptly        -0.0893551174  0.595651565\n",
              "absorbing        0.4077864150 -0.060930952\n",
              "accomplishments -0.6153160090  0.409156719\n",
              "accused          0.7395227427  0.211819633\n",
              "achievement      0.2153829511  0.379436211\n",
              "acquired         0.6116579132  0.846860814\n",
              "acres           -0.3479136521 -0.313874057\n",
              "adams           -0.0064399829  0.692910905\n",
              "address         -0.1092724028 -0.277868435\n",
              "adjust           0.1101698701 -0.590714979\n",
              "admires         -0.1047113229 -0.120111746\n",
              "admits          -0.0171481467 -0.004069417\n",
              "admitted         0.5334340693 -0.146044437\n",
              "adorn            0.1840377769 -0.337146807\n",
              "adrenalin        0.0800636819 -0.558739711\n",
              "adress          -0.0118247254 -0.507969695\n",
              "adrift           0.3512133517  0.113470366\n",
              "advanced        -0.0008298921  0.148789316\n",
              "⋮               ⋮             ⋮           \n",
              "your            -0.13837945   -0.59079055 \n",
              "she             -0.07308069   -0.00315397 \n",
              "love            -0.52112356    0.10285955 \n",
              "they             0.07413305    0.22283298 \n",
              "all              0.39555708    0.12852380 \n",
              "as               0.18922229   -0.11379285 \n",
              "for             -0.58417665   -0.82657403 \n",
              "be               0.05861234   -0.95253640 \n",
              "when             0.06769368   -0.19530544 \n",
              "with            -0.20973266    0.09242494 \n",
              "so              -0.37343258   -0.08273767 \n",
              "on               0.73803651   -0.09712845 \n",
              "was              0.63683263   -0.38573138 \n",
              "but             -0.08358231   -0.04387687 \n",
              "he              -0.09124719    0.01090295 \n",
              "like             0.15938925    0.41821575 \n",
              "that             0.26499621   -0.21052289 \n",
              "are             -0.03605581    1.46582011 \n",
              "me              -0.56689741   -1.06950911 \n",
              "of              -0.06371220   -0.66647384 \n",
              "in               0.15978812   -0.92226165 \n",
              "it               0.22282358   -0.39265535 \n",
              "you             -0.55528292   -1.12197184 \n",
              "my              -0.39794365    0.36894574 \n",
              "is              -0.80676722    0.12511416 \n",
              "to              -0.62428465   -1.11231286 \n",
              "and             -0.42989400    0.03637533 \n",
              "a                0.12117929   -0.29739998 \n",
              "the              0.27167243   -0.51934954 \n",
              "i               -0.32932463   -0.67716317 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoS90zpVNs4O"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arghyAVINGtH"
      },
      "source": [
        "Now we can begin to play. Similarly to standard correlation, we can look at comparing two vectors using **cosine similarity**. Let's see what is similar with 'school':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XLSKt6-vN2_i",
        "outputId": "1c18338c-9973-413a-ce4e-20bbc9853227"
      },
      "source": [
        "# Word vector for school\n",
        "school <- word_vectors[\"school\", , drop = FALSE]\n",
        "\n",
        "# Cosine similarity\n",
        "school_cos_sim <- sim2(x = word_vectors, y = school, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top ten words relating to school\n",
        "head(round(sort(school_cos_sim[,1], decreasing = TRUE), digits = 3), 10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>school</dt><dd>1</dd><dt>today</dt><dd>0.69</dd><dt>day</dt><dd>0.67</dd><dt>work</dt><dd>0.664</dd><dt>time</dt><dd>0.659</dd><dt>we</dt><dd>0.643</dd><dt>pool</dt><dd>0.636</dd><dt>fun</dt><dd>0.634</dd><dt>cool</dt><dd>0.627</dd><dt>end</dt><dd>0.625</dd></dl>\n"
            ],
            "text/markdown": "school\n:   1today\n:   0.69day\n:   0.67work\n:   0.664time\n:   0.659we\n:   0.643pool\n:   0.636fun\n:   0.634cool\n:   0.627end\n:   0.625\n\n",
            "text/latex": "\\begin{description*}\n\\item[school] 1\n\\item[today] 0.69\n\\item[day] 0.67\n\\item[work] 0.664\n\\item[time] 0.659\n\\item[we] 0.643\n\\item[pool] 0.636\n\\item[fun] 0.634\n\\item[cool] 0.627\n\\item[end] 0.625\n\\end{description*}\n",
            "text/plain": [
              "school  today    day   work   time     we   pool    fun   cool    end \n",
              " 1.000  0.690  0.670  0.664  0.659  0.643  0.636  0.634  0.627  0.625 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCiI8fafOUJR"
      },
      "source": [
        "Obviously, school is the most similar to school. Based on the poems that the children wrote, we can also see words like 'work', 'fun' and 'home' as most similar to 'school'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9NuW-5GzNAo"
      },
      "source": [
        "## Pet example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQVER95zFn6"
      },
      "source": [
        "Let's try our pet example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZlyQDS9EzYpY",
        "outputId": "6d080f32-2e94-4608-f39d-aecfbaff62e0"
      },
      "source": [
        "# cat - meow + bark should equal dog\n",
        "dog <- word_vectors[\"cat\", , drop = FALSE] -\n",
        "  word_vectors[\"meow\", , drop = FALSE] +\n",
        "  word_vectors[\"bark\", , drop = FALSE]\n",
        "  \n",
        "# Calculates pairwise similarities between the rows of two matrices\n",
        "dog_cos_sim <- sim2(x = word_vectors, y = dog, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(dog_cos_sim[,1], decreasing = TRUE), digits = 4), 5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>cat</dt><dd>0.7769</dd><dt>dog</dt><dd>0.7682</dd><dt>fat</dt><dd>0.7305</dd><dt>rat</dt><dd>0.6633</dd><dt>likes</dt><dd>0.6514</dd></dl>\n"
            ],
            "text/markdown": "cat\n:   0.7769dog\n:   0.7682fat\n:   0.7305rat\n:   0.6633likes\n:   0.6514\n\n",
            "text/latex": "\\begin{description*}\n\\item[cat] 0.7769\n\\item[dog] 0.7682\n\\item[fat] 0.7305\n\\item[rat] 0.6633\n\\item[likes] 0.6514\n\\end{description*}\n",
            "text/plain": [
              "   cat    dog    fat    rat  likes \n",
              "0.7769 0.7682 0.7305 0.6633 0.6514 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZNvOiUXynMr"
      },
      "source": [
        "Success - our predicted result was correct! We get 'dog' as the highest predicted result after the one we used (cat). We can think of this scenario as cats say meow and dogs say bark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO8mYP8fzIPY"
      },
      "source": [
        "## Parent example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_sKk9O-z4Tf"
      },
      "source": [
        "Let's move on to the parent example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tk1jjEFlwDKB",
        "outputId": "4a8e8285-e89b-418b-a20b-6368f051b9ae"
      },
      "source": [
        "# mom - girl + boy should equal dad\n",
        "dad <- word_vectors[\"mom\", , drop = FALSE] -\n",
        "  word_vectors[\"girl\", , drop = FALSE] +\n",
        "  word_vectors[\"boy\", , drop = FALSE]\n",
        "  \n",
        "# Calculates pairwise similarities between the rows of two matrices\n",
        "dad_cos_sim <- sim2(x = word_vectors, y = dad, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(dad_cos_sim[,1], decreasing = TRUE), digits = 4), 5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>mom</dt><dd>0.865</dd><dt>dad</dt><dd>0.7863</dd><dt>said</dt><dd>0.7067</dd><dt>my</dt><dd>0.6561</dd><dt>says</dt><dd>0.6497</dd></dl>\n"
            ],
            "text/markdown": "mom\n:   0.865dad\n:   0.7863said\n:   0.7067my\n:   0.6561says\n:   0.6497\n\n",
            "text/latex": "\\begin{description*}\n\\item[mom] 0.865\n\\item[dad] 0.7863\n\\item[said] 0.7067\n\\item[my] 0.6561\n\\item[says] 0.6497\n\\end{description*}\n",
            "text/plain": [
              "   mom    dad   said     my   says \n",
              "0.8650 0.7863 0.7067 0.6561 0.6497 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5XmJd60Ayg"
      },
      "source": [
        "'Dad' was a top result. Finally, let's try the infamous king and queen example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS-z1CCx8yOy"
      },
      "source": [
        "## King and queen example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YFGQfG7i8xSC",
        "outputId": "e2be422d-135d-45d3-d961-373e6bc3ce98"
      },
      "source": [
        "# king - man + woman should equal queen\n",
        "queen <- word_vectors[\"king\", , drop = FALSE] -\n",
        "  word_vectors[\"man\", , drop = FALSE] +\n",
        "  word_vectors[\"woman\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "queen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(queen_cos_sim[,1], decreasing = TRUE), digits = 4), 5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>king</dt><dd>0.7068</dd><dt>martin</dt><dd>0.6126</dd><dt>careless</dt><dd>0.5958</dd><dt>luther</dt><dd>0.5952</dd><dt>lil</dt><dd>0.583</dd></dl>\n"
            ],
            "text/markdown": "king\n:   0.7068martin\n:   0.6126careless\n:   0.5958luther\n:   0.5952lil\n:   0.583\n\n",
            "text/latex": "\\begin{description*}\n\\item[king] 0.7068\n\\item[martin] 0.6126\n\\item[careless] 0.5958\n\\item[luther] 0.5952\n\\item[lil] 0.583\n\\end{description*}\n",
            "text/plain": [
              "    king   martin careless   luther      lil \n",
              "  0.7068   0.6126   0.5958   0.5952   0.5830 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rOg8QQY_Sbq"
      },
      "source": [
        "Unfortunately, queen came in at 4th. Let's try changing **man** and **woman** to **boy** and **girl** to account for the kid's writting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "zu15bTVZ_gp_",
        "outputId": "1b85df58-782d-4b2e-88d5-6a635e984c88"
      },
      "source": [
        "# king - boy + girl should equal queen\n",
        "queen <- word_vectors[\"king\", , drop = FALSE] -\n",
        "  word_vectors[\"boy\", , drop = FALSE] +\n",
        "  word_vectors[\"girl\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "queen_cos_sim = sim2(x = word_vectors, y = queen, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(queen_cos_sim[,1], decreasing = TRUE), digits = 4), 5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>king</dt><dd>0.8683</dd><dt>queen</dt><dd>0.6602</dd><dt>kong</dt><dd>0.6104</dd><dt>dr</dt><dd>0.6078</dd><dt>blur</dt><dd>0.5735</dd></dl>\n"
            ],
            "text/markdown": "king\n:   0.8683queen\n:   0.6602kong\n:   0.6104dr\n:   0.6078blur\n:   0.5735\n\n",
            "text/latex": "\\begin{description*}\n\\item[king] 0.8683\n\\item[queen] 0.6602\n\\item[kong] 0.6104\n\\item[dr] 0.6078\n\\item[blur] 0.5735\n\\end{description*}\n",
            "text/plain": [
              "  king  queen   kong     dr   blur \n",
              "0.8683 0.6602 0.6104 0.6078 0.5735 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeEJQciGAcdm"
      },
      "source": [
        "It worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Tangent on Bias\n",
        "\n",
        "As we discussed in class, word embeddings have proven to be a useful tool for uncovering/revealing bias in large corpora. Here, we can see how well the kids fare. We'll look at occupations. "
      ],
      "metadata": {
        "id": "mvXWRuybd8bh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6EHjaWj1Q6ie",
        "outputId": "bf0534f0-1718-4683-8d36-16a2a9334e87"
      },
      "source": [
        "job <- word_vectors[\"job\", , drop = FALSE] -\n",
        "  word_vectors[\"boy\", , drop = FALSE] +\n",
        "  word_vectors[\"girl\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "job_cos_sim = sim2(x = word_vectors, y = job, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(job_cos_sim[,1], decreasing = TRUE), digits = 4), 5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>job</dt><dd>0.7364</dd><dt>art</dt><dd>0.5116</dd><dt>site</dt><dd>0.4817</dd><dt>brain</dt><dd>0.4759</dd><dt>least</dt><dd>0.4573</dd></dl>\n"
            ],
            "text/markdown": "job\n:   0.7364art\n:   0.5116site\n:   0.4817brain\n:   0.4759least\n:   0.4573\n\n",
            "text/latex": "\\begin{description*}\n\\item[job] 0.7364\n\\item[art] 0.5116\n\\item[site] 0.4817\n\\item[brain] 0.4759\n\\item[least] 0.4573\n\\end{description*}\n",
            "text/plain": [
              "   job    art   site  brain  least \n",
              "0.7364 0.5116 0.4817 0.4759 0.4573 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AT6TXy8sRDvL",
        "outputId": "d7920924-0b50-43ef-c851-fe8a2deb0c5b"
      },
      "source": [
        "job <- word_vectors[\"job\", , drop = FALSE] -\n",
        "  word_vectors[\"girl\", , drop = FALSE] +\n",
        "  word_vectors[\"boy\", , drop = FALSE]\n",
        "\n",
        "# Calculate pairwise similarities\n",
        "job_cos_sim = sim2(x = word_vectors, y = job, method = \"cosine\", norm = \"l2\")\n",
        "\n",
        "# Top five predictions\n",
        "head(round(sort(job_cos_sim[,1], decreasing = TRUE), digits = 3), 5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>job</dt><dd>0.837</dd><dt>test</dt><dd>0.546</dd><dt>bob</dt><dd>0.506</dd><dt>soccer</dt><dd>0.495</dd><dt>wife</dt><dd>0.489</dd></dl>\n"
            ],
            "text/markdown": "job\n:   0.837test\n:   0.546bob\n:   0.506soccer\n:   0.495wife\n:   0.489\n\n",
            "text/latex": "\\begin{description*}\n\\item[job] 0.837\n\\item[test] 0.546\n\\item[bob] 0.506\n\\item[soccer] 0.495\n\\item[wife] 0.489\n\\end{description*}\n",
            "text/plain": [
              "   job   test    bob soccer   wife \n",
              " 0.837  0.546  0.506  0.495  0.489 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! We're not seeing the same dynamics observed in other settings. Given the small corpus, though, we'd want to add a lot more data before we could be confident that those biases weren't present here."
      ],
      "metadata": {
        "id": "YGaGPvmRAqSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Estimated Embeddings"
      ],
      "metadata": {
        "id": "tsKGecGLCP9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that in hand, we can estimate a simple clustering algorithm. We specify 5 clusters, but feel free to play around with that number."
      ],
      "metadata": {
        "id": "YlF5IxbVGitT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(12345)\n",
        "n_means <- 5\n",
        "clusters <- kmeans(word_vectors, centers = n_means, iter.max  = 30)"
      ],
      "metadata": {
        "id": "q0ZO9qPsGnno"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note what we have estimated with KMeans. We have 5 cluster centers, each of 50 dimensions, the same number of dimensions that we have for each of our tokens. Therefore, we look for which of the tokens are most similar to one of our cluster centers. "
      ],
      "metadata": {
        "id": "XvDjFDOmGoEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster1 <- t(as.matrix(clusters$centers[1,]))"
      ],
      "metadata": {
        "id": "qzYg8qWtIHWO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clus_cos_sim = sim2(x = word_vectors, y = cluster1, method = \"cosine\", norm = \"l2\")\n"
      ],
      "metadata": {
        "id": "lX4qDDhYISjP"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top ten cluster words\n",
        "head(sort(clus_cos_sim[,1], decreasing = TRUE), 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "aGxB3wakJYen",
        "outputId": "f1b34683-51d2-4a00-f870-ed3934d83e42"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>gripping</dt><dd>0.65860993953408</dd><dt>phases</dt><dd>0.658410266112949</dd><dt>roofs</dt><dd>0.657932266145299</dd><dt>lowest</dt><dd>0.657266098630193</dd><dt>kindest</dt><dd>0.647351647552704</dd><dt>freshness</dt><dd>0.635636499776322</dd><dt>elements</dt><dd>0.631759676660343</dd><dt>jumpin</dt><dd>0.631156187559815</dd><dt>swerving</dt><dd>0.628711133133854</dd><dt>wisper</dt><dd>0.624235958133409</dd></dl>\n"
            ],
            "text/markdown": "gripping\n:   0.65860993953408phases\n:   0.658410266112949roofs\n:   0.657932266145299lowest\n:   0.657266098630193kindest\n:   0.647351647552704freshness\n:   0.635636499776322elements\n:   0.631759676660343jumpin\n:   0.631156187559815swerving\n:   0.628711133133854wisper\n:   0.624235958133409\n\n",
            "text/latex": "\\begin{description*}\n\\item[gripping] 0.65860993953408\n\\item[phases] 0.658410266112949\n\\item[roofs] 0.657932266145299\n\\item[lowest] 0.657266098630193\n\\item[kindest] 0.647351647552704\n\\item[freshness] 0.635636499776322\n\\item[elements] 0.631759676660343\n\\item[jumpin] 0.631156187559815\n\\item[swerving] 0.628711133133854\n\\item[wisper] 0.624235958133409\n\\end{description*}\n",
            "text/plain": [
              " gripping    phases     roofs    lowest   kindest freshness  elements    jumpin \n",
              "0.6586099 0.6584103 0.6579323 0.6572661 0.6473516 0.6356365 0.6317597 0.6311562 \n",
              " swerving    wisper \n",
              "0.6287111 0.6242360 "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's loop over the cluster centers:"
      ],
      "metadata": {
        "id": "umhocUbFJmmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topWordMatrix <- matrix(NA, n_means, 10)\n",
        "\n",
        "for (i in 1:n_means){\n",
        "  cluster <- t(as.matrix(clusters$centers[i,]))\n",
        "  clus_cos_sim = sim2(x = word_vectors, y = cluster, method = \"cosine\", norm = \"l2\")\n",
        "  topWordMatrix[i,] <- names(head(sort(clus_cos_sim[,1], decreasing = TRUE), 10))\n",
        "}"
      ],
      "metadata": {
        "id": "xw9rj-2zJuWK"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topWordMatrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "1oluWhm3KX77",
        "outputId": "6edf4774-bbea-4e95-e2da-f5dcf735a190"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 5 × 10 of type chr</caption>\n",
              "<tbody>\n",
              "\t<tr><td>gripping</td><td>phases  </td><td>roofs  </td><td>lowest </td><td>kindest </td><td>freshness</td><td>elements</td><td>jumpin    </td><td>swerving</td><td>wisper</td></tr>\n",
              "\t<tr><td>lill    </td><td>noâ     </td><td>tempted</td><td>ore    </td><td>homesick</td><td>backpacks</td><td>monroe  </td><td>sharpening</td><td>besties </td><td>blunt </td></tr>\n",
              "\t<tr><td>werewolf</td><td>discrace</td><td>tad    </td><td>platter</td><td>doller  </td><td>rio      </td><td>blocker </td><td>turbo     </td><td>frankie </td><td>louse </td></tr>\n",
              "\t<tr><td>but     </td><td>just    </td><td>when   </td><td>because</td><td>that    </td><td>you      </td><td>now     </td><td>not       </td><td>all     </td><td>if    </td></tr>\n",
              "\t<tr><td>bag     </td><td>bathroom</td><td>salt   </td><td>bedroom</td><td>cross   </td><td>thrill   </td><td>kitchen </td><td>bucket    </td><td>lock    </td><td>draw  </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 5 × 10 of type chr\n\n| gripping | phases   | roofs   | lowest  | kindest  | freshness | elements | jumpin     | swerving | wisper |\n| lill     | noâ      | tempted | ore     | homesick | backpacks | monroe   | sharpening | besties  | blunt  |\n| werewolf | discrace | tad     | platter | doller   | rio       | blocker  | turbo      | frankie  | louse  |\n| but      | just     | when    | because | that     | you       | now      | not        | all      | if     |\n| bag      | bathroom | salt    | bedroom | cross    | thrill    | kitchen  | bucket     | lock     | draw   |\n\n",
            "text/latex": "A matrix: 5 × 10 of type chr\n\\begin{tabular}{llllllllll}\n\t gripping & phases   & roofs   & lowest  & kindest  & freshness & elements & jumpin     & swerving & wisper\\\\\n\t lill     & noâ      & tempted & ore     & homesick & backpacks & monroe   & sharpening & besties  & blunt \\\\\n\t werewolf & discrace & tad     & platter & doller   & rio       & blocker  & turbo      & frankie  & louse \\\\\n\t but      & just     & when    & because & that     & you       & now      & not        & all      & if    \\\\\n\t bag      & bathroom & salt    & bedroom & cross    & thrill    & kitchen  & bucket     & lock     & draw  \\\\\n\\end{tabular}\n",
            "text/plain": [
              "     [,1]     [,2]     [,3]    [,4]    [,5]     [,6]      [,7]     [,8]      \n",
              "[1,] gripping phases   roofs   lowest  kindest  freshness elements jumpin    \n",
              "[2,] lill     noâ      tempted ore     homesick backpacks monroe   sharpening\n",
              "[3,] werewolf discrace tad     platter doller   rio       blocker  turbo     \n",
              "[4,] but      just     when    because that     you       now      not       \n",
              "[5,] bag      bathroom salt    bedroom cross    thrill    kitchen  bucket    \n",
              "     [,9]     [,10] \n",
              "[1,] swerving wisper\n",
              "[2,] besties  blunt \n",
              "[3,] frankie  louse \n",
              "[4,] all      if    \n",
              "[5,] lock     draw  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little hard to see too much coming through in the way of the clusters here. Play around with the specifications to see if shifting the number of clusters in KMeans, the size of the window in GloVe, etc. get us to more sensible clusters. If not, it may just be that the dataset is too limited to really learn much."
      ],
      "metadata": {
        "id": "AucbV4LGKu9Y"
      }
    }
  ]
}